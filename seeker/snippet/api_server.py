#date: 2025-11-13T17:02:49Z
#url: https://api.github.com/gists/f99a8104b419a7b2f6437a18ce2a8790
#owner: https://api.github.com/users/wangwei334455

"""
Flaskåç«¯æœåŠ¡
æä¾›APIå’ŒWebSocketå®æ—¶é€šä¿¡
"""
# ğŸ”´ å…³é”®ï¼šFlask-SocketIO ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ eventletï¼ˆå¦‚æœå¯ç”¨ï¼‰
# ä½†æ˜¯ï¼Œå¦‚æœé‡åˆ°è¿æ¥é—®é¢˜ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨åº”ç”¨ monkey_patch
# å…ˆå°è¯•ä¸æ‰‹åŠ¨è°ƒç”¨ï¼Œå¦‚æœè¿æ¥å¤±è´¥å†å¯ç”¨

# å°è¯•æ‰‹åŠ¨åº”ç”¨ eventlet monkey_patchï¼ˆå¦‚æœ eventlet å¯ç”¨ï¼‰
try:
    import eventlet
    eventlet.monkey_patch()
    print("âœ… eventlet monkey_patch å·²åº”ç”¨")
except ImportError:
    print("âš ï¸ eventlet æœªå®‰è£…ï¼Œå°†ä½¿ç”¨é»˜è®¤å¼‚æ­¥æ¨¡å¼")

from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from pathlib import Path
import sys
import json
import redis
import time
from loguru import logger

# æ·»åŠ è·¯å¾„ï¼ˆå¿…é¡»åœ¨å¯¼å…¥æ¨¡å—ä¹‹å‰ï¼‰
BASE_DIR = Path(__file__).resolve().parent.parent
SRC_DIR = BASE_DIR / "src"
sys.path.insert(0, str(BASE_DIR))
sys.path.insert(0, str(SRC_DIR))

# ç°åœ¨å¯ä»¥å¯¼å…¥tradingæ¨¡å—
from src.trading.api.order_engine import OrderEngine

# å®šä¹‰æ—¥å¿—ç›®å½•
LOG_DIR = BASE_DIR / "logs"
LOG_DIR.mkdir(exist_ok=True)

# å®šä¹‰æ ‡æ³¨ç›®å½•
ANNOTATION_DIR = BASE_DIR / "data" / "annotations"
ANNOTATION_DIR.mkdir(parents=True, exist_ok=True)

# âŒ å·²åˆ é™¤ï¼šMT5è¿æ¥å™¨ï¼ˆæ”¹ç”¨æ•°æ®é‡‡é›†å™¨ + Redisï¼‰
# âŒ å·²åˆ é™¤ï¼štick_streamerï¼ˆæ”¹ç”¨Redis Streamï¼‰

# åˆ›å»ºFlaskåº”ç”¨ï¼ˆå‰ç«¯ä½¿ç”¨ç‹¬ç«‹çš„Viteå¼€å‘æœåŠ¡å™¨ï¼Œä¸éœ€è¦Flaskæä¾›é™æ€æ–‡ä»¶ï¼‰
app = Flask(__name__)

# è®¾ç½®å¯†é’¥
app.config['SECRET_KEY'] = "**********"

# å¯ç”¨CORS
CORS(app)

# æ­£ç¡®åˆå§‹åŒ–SocketIO - åœ¨æ‰€æœ‰è£…é¥°å™¨ä¹‹å‰
# ã€å…³é”®ã€‘å…¼å®¹ socket.io-client 4.xï¼šä½¿ç”¨ python-socketio 5.x çš„é»˜è®¤é…ç½®
# Flask-SocketIO 5.5.1 + python-socketio 5.14.3 æ”¯æŒ Socket.IO åè®® v4
# ã€ä¿®å¤ã€‘ä½¿ç”¨ eventlet å¼‚æ­¥æ¨¡å¼ï¼Œç¡®ä¿ WebSocket è¿æ¥ç¨³å®š
# ğŸ”´ ä¿®å¤ï¼šå¦‚æœ eventlet ä¸å¯ç”¨ï¼Œä½¿ç”¨ threading æ¨¡å¼
try:
    import eventlet
    async_mode = 'eventlet'
except ImportError:
    async_mode = 'threading'
    logger.warning("eventlet æœªå®‰è£…ï¼Œä½¿ç”¨ threading æ¨¡å¼ï¼ˆæ€§èƒ½è¾ƒä½ï¼‰")

socketio = SocketIO(
    app, 
    cors_allowed_origins="*",  # å…è®¸æ‰€æœ‰æ¥æºï¼ˆå¼€å‘ç¯å¢ƒï¼‰
    logger=False,  # ç¦ç”¨SocketIOæ—¥å¿—
    engineio_logger=False,
    ping_timeout=120,     # ğŸ”´ ä¿®å¤ï¼šå¢åŠ å¿ƒè·³è¶…æ—¶æ—¶é—´åˆ°120ç§’ï¼Œç»™å‰ç«¯å’Œç½‘ç»œæ›´å¤šå®½é™ï¼Œé¿å…ping timeout
    ping_interval=25,      # ğŸ”´ ä¿®å¤ï¼šä½¿ç”¨æ ‡å‡†å¿ƒè·³é—´éš”25ç§’ï¼ˆä¸ping_timeouté…åˆï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ—¶é—´å“åº”ï¼‰
    async_mode=async_mode,  # è‡ªåŠ¨é€‰æ‹©å¼‚æ­¥æ¨¡å¼
    allow_upgrades=True,  # å…è®¸åè®®å‡çº§
    transports=['polling', 'websocket'],  # ä¼˜å…ˆ pollingï¼Œæ›´ç¨³å®š
    max_http_buffer_size=1e6,  # ğŸ”´ ä¼˜åŒ–ï¼šå¢åŠ ç¼“å†²åŒºå¤§å°ï¼Œé¿å…å¤§æ•°æ®åŒ…ä¸¢å¤±
    cors_credentials=True,  # ğŸ”´ ä¼˜åŒ–ï¼šå…è®¸è·¨åŸŸå‡­è¯
)

# ã€Redisæœ€ä½³å®è·µã€‘ä½¿ç”¨è¿æ¥æ± ï¼Œæé«˜å¹¶å‘æ€§èƒ½
redis_pool = redis.ConnectionPool(
    host='localhost',
    port=6379,
    db=0,
    decode_responses=True,
    max_connections=10,  # è¿æ¥æ± å¤§å°
    socket_timeout=1.0,  # ğŸ”´ ä¿®å¤ï¼šè®¾ç½®socketè¶…æ—¶ï¼Œé¿å…é˜»å¡
    socket_connect_timeout=1.0  # è¿æ¥è¶…æ—¶
)
redis_client = redis.Redis(connection_pool=redis_pool)

# å…¨å±€å˜é‡
model = None
model_loaded = False

# åˆå§‹åŒ–è®¢å•å¼•æ“
order_engine = OrderEngine()

# ğŸ”´ å®‰å…¨æœºåˆ¶ï¼šå¯¼å…¥ç¯å¢ƒæ£€æŸ¥æ¨¡å—ï¼ˆç”¨äºè®¢å•åˆ›å»ºæ¥å£ï¼‰
try:
    from src.trading.utils.env_check import (
        is_production_mode,
        require_production_mode,
        get_env_info,
        log_env_status
    )
    # å¯åŠ¨æ—¶è®°å½•ç¯å¢ƒçŠ¶æ€
    log_env_status()
except ImportError:
    logger.warning("âš ï¸ æ— æ³•å¯¼å…¥ç¯å¢ƒæ£€æŸ¥æ¨¡å—ï¼Œè®¢å•åˆ›å»ºæ¥å£å°†ä¸è¿›è¡Œç¯å¢ƒæ£€æŸ¥")
    def is_production_mode():
        return True  # é™çº§ï¼šå…è®¸äº¤æ˜“
    def require_production_mode(func_name: str = "åˆ›å»ºè®¢å•"):
        return True
    def get_env_info():
        return {'env': 'UNKNOWN', 'is_production': True}

# âŒ æ—§çš„tick_streameræ¨¡å—å·²åˆ é™¤ï¼Œç°åœ¨ä½¿ç”¨Redis Streamæ–¹æ¡ˆ


# ==================== å·¥å…·å‡½æ•° ====================

def load_model(model_path: str = None):
    """åŠ è½½AIæ¨¡å‹ï¼ˆæš‚æ—¶ç¦ç”¨ï¼‰"""
    global model, model_loaded
    logger.warning("AIæ¨¡å‹åŠ è½½å·²ç¦ç”¨ï¼Œä»…æä¾›æ•°æ®API")
    model_loaded = False
    return False


def _fill_missing_klines(klines: list, timeframe: str, minutes_per_kline: int) -> list:
    """
    å¡«å……ç¼ºå¤±çš„Kçº¿ï¼ˆMT5å®˜æ–¹æœ€ä½³å®è·µï¼‰
    
    ã€è¡¥ç©ºç­–ç•¥ã€‘
    1. æ£€æµ‹æ—¶é—´æˆ³è¿ç»­æ€§
    2. å¦‚æœå‘ç°ç©ºç¼ºï¼Œç”¨å‰ä¸€æ ¹Kçº¿çš„æ”¶ç›˜ä»·å¡«å……
    3. å¡«å……çš„Kçº¿ï¼šopen=close=high=low=å‰ä¸€æ ¹æ”¶ç›˜ä»·ï¼Œvolume=0
    
    ã€å®˜æ–¹è¦æ±‚ã€‘
    - Lightweight Chartsè¦æ±‚ï¼šæ—¶é—´æˆ³å¿…é¡»è¿ç»­ï¼Œä¸èƒ½æœ‰ç¼ºå¤±
    - å¦‚æœå¸‚åœºä¼‘å¸‚ï¼Œä¹Ÿåº”è¯¥å¡«å……ï¼ˆç”¨å‰ä¸€æ ¹æ”¶ç›˜ä»·ï¼‰
    
    Args:
        klines: Kçº¿åˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
        timeframe: æ—¶é—´å‘¨æœŸï¼ˆ'1m', '5m'ç­‰ï¼‰
        minutes_per_kline: æ¯æ ¹Kçº¿çš„åˆ†é’Ÿæ•°
        
    Returns:
        å¡«å……åçš„Kçº¿åˆ—è¡¨
    """
    if len(klines) < 2:
        return klines
    
    filled_klines = [klines[0]]  # ç¬¬ä¸€æ ¹Kçº¿
    
    for i in range(1, len(klines)):
        prev_kline = klines[i - 1]
        current_kline = klines[i]
        
        prev_time = prev_kline.get('time', 0)
        current_time = current_kline.get('time', 0)
        
        # è®¡ç®—æœŸæœ›çš„æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰
        expected_interval = minutes_per_kline * 60
        
        # è®¡ç®—å®é™…æ—¶é—´é—´éš”
        actual_interval = current_time - prev_time
        
        # å¦‚æœæ—¶é—´é—´éš”å¤§äºæœŸæœ›é—´éš”ï¼Œè¯´æ˜æœ‰ç¼ºå¤±
        # ğŸ”´ ä¿®å¤ï¼šå…è®¸1ç§’çš„å®¹å·®ï¼Œé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
        if actual_interval > expected_interval + 1:  # å…è®¸1ç§’å®¹å·®
            # ğŸ”´ ä¿®å¤ï¼šæ›´ç²¾ç¡®çš„ç¼ºå¤±Kçº¿æ•°é‡è®¡ç®—
            # è®¡ç®—ç†è®ºä¸Šåº”è¯¥æœ‰çš„Kçº¿æ•°é‡ï¼Œå¹¶å‡å»å·²æœ‰çš„1æ ¹ï¼ˆå½“å‰Kçº¿ï¼‰
            # ä¾‹å¦‚ï¼šé—´éš”120sï¼ŒæœŸæœ›60sï¼Œåº”è¯¥æœ‰2æ ¹ï¼Œç¼ºå¤±1æ ¹
            # ä¾‹å¦‚ï¼šé—´éš”180sï¼ŒæœŸæœ›60sï¼Œåº”è¯¥æœ‰3æ ¹ï¼Œç¼ºå¤±2æ ¹
            kline_count_in_gap = actual_interval // expected_interval
            missing_count = kline_count_in_gap - 1  # åº”è¯¥æœ‰çš„æ•°é‡ - 1 (å½“å‰Kçº¿)
            
            # å¡«å……ç¼ºå¤±çš„Kçº¿
            prev_close = prev_kline.get('close', 0)
            if prev_close > 0:  # ç¡®ä¿æœ‰æœ‰æ•ˆçš„æ”¶ç›˜ä»·
                for j in range(1, missing_count + 1):
                    missing_time = prev_time + (j * expected_interval)
                    
                    # åˆ›å»ºå¡«å……Kçº¿ï¼ˆä½¿ç”¨å‰ä¸€æ ¹æ”¶ç›˜ä»·ï¼‰
                    filled_kline = {
                        'time': missing_time,
                        'open': prev_close,
                        'high': prev_close,
                        'low': prev_close,
                        'close': prev_close,
                        'volume': 0,
                        'real_volume': 0,
                        'is_filled': True  # æ ‡è®°ä¸ºå¡«å……çš„Kçº¿
                    }
                    filled_klines.append(filled_kline)
                    from datetime import datetime as dt
                    logger.info(f"ğŸ”§ å¡«å……ç¼ºå¤±Kçº¿: time={missing_time} ({dt.fromtimestamp(missing_time).strftime('%Y-%m-%d %H:%M:%S')}), price={prev_close:.2f}, é—´éš”={actual_interval}ç§’")
            else:
                logger.warning(f"âš ï¸ æ— æ³•å¡«å……ç¼ºå¤±Kçº¿: å‰ä¸€æ ¹æ”¶ç›˜ä»·ä¸º0, time={prev_time}")
        
        # æ·»åŠ å½“å‰Kçº¿
        filled_klines.append(current_kline)
    
    return filled_klines


def format_kline_for_frontend(kline: dict) -> dict:
    """
    å°†MT5åŸå§‹æ ¼å¼è½¬æ¢ä¸ºå‰ç«¯å›¾è¡¨æ ¼å¼ï¼ˆECharts/lightweight-chartsï¼‰
    
    ã€æ•°æ®æ ¼å¼è¯´æ˜ã€‘
    - Rediså­˜å‚¨ï¼šMT5åŸå§‹æ ¼å¼ï¼ˆtime, open, high, low, close, volume, real_volumeï¼‰
    - å‰ç«¯å±•ç¤ºï¼šè½¬æ¢ä¸ºå›¾è¡¨åº“éœ€è¦çš„æ ¼å¼ï¼ˆå¯æ·»åŠ timezoneç­‰å­—æ®µï¼‰
    - ä¿ç•™is_filledæ ‡è®°ï¼šç”¨äºå‰ç«¯è¯†åˆ«å¡«å……çš„Kçº¿
    
    Args:
        kline: MT5åŸå§‹æ ¼å¼çš„Kçº¿æ•°æ®
        
    Returns:
        å‰ç«¯å›¾è¡¨æ ¼å¼çš„Kçº¿æ•°æ®
    """
    formatted = {
        'time': kline.get('time', 0),  # Unixæ—¶é—´æˆ³ï¼ˆç§’ï¼ŒUTCï¼‰
        'timezone': 'UTC',  # æ˜ç¡®æ ‡è¯†æ—¶åŒº
        'open': float(kline.get('open', 0)),
        'high': float(kline.get('high', 0)),
        'low': float(kline.get('low', 0)),
        'close': float(kline.get('close', 0)),
        'volume': int(kline.get('volume', 0)),  # tick_volume
        'real_volume': int(kline.get('real_volume', 0)),  # å®é™…æˆäº¤é‡
    }
    
    # ä¿ç•™is_filledæ ‡è®°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œç”¨äºå‰ç«¯è¯†åˆ«å¡«å……çš„Kçº¿
    if kline.get('is_filled', False):
        formatted['is_filled'] = True
    
    return formatted


def format_position_for_frontend(pos: dict) -> dict:
    """
    å°† MT5 åŸå§‹æ ¼å¼ï¼ˆä¸‹åˆ’çº¿å‘½åï¼‰çš„æŒä»“æ•°æ®è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼ï¼ˆé©¼å³°å‘½åï¼‰ã€‚
    
    è¿™æ˜¯æ‰€æœ‰å¯¹å¤–æ¥å£ï¼ˆHTTP API å’Œ WebSocketï¼‰çš„å”¯ä¸€è½¬æ¢å…¥å£ã€‚
    éµå¾ª"ä¸€æ¬¡è½¬æ¢ï¼Œå¤šå¤„ä½¿ç”¨"åŸåˆ™ï¼Œç¡®ä¿æ•°æ®æ ¼å¼ä¸€è‡´æ€§ã€‚
    
    ã€è®¾è®¡åŸåˆ™ã€‘
    1. å•ä¸€æ•°æ®æºï¼šæ‰€æœ‰æ¥å£å¿…é¡»è°ƒç”¨æ­¤å‡½æ•°
    2. æ¸…æ™°è¯­ä¹‰ï¼šä½¿ç”¨ CamelCase å’Œä¸šåŠ¡æ„ä¹‰çš„å­—æ®µå
    3. æ—¶é—´æ ‡å‡†åŒ–ï¼šç»Ÿä¸€ä½¿ç”¨æ¯«ç§’çº§ Unix æ—¶é—´æˆ³
    4. ç²¾åº¦æ§åˆ¶ï¼šæµ®ç‚¹æ•°ä¿ç•™åˆç†ç²¾åº¦ï¼Œé¿å…ç²¾åº¦é—®é¢˜
    
    Args:
        pos: MT5 åŸå§‹æ ¼å¼çš„æŒä»“æ•°æ®ï¼ˆåŒ…å« ticket, price_open, price_current ç­‰ï¼‰
        
    Returns:
        dict: å‰ç«¯æœŸæœ›çš„ç»Ÿä¸€æ ¼å¼æŒä»“æ•°æ®ï¼ˆCamelCase å‘½åï¼‰
    """
    import time
    
    # ç¡®ä¿æ—¶é—´æˆ³æ˜¯æ¯«ç§’çº§ï¼Œå‰ç«¯é€šå¸¸ä½¿ç”¨æ¯«ç§’
    current_time_ms = int(time.time() * 1000)
    
    # è·å–åŸºç¡€å­—æ®µ
    position_id = str(pos.get('ticket', 0))
    symbol = pos.get('symbol', 'BTCUSDm')
    position_type = pos.get('type', 0)  # 0=BUY/LONG, 1=SELL/SHORT
    
    # ä»·æ ¼å­—æ®µï¼ˆMT5 ä½¿ç”¨ä¸‹åˆ’çº¿å‘½åï¼‰
    price_open = float(pos.get('price_open', 0.0))
    price_current = float(pos.get('price_current', 0.0))
    volume = float(pos.get('volume', 0.0))
    
    # ç›ˆäºè®¡ç®—
    profit = float(pos.get('profit', 0.0))
    swap = float(pos.get('swap', 0.0))
    commission = float(pos.get('commission', 0.0))
    unrealized_pnl = profit + swap
    
    # æ­¢æŸæ­¢ç›ˆï¼ˆåªæœ‰å¤§äº0æ‰è¿”å›ï¼Œå¦åˆ™ä¸ºNoneï¼‰
    sl = float(pos.get('sl', 0.0))
    tp = float(pos.get('tp', 0.0))
    
    # æ—¶é—´å¤„ç†ï¼šMT5å¯èƒ½æä¾›æ¯«ç§’æ—¶é—´æˆ³(time_msc/time_update_msc)æˆ–ç§’æ—¶é—´æˆ³(time/time_update)
    time_msc = pos.get('time_msc')
    time_sec = pos.get('time', 0)
    opened_at = time_msc if time_msc else (time_sec * 1000 if time_sec > 0 else 0)
    
    time_update_msc = pos.get('time_update_msc')
    time_update_sec = pos.get('time_update', 0)
    updated_at = (time_update_msc if time_update_msc 
                  else (time_update_sec * 1000 if time_update_sec > 0 else current_time_ms))
    
    # æ„å»ºå‰ç«¯æ ¼å¼æ•°æ®ï¼ˆCamelCase å‘½åï¼‰
    frontend_position = {
        'positionId': position_id,
        'symbol': symbol,
        'side': 'LONG' if position_type == 0 else 'SHORT',
        'volume': volume,
        'entryPrice': price_open,
        'currentPrice': price_current,
        'unrealizedPnL': unrealized_pnl,
        'commission': commission,
        'stopLoss': sl if sl > 0 else None,
        'takeProfit': tp if tp > 0 else None,
        'openedAt': opened_at,
        'updatedAt': updated_at,
    }
    
    # æ¸…ç†æµ®ç‚¹æ•°ç²¾åº¦ï¼ˆä¿ç•™6ä½å°æ•°ï¼Œè¶³å¤Ÿç²¾åº¦ä¸”é¿å…ç²¾åº¦é—®é¢˜ï¼‰
    precision_fields = ['volume', 'entryPrice', 'currentPrice', 'unrealizedPnL', 'commission']
    for key in precision_fields:
        if key in frontend_position and frontend_position[key] is not None:
            frontend_position[key] = round(frontend_position[key], 6)
    
    return frontend_position


def get_kline_data_from_redis(symbol: str = 'BTCUSDm', timeframe: str = '1m', count: int = 100, format_for_frontend: bool = True):
    """
    è·å–Kçº¿æ•°æ®ï¼ˆä» Redis è¯»å–ï¼‰
    
    data_puller é€šè¿‡ gRPC StreamTicks æ¥æ”¶æ•°æ®å¹¶è®¡ç®— Kçº¿ï¼Œå­˜å‚¨åˆ° Redis
    è¿™é‡Œç›´æ¥ä» Redis è¯»å–ï¼Œæ€§èƒ½æœ€å¥½ï¼ˆ<5msï¼‰
    
    Args:
        symbol: äº¤æ˜“å¯¹ï¼Œå¦‚ 'BTCUSDm'
        timeframe: æ—¶é—´å‘¨æœŸï¼Œæ”¯æŒ '1m', '1h', '1d'
        count: è·å–æ•°é‡ï¼Œå¦‚æœä¸º-1æˆ–å¤§äºç­‰äºæ€»æ•°ï¼Œåˆ™è·å–å…¨éƒ¨æ•°æ®
        format_for_frontend: æ˜¯å¦æ ¼å¼åŒ–ä¸ºå‰ç«¯å›¾è¡¨æ ¼å¼ï¼ˆé»˜è®¤Trueï¼‰
    """
    try:
        key = f"kline:{symbol}:{timeframe}"
        
        # ğŸš€ ä¿®å¤ï¼šç¡®ä¿è·å–ç¬¬ä¸€ä¸ªKçº¿
        # å¦‚æœcountä¸º-1æˆ–éå¸¸å¤§ï¼Œä½¿ç”¨0åˆ°-1è·å–å…¨éƒ¨æ•°æ®
        if count == -1 or count >= 10000:
            klines = redis_client.zrange(key, 0, -1, withscores=False)
        else:
            # è·å–æœ€åcountæ¡æ•°æ®ï¼Œä½†ç¡®ä¿ä»ç¬¬ä¸€ä¸ªå¼€å§‹
            total_count = redis_client.zcard(key)
            if total_count == 0:
                logger.warning(f"Redisä¸­æ²¡æœ‰Kçº¿æ•°æ®: {key}ï¼Œè¯·ç¡®ä¿ data_puller æ­£åœ¨è¿è¡Œ")
                return []
            
            # å¦‚æœè¯·æ±‚çš„æ•°é‡å¤§äºç­‰äºæ€»æ•°ï¼Œè·å–å…¨éƒ¨
            if count >= total_count:
                # ğŸ”´ ä¿®å¤ï¼šé™åˆ¶æœ€å¤§æ•°é‡ï¼Œé¿å…ä¸€æ¬¡æ€§è¯»å–è¿‡å¤šæ•°æ®é˜»å¡äº‹ä»¶å¾ªç¯
                max_count = 10000  # æœ€å¤šè¯»å–10000æ¡
                if total_count > max_count:
                    klines = redis_client.zrange(key, -max_count, -1, withscores=False)
                else:
                    klines = redis_client.zrange(key, 0, -1, withscores=False)
            else:
                # è·å–æœ€åcountæ¡ï¼ˆåŒ…å«ç¬¬ä¸€ä¸ªKçº¿ï¼‰
                klines = redis_client.zrange(key, -count, -1, withscores=False)
        
        if not klines:
            logger.warning(f"Redisä¸­æ²¡æœ‰Kçº¿æ•°æ®: {key}ï¼Œè¯·ç¡®ä¿ data_puller æ­£åœ¨è¿è¡Œ")
            return []
        
        data = []
        for kline_json in klines:
            kline = json.loads(kline_json)
            if format_for_frontend:
                data.append(format_kline_for_frontend(kline))
            else:
                data.append(kline)
        
        if data:
            from datetime import datetime
            first_time = data[0]['time']
            last_time = data[-1]['time']
            first_dt = datetime.fromtimestamp(first_time)
            last_dt = datetime.fromtimestamp(last_time)
            logger.debug(f"ä»Redisè¯»å–åˆ° {len(data)} æ¡Kçº¿æ•°æ®ï¼Œç¬¬1æ¡æ—¶é—´: {first_time} ({first_dt.strftime('%Y-%m-%d %H:%M:%S')})ï¼Œæœ€åä¸€æ¡æ—¶é—´: {last_time} ({last_dt.strftime('%Y-%m-%d %H:%M:%S')})")
        
        return data
        
    except Exception as e:
        logger.error(f"ä»Redisè·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
        return []


def get_latest_tick_from_redis(symbol: str = 'BTCUSDm'):
    """
    è·å–æœ€æ–°TICKæ•°æ®ï¼ˆä» Redis è¯»å–ï¼‰
    
    data_puller é€šè¿‡ gRPC StreamTicks æ¥æ”¶æ•°æ®å¹¶å­˜å‚¨åˆ° Redis
    è¿™é‡Œç›´æ¥ä» Redis è¯»å–ï¼Œæ€§èƒ½æœ€å¥½ï¼ˆO(1)æŸ¥è¯¢ï¼Œ<0.1msï¼‰
    """
    try:
        key = f"tick:{symbol}:latest"
        tick_json = redis_client.get(key)
        if tick_json:
            return json.loads(tick_json)
        else:
            logger.warning(f"Redisä¸­æ²¡æœ‰æœ€æ–°TICKæ•°æ®: {key}ï¼Œè¯·ç¡®ä¿ data_puller æ­£åœ¨è¿è¡Œ")
        return None
    except Exception as e:
        logger.error(f"ä»Redisè·å–æœ€æ–°TICKå¤±è´¥: {e}")
        return None


def get_tick_history_from_redis(symbol: str = 'BTCUSDm', count: int = 500):
    """
    è·å–å†å²TICKæ•°æ®ï¼ˆä» Redis è¯»å–ï¼‰
    
    data_puller é€šè¿‡ gRPC StreamTicks æ¥æ”¶æ•°æ®å¹¶å­˜å‚¨åˆ° Redis
    è¿™é‡Œç›´æ¥ä» Redis è¯»å–ï¼Œæ€§èƒ½æœ€å¥½ï¼ˆ<1msï¼‰
    
    ã€æ”¯æŒæ¯«ç§’çº§ç²¾åº¦ã€‘ä¿ç•™å®Œæ•´çš„time_mscï¼Œæ— éœ€å»é‡
    """
    try:
        key = f"tick:{symbol}:realtime"
        ticks = redis_client.zrange(key, -count, -1, withscores=True)
        
        if not ticks:
            logger.warning(f"Redisä¸­æ²¡æœ‰TICKå†å²æ•°æ®: {key}ï¼Œè¯·ç¡®ä¿ data_puller æ­£åœ¨è¿è¡Œ")
            return []
        
        result = []
        for tick_json, score in ticks:
            tick = json.loads(tick_json)
            result.append({
                'time': int(score),  # Unixæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
                'time_msc': tick.get('time_msc', int(score * 1000)),  # æ¯«ç§’çº§
                'bid': tick['bid'],
                'ask': tick['ask'],
                'last': tick.get('last', 0.0),
                'volume': tick.get('volume', 0),
            })
        
        logger.debug(f"ä»Redisè¯»å– {len(result)} æ¡å†å²TICKæ•°æ®ï¼ˆæ¯«ç§’çº§ï¼‰")
        return result
        
    except Exception as e:
        logger.error(f"ä»Redisè·å–TICKå†å²æ•°æ®å¤±è´¥: {e}")
        return []


# ==================== REST API ====================

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return send_from_directory(app.template_folder, 'index.html')


@app.route('/api/health', methods=['GET'])
def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹ - Readiness Probe
    
    ã€è®¾è®¡åŸåˆ™ã€‘
    1. å¿«é€Ÿå¤±è´¥ï¼šæ‰€æœ‰ä¾èµ–æ£€æŸ¥å¿…é¡»åœ¨0.5ç§’å†…å®Œæˆ
    2. ä¾èµ–è§£è€¦ï¼šä¾èµ–å¤±è´¥ä¸å½±å“æœåŠ¡å­˜æ´»çŠ¶æ€
    3. çŠ¶æ€æ˜ç¡®ï¼šè¿”å›è¯¦ç»†çš„ä¾èµ–å¥åº·çŠ¶æ€
    
    ã€çŠ¶æ€è¯´æ˜ã€‘
    - status='ok': æœåŠ¡æœ¬èº«è¿è¡Œæ­£å¸¸ï¼ˆLivenessï¼‰
    - dependencies: å„ä¾èµ–é¡¹çš„è¯¦ç»†çŠ¶æ€ï¼ˆReadinessï¼‰
    """
    # ğŸ”´ æœ€ä½³å®è·µï¼šä½¿ç”¨ThreadPoolExecutoråŒ…è£…Redis pingï¼Œè®¾ç½®ä¸¥æ ¼è¶…æ—¶
    redis_status = "Error"
    try:
        import concurrent.futures
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œè®¾ç½®0.5ç§’è¶…æ—¶ï¼ˆå¥åº·æ£€æŸ¥å¿…é¡»å¿«é€Ÿï¼‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(redis_client.ping)
            try:
                redis_connected = future.result(timeout=0.5)  # 0.5ç§’è¶…æ—¶
                redis_status = "OK" if redis_connected else "Error"
            except concurrent.futures.TimeoutError:
                logger.debug("å¥åº·æ£€æŸ¥: Redis pingè¶…æ—¶ï¼ˆ0.5ç§’ï¼‰")
                redis_status = "Timeout"
    except Exception as e:
        logger.debug(f"å¥åº·æ£€æŸ¥: Redis pingå¤±è´¥: {e}")
        redis_status = "Error"
    
    # ğŸ”´ æœ€ä½³å®è·µï¼šå³ä½¿ä¾èµ–å¤±è´¥ï¼ŒæœåŠ¡ä»è¿”å›200ï¼ˆReadiness Probeï¼‰
    # åªæœ‰æœåŠ¡æœ¬èº«å´©æºƒæ—¶æ‰æ— æ³•å“åº”ï¼ˆLiveness Probeï¼‰
    return jsonify({
        'status': 'ok',  # æœåŠ¡å­˜æ´»çŠ¶æ€ï¼ˆLivenessï¼‰
        'service': 'API Server',
        'dependencies': {
            'redis': redis_status,  # ä¾èµ–å¥åº·çŠ¶æ€ï¼ˆReadinessï¼‰
            'model_loaded': model_loaded
        }
    }), 200  # æ˜ç¡®è¿”å›200çŠ¶æ€ç 


@app.route('/api/klines', methods=['GET'])
def get_klines():
    """
    è·å–Kçº¿æ•°æ®ï¼ˆMT5å®˜æ–¹æœ€ä½³å®è·µï¼‰
    
    ã€æ•°æ®è·å–ç­–ç•¥ã€‘
    1. å†å²Kçº¿ï¼šç›´æ¥ä»MT5è·å–ï¼ˆä½¿ç”¨gRPC GetKlinesï¼ŒMT5å®˜æ–¹APIï¼‰
    2. å½“å‰Kçº¿ï¼šä»Redisè¯»å–ï¼ˆç”¨TICKå®æ—¶ç”Ÿæˆï¼Œå› ä¸ºå½“å‰Kçº¿è¿˜æœªé—­åˆï¼‰
    3. åˆå¹¶è¿”å›ï¼šå†å²Kçº¿ + å½“å‰Kçº¿
    
    å‚æ•°:
        symbol: äº¤æ˜“å¯¹ (é»˜è®¤: BTCUSDm)
        timeframe: æ—¶é—´å‘¨æœŸï¼Œæ”¯æŒ '1m', '1h', '1d' (é»˜è®¤: 1m)
        count: æ•°é‡æˆ–'all' (é»˜è®¤: all)
    """
    try:
        from datetime import datetime, timedelta
        
        symbol = request.args.get('symbol', 'BTCUSDm')
        timeframe = request.args.get('timeframe', '1m')
        count = request.args.get('count', 'all')  # æ”¯æŒ'all'è·å–å…¨éƒ¨æ•°æ®
        
        # è°ƒè¯•ï¼šæ‰“å°å®Œæ•´è¯·æ±‚
        referer = request.headers.get('Referer', 'unknown')
        all_args = dict(request.args)
        logger.info(f"ğŸ“¥ APIè¯·æ±‚ - æ‰€æœ‰å‚æ•°:{all_args}, æ¥æº:{referer}")
        logger.info(f"ğŸ“¥ è§£æå - symbol:{symbol}, timeframe:{timeframe}, count:{count}")
        
        # è§£æcountå‚æ•°
        if count == 'all':
            count = 2880  # é»˜è®¤è·å–2å¤©M1æ•°æ®ï¼ˆ2880æ ¹ï¼‰
        else:
            count = int(count)
        
        # å®šä¹‰æ—¶é—´å‘¨æœŸæ˜ å°„ï¼ˆä¾›åç»­ä½¿ç”¨ï¼‰
        timeframe_minutes = {
            '1m': 1, '5m': 5, '15m': 15, '30m': 30,
            '1h': 60, '4h': 240, '1d': 1440
        }
        minutes_per_kline = timeframe_minutes.get(timeframe, 1)
        
        all_klines = []
        
        # ğŸš€ ä¼˜åŒ–ç­–ç•¥ï¼šä¼˜å…ˆä»Redisè¯»å–ï¼ˆå¿«é€Ÿå“åº”ï¼‰ï¼Œç„¶åå¼‚æ­¥å°è¯•ä»MT5è·å–æ›´æ–°
        # è¿™æ ·å¯ä»¥é¿å…gRPCé˜»å¡å¯¼è‡´APIè¶…æ—¶
        logger.info("ä¼˜å…ˆä»Redisè¯»å–Kçº¿æ•°æ®ï¼ˆå¿«é€Ÿå“åº”ï¼‰")
        redis_klines = get_kline_data_from_redis(symbol, timeframe, count, format_for_frontend=False)
        if redis_klines:
            all_klines = redis_klines
            logger.info(f"âœ“ ä»Redisè·å–åˆ° {len(all_klines)} æ ¹Kçº¿")
        
        # ğŸš€ ç­–ç•¥1: å°è¯•ä»MT5è·å–å†å²Kçº¿ï¼ˆåå°å¼‚æ­¥ï¼Œä¸é˜»å¡å“åº”ï¼‰
        # æ³¨æ„ï¼šå¦‚æœRediså·²æœ‰æ•°æ®ï¼ŒMT5è·å–å¤±è´¥ä¸å½±å“å“åº”
        try:
            from src.trading.services.grpc_trade_client import get_grpc_client, is_grpc_available
            import concurrent.futures
            
            if is_grpc_available() and len(all_klines) < count * 0.5:  # å¦‚æœRedisæ•°æ®ä¸è¶³ï¼Œæ‰å°è¯•MT5
                try:
                    client = get_grpc_client()
                    
                    # è®¡ç®—æ—¶é—´èŒƒå›´
                    import pytz
                    timezone = pytz.timezone("Etc/UTC")
                    to_dt = datetime.now(timezone)
                    from_dt = to_dt - timedelta(minutes=count * minutes_per_kline)
                    
                    to_time = int(to_dt.timestamp())
                    from_time = int(from_dt.timestamp())
                    
                    logger.debug(f"å°è¯•ä»MT5è·å–å†å²Kçº¿: {symbol} {timeframe}, ä» {from_time} åˆ° {to_time}")
                    
                    # ğŸ”´ ä¿®å¤ï¼šä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œè®¾ç½®è¶…æ—¶ä¿æŠ¤ï¼ˆ2ç§’ï¼‰ï¼Œé¿å…é˜»å¡API
                    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                        future = executor.submit(
                            client.get_klines,
                            symbol=symbol,
                            timeframe=timeframe,
                            from_time=from_time,
                            to_time=to_time,
                            count=count
                        )
                        try:
                            result = future.result(timeout=2.0)
                        except concurrent.futures.TimeoutError:
                            logger.warning(f"ä»MT5è·å–å†å²Kçº¿è¶…æ—¶ï¼ˆ2ç§’ï¼‰ï¼Œä½¿ç”¨Redisæ•°æ®")
                            result = None
                    
                    if result and result.get('success') and result.get('klines'):
                        mt5_klines = result['klines']
                        logger.info(f"âœ“ ä»MT5è·å–åˆ° {len(mt5_klines)} æ ¹å†å²Kçº¿ï¼Œå°†åˆå¹¶åˆ°ç°æœ‰æ•°æ®")
                        
                        # åˆå¹¶MT5æ•°æ®ï¼ˆå»é‡ï¼‰
                        mt5_dict = {int(k['time']): {
                            'time': int(k['time']),
                            'open': float(k['open']),
                            'high': float(k['high']),
                            'low': float(k['low']),
                            'close': float(k['close']),
                            'volume': int(k.get('volume', k.get('tick_volume', 0))),
                            'real_volume': int(k.get('real_volume', 0))
                        } for k in mt5_klines}
                        
                        # åˆå¹¶åˆ°all_klinesï¼ˆMT5æ•°æ®ä¼˜å…ˆï¼‰
                        existing_times = {k.get('time', 0) for k in all_klines}
                        for time_key, kline_data in mt5_dict.items():
                            if time_key not in existing_times:
                                all_klines.append(kline_data)
                except Exception as grpc_error:
                    logger.warning(f"ä»MT5è·å–Kçº¿è¶…æ—¶æˆ–å¤±è´¥: {grpc_error}ï¼Œä½¿ç”¨Redisæ•°æ®")
        except Exception as e:
            logger.debug(f"MT5è·å–Kçº¿å¼‚å¸¸ï¼ˆéå…³é”®ï¼‰: {e}ï¼Œä½¿ç”¨Redisæ•°æ®")
        
        # ğŸš€ ç­–ç•¥2: å¦‚æœRedisä¹Ÿæ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºæ•°ç»„ï¼ˆå¿«é€Ÿå“åº”ï¼Œä¸é˜»å¡ï¼‰
        if len(all_klines) == 0:
            logger.warning(f"Redisä¸­æ²¡æœ‰Kçº¿æ•°æ®: {symbol} {timeframe}ï¼Œè¿”å›ç©ºæ•°ç»„")
            return jsonify({
                'success': True,
                'data': [],
                'count': 0,
                'source': 'Redis',
                'filled_count': 0,
                'message': f'æš‚æ— Kçº¿æ•°æ®ï¼Œè¯·ç¡®ä¿ data_puller å’Œ L2 ç­–ç•¥æ ¸å¿ƒæ­£åœ¨è¿è¡Œ'
            })
        
        # ğŸš€ ç­–ç•¥3: è·å–å½“å‰æœªé—­åˆçš„Kçº¿ï¼ˆä»Rediså¿«ç…§ï¼Œç”¨TICKå®æ—¶ç”Ÿæˆï¼‰
        # MT5å®˜æ–¹è¯´æ˜ï¼šå½“å‰Kçº¿è¿˜æœªé—­åˆï¼ŒMT5çš„copy_rates_rangeä¸åŒ…å«
        # å¿…é¡»ç”¨TICKå®æ—¶ç”Ÿæˆå½“å‰Kçº¿ï¼Œä»L2ç­–ç•¥æ ¸å¿ƒæ¨é€çš„current_klineå¿«ç…§è·å–
        try:
            current_kline_key = f"current_kline:{symbol}:{timeframe}:snapshot"
            current_kline_json = redis_client.get(current_kline_key)
            if current_kline_json:
                current_kline = json.loads(current_kline_json)
                current_time = current_kline.get('time', 0)
                
                # æ£€æŸ¥æ˜¯å¦å·²åŒ…å«åœ¨all_klinesä¸­ï¼ˆå»é‡ï¼‰
                if not all_klines or all_klines[-1].get('time') != current_time:
                    all_klines.append(current_kline)
                    logger.debug(f"æ·»åŠ å½“å‰æœªé—­åˆKçº¿: time={current_time}, close={current_kline.get('close', 0):.2f}")
        except Exception as e:
            logger.debug(f"è·å–å½“å‰Kçº¿å¤±è´¥ï¼ˆéå…³é”®ï¼‰: {e}")
        
        # ğŸš€ æ•°æ®å»é‡å’Œè¡¥ç©ºï¼ˆæŒ‰å®˜æ–¹è¦æ±‚ï¼‰
        if all_klines:
            # 1. å»é‡ï¼šæŒ‰æ—¶é—´æˆ³å»é‡ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®ï¼ˆMT5å®˜æ–¹è¦æ±‚ï¼šæ—¶é—´æˆ³å¿…é¡»å”¯ä¸€ï¼‰
            seen_times = {}
            for kline in all_klines:
                kline_time = kline.get('time', 0)
                if kline_time > 0:  # è¿‡æ»¤æ— æ•ˆæ—¶é—´æˆ³
                    # ä¿ç•™æœ€æ–°çš„æ•°æ®ï¼ˆåé¢çš„æ•°æ®è¦†ç›–å‰é¢çš„ï¼Œç¡®ä¿æ—¶é—´æˆ³å”¯ä¸€ï¼‰
                    seen_times[kline_time] = kline
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æ—¶é—´æ’åºï¼ˆLightweight Chartsè¦æ±‚ï¼šå¿…é¡»æ’åºï¼‰
            unique_klines = list(seen_times.values())
            unique_klines.sort(key=lambda x: x.get('time', 0))
            
            # 2. è¡¥ç©ºï¼šæ£€æµ‹å¹¶å¡«å……ç¼ºå¤±çš„Kçº¿ï¼ˆMT5å®˜æ–¹æœ€ä½³å®è·µï¼‰
            # ğŸ”´ å…³é”®ï¼šå¿…é¡»åœ¨å‘é€ç»™å‰ç«¯ä¹‹å‰è¡¥ç©ºï¼Œç¡®ä¿å›¾è¡¨è¿ç»­æ€§
            if len(unique_klines) > 1:
                filled_klines = _fill_missing_klines(
                    unique_klines, 
                    timeframe, 
                    minutes_per_kline
                )
                # ç»Ÿè®¡å¡«å……çš„Kçº¿æ•°é‡
                filled_count = sum(1 for k in filled_klines if k.get('is_filled', False))
                if filled_count > 0:
                    logger.info(f"âœ… è¡¥ç©ºå®Œæˆ: å¡«å……äº† {filled_count} æ ¹ç¼ºå¤±Kçº¿")
                unique_klines = filled_klines
            
            # 3. é™åˆ¶æ•°é‡
            if count > 0 and len(unique_klines) > count:
                unique_klines = unique_klines[-count:]
            
            all_klines = unique_klines
        
        # æ ¼å¼åŒ–ä¸ºå‰ç«¯æ ¼å¼
        formatted_klines = []
        for kline in all_klines:
            formatted_klines.append(format_kline_for_frontend(kline))
        
        # è¾“å‡ºå‘é€ç»™å‰ç«¯çš„æ•°æ®
        if formatted_klines:
            first = formatted_klines[0]
            last = formatted_klines[-1]
            first_dt = datetime.fromtimestamp(first['time'])
            last_dt = datetime.fromtimestamp(last['time'])
            logger.info(f"ğŸ“¤ APIè¿”å› {len(formatted_klines)} æ¡Kçº¿æ•°æ®:")
            logger.info(f"   ç¬¬1æ¡: {first['time']} ({first_dt.strftime('%Y-%m-%d %H:%M:%S')}) - æ”¶ç›˜:{first['close']:.2f}")
            logger.info(f"   æœ€å: {last['time']} ({last_dt.strftime('%Y-%m-%d %H:%M:%S')}) - æ”¶ç›˜:{last['close']:.2f}")
        else:
            logger.warning("ğŸ“¤ APIè¿”å›0æ¡æ•°æ®")
        
        # åˆ¤æ–­æ•°æ®æ¥æºå’Œç»Ÿè®¡ä¿¡æ¯
        # å¦‚æœä»MT5è·å–åˆ°äº†æ•°æ®ï¼Œæ ‡è®°ä¸ºMT5ï¼›å¦åˆ™ä¸ºRedis
        data_source = 'MT5' if len(all_klines) > 0 and any(
            k.get('time', 0) > 0 and not k.get('is_filled', False) 
            for k in all_klines
        ) else 'Redis'
        
        # ç»Ÿè®¡å¡«å……çš„Kçº¿æ•°é‡
        filled_count = sum(1 for k in all_klines if k.get('is_filled', False))
        if filled_count > 0:
            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: æ€»Kçº¿={len(all_klines)}, å¡«å……Kçº¿={filled_count}, æ¥æº={data_source}")
        
        return jsonify({
            'success': True,
            'data': formatted_klines,
            'count': len(formatted_klines),
            'source': data_source,
            'filled_count': filled_count  # å¡«å……çš„Kçº¿æ•°é‡ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
        })
    except Exception as e:
        logger.error(f"âŒ /api/klines è·¯ç”±é”™è¯¯: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'data': [],
            'count': 0,
            'error': str(e)
        }), 500


@app.route('/api/patterns', methods=['GET'])
def get_patterns():
    """è·å–å½¢æ€åˆ—è¡¨"""
    # ä¸´æ—¶å®šä¹‰å½¢æ€ç±»åˆ«
    PATTERN_CATEGORIES = {
        0: {"name": "æ— æ˜æ˜¾å½¢æ€", "english": "no_pattern", "type": "neutral"},
        1: {"name": "å¤´è‚©é¡¶", "english": "head_shoulders_top", "type": "bearish"},
        2: {"name": "å¤´è‚©åº•", "english": "head_shoulders_bottom", "type": "bullish"},
    }
    return jsonify({
        'success': True,
        'patterns': PATTERN_CATEGORIES
    })


@app.route('/api/annotations', methods=['GET'])
def get_annotations():
    """è·å–æ ‡æ³¨åˆ—è¡¨"""
    try:
        annotation_files = list(ANNOTATION_DIR.glob("*.json"))
        
        annotations = []
        for ann_file in annotation_files:
            with open(ann_file, 'r', encoding='utf-8') as f:
                ann_data = json.load(f)
                ann_data['id'] = ann_file.stem
                annotations.append(ann_data)
        
        return jsonify({
            'success': True,
            'data': annotations,
            'count': len(annotations)
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/annotations', methods=['POST'])
def save_annotation():
    """ä¿å­˜æ ‡æ³¨"""
    try:
        data = request.json
        
        # ç”Ÿæˆæ–‡ä»¶å
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        pattern_id = data['pattern_id']
        filename = f"annotation_{timestamp}_pattern_{pattern_id}.json"
        
        # ä¿å­˜æ–‡ä»¶
        annotation_path = ANNOTATION_DIR / filename
        with open(annotation_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ“ æ ‡æ³¨å·²ä¿å­˜: {annotation_path}")
        
        return jsonify({
            'success': True,
            'message': 'æ ‡æ³¨ä¿å­˜æˆåŠŸ',
            'id': filename.replace('.json', '')
        })
    except Exception as e:
        logger.error(f"ä¿å­˜æ ‡æ³¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/predict', methods=['POST'])
def predict_pattern():
    """å½¢æ€è¯†åˆ«é¢„æµ‹"""
    global model, model_loaded
    
    if not model_loaded:
        return jsonify({
            'success': False,
            'error': 'æ¨¡å‹æœªåŠ è½½'
        }), 503
    
    try:
        # é¢„æµ‹åŠŸèƒ½å·²ç§»é™¤ï¼Œæ­¤ç«¯ç‚¹ä¿ç•™ç”¨äºå…¼å®¹æ€§
        # å®é™…é¢„æµ‹åŠŸèƒ½åº”ç”±ä¸“é—¨çš„é¢„æµ‹æœåŠ¡æä¾›
        return jsonify({
            'success': False,
            'error': 'é¢„æµ‹åŠŸèƒ½æš‚æœªå®ç°ï¼Œè¯·ä½¿ç”¨å…¶ä»–é¢„æµ‹æœåŠ¡'
        }), 501
    except Exception as e:
        logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ==================== WebSocket ====================

# å®¢æˆ·ç«¯è¿æ¥çŠ¶æ€ç®¡ç†
clients = {}

@socketio.on('connect')
def handle_connect():
    """å®¢æˆ·ç«¯è¿æ¥"""
    # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆlast_time=0ï¼Œæ¥æ”¶æ‰€æœ‰å®æ—¶tickï¼‰
    clients[request.sid] = {
        'last_time': 0,
        'connected_at': time.time()  # è®°å½•è¿æ¥æ—¶é—´
    }
    logger.info(f"âœ“ å®¢æˆ·ç«¯å·²è¿æ¥: {request.sid} | å½“å‰å®¢æˆ·ç«¯æ•°: {len(clients)}")
    
    emit('connected', {'message': 'è¿æ¥æˆåŠŸ'})
    
    # ã€æœ€ä½³å®è·µï¼šåƒKçº¿ä¸€æ ·ï¼Œä»Sorted Setè¯»å–å†å²æ•°æ® + Streamè¯»å–å®æ—¶æ•°æ®ã€‘
    # åŠ è½½æœ€è¿‘5000ä¸ªTICKç‚¹ï¼ˆçº¦1-2å°æ—¶çš„æ•°æ®ï¼Œå–å†³äºå¸‚åœºæ´»è·ƒåº¦ï¼‰
    try:
        # ã€å…¼å®¹æ€§ã€‘ä¼˜å…ˆä½¿ç”¨æ–°keyï¼Œå¦‚æœä¸ºç©ºåˆ™å°è¯•æ—§key
        tick_key_new = 'tick:BTCUSDm'  # âœ… æ ‡å‡†æ ¼å¼
        tick_key_old = 'tick:BTCUSDm:realtime'  # æ—§æ ¼å¼ï¼ˆå…¼å®¹ï¼‰
        
        # 1. ä»Sorted Setè¯»å–æœ€è¿‘5000æ¡å†å²TICK
        tick_data = redis_client.zrevrange(tick_key_new, 0, 4999, withscores=True)
        
        # å¦‚æœæ–°keyæ²¡æ•°æ®ï¼Œå°è¯•æ—§keyï¼ˆå…¼å®¹å†å²æ•°æ®ï¼‰
        if not tick_data:
            tick_data = redis_client.zrevrange(tick_key_old, 0, 4999, withscores=True)
            if tick_data:
                logger.info(f"ä»æ—§keyè¯»å–æ•°æ®: {tick_key_old}")
        
        if tick_data:
            # åè½¬é¡ºåºï¼ˆä»æ—§åˆ°æ–°ï¼‰
            tick_data.reverse()
            
            # è§£ætickæ•°æ®
            ticks = []
            for tick_json, score in tick_data:
                try:
                    if isinstance(tick_json, bytes):
                        tick_json = tick_json.decode('utf-8')
                    tick = json.loads(tick_json)
                    ticks.append(tick)
                except Exception as parse_error:
                    logger.warning(f"è§£æTICKæ•°æ®å¤±è´¥: {parse_error}")
                    continue
            
            if ticks:
                emit('tick_history', ticks)
                logger.info(f"âœ“ ä»Sorted Setæ¨é€ {len(ticks)} æ¡å†å²TICKç»™å®¢æˆ·ç«¯ {request.sid}")
                # ã€å…³é”®ã€‘ä¸è®¾ç½®last_timeï¼Œä¿æŒä¸º0ï¼Œç¡®ä¿æ‰€æœ‰æ–°çš„å®æ—¶æ•°æ®éƒ½èƒ½æ¨é€
        else:
            logger.warning("Sorted Setä¸ºç©ºï¼Œå®¢æˆ·ç«¯å°†åªæ¥æ”¶å®æ—¶æ•°æ®")
    except Exception as e:
        logger.error(f"ä»Sorted Setè¯»å–å†å²æ•°æ®å¤±è´¥: {e}")


@socketio.on('disconnect')
def handle_disconnect():
    """å®¢æˆ·ç«¯æ–­å¼€"""
    if request.sid in clients:
        del clients[request.sid]
        logger.info(f"âœ— å®¢æˆ·ç«¯å·²æ–­å¼€å¹¶æ¸…ç†: {request.sid} | å‰©ä½™å®¢æˆ·ç«¯: {len(clients)}")


@socketio.on('subscribe_kline')
def handle_subscribe_kline(data):
    """
    è®¢é˜…Kçº¿å®æ—¶æ•°æ®
    
    ğŸ”´ ä¿®å¤ï¼šæ”¯æŒ interval å‚æ•°ï¼Œå¤„ç†å¤šæ—¶é—´å‘¨æœŸè®¢é˜…
    """
    symbol = data.get('symbol', 'BTCUSDm')
    interval = data.get('interval', '1m')  # ğŸ”´ ä¿®å¤ï¼šæ”¯æŒå‰ç«¯ä¼ å…¥çš„intervalå‚æ•°
    
    # è½¬æ¢å‰ç«¯æ ¼å¼åˆ°åç«¯æ ¼å¼ï¼š1m -> 1m, 5m -> 5m (ä¿æŒä¸å˜)
    # åç«¯Rediså­˜å‚¨ä½¿ç”¨å°å†™ï¼škline:{symbol}:1m
    timeframe = interval.lower()
    
    logger.info(f"å®¢æˆ·ç«¯è®¢é˜…Kçº¿: {symbol} @ {interval}")
    
    # ç«‹å³å‘é€å½“å‰å†å²æ•°æ®ï¼ˆå¯é€‰ï¼Œå‰ç«¯å·²ç»é€šè¿‡APIè·å–äº†å†å²æ•°æ®ï¼‰
    # è¿™é‡Œå¯ä»¥é€‰æ‹©ä¸å‘é€ï¼Œæˆ–è€…å‘é€æœ€æ–°çš„å‡ æ¡ä½œä¸ºè¡¥å……
    # klines = get_kline_data_from_redis(symbol, timeframe, 10, format_for_frontend=True)
    # if klines:
    #     for kline in klines:
    #         kline['symbol'] = symbol
    #         kline['interval'] = interval
    #         emit('kline_update', kline)
    
    # è®¢é˜…æˆåŠŸç¡®è®¤ï¼ˆå¯é€‰ï¼‰
    emit('kline_subscribed', {'symbol': symbol, 'interval': interval, 'success': True})


@socketio.on('unsubscribe_kline')
def handle_unsubscribe_kline(data):
    """
    å–æ¶ˆè®¢é˜…Kçº¿å®æ—¶æ•°æ®
    
    ğŸ”´ ä¿®å¤ï¼šæ·»åŠ  unsubscribe_kline äº‹ä»¶å¤„ç†
    """
    symbol = data.get('symbol', 'BTCUSDm')
    interval = data.get('interval', '1m')
    logger.info(f"å®¢æˆ·ç«¯å–æ¶ˆè®¢é˜…Kçº¿: {symbol} @ {interval}")
    
    # å–æ¶ˆè®¢é˜…æˆåŠŸç¡®è®¤ï¼ˆå¯é€‰ï¼‰
    emit('kline_unsubscribed', {'symbol': symbol, 'interval': interval, 'success': True})


@socketio.on('subscribe_tick')
def handle_subscribe_tick(data):
    """è®¢é˜…Tickå®æ—¶æ•°æ®"""
    symbol = data.get('symbol', 'BTCUSDm')
    logger.info(f"å®¢æˆ·ç«¯è®¢é˜…Tick: {symbol}")


# ==================== åå°ä»»åŠ¡ ====================

def broadcast_positions_updates():
    """
    å®šæœŸä»MT5è·å–æœ€æ–°æŒä»“å¹¶æ¨é€å®æ—¶æ›´æ–°
    
    åŠŸèƒ½ï¼š
    1. ç»Ÿä¸€ä½¿ç”¨ gRPC ä» Windows MT5 ä¸­ç»§æœåŠ¡è·å–æœ€æ–°æŒä»“
    2. è®¡ç®—æµ®åŠ¨ç›ˆäºå˜åŒ–
    3. é€šè¿‡Socket.IOæ¨é€ position_update äº‹ä»¶
    """
    from src.trading.services.grpc_trade_client import get_grpc_client, is_grpc_available
    
    last_positions = {}  # è®°å½•ä¸Šæ¬¡çš„æŒä»“ï¼Œç”¨äºæ£€æµ‹å˜åŒ–
    last_update_time = 0
    
    logger.info("âœ“ æŒä»“å®æ—¶æ›´æ–°çº¿ç¨‹å·²å¯åŠ¨ï¼ˆä½¿ç”¨ gRPCï¼‰")
    
    # å»¶è¿Ÿåˆå§‹åŒ– gRPC å®¢æˆ·ç«¯ï¼Œé¿å…å¯åŠ¨æ—¶é˜»å¡
    grpc_client = None
    
    while True:
        try:
            current_time = time.time()
            
            # ç»Ÿä¸€ä½¿ç”¨ gRPC è·å–æŒä»“
            if is_grpc_available():
                try:
                    # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¯åŠ¨æ—¶é˜»å¡
                    if grpc_client is None:
                        try:
                            grpc_client = get_grpc_client()
                            # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶ï¼Œé¿å…é˜»å¡
                            grpc_client.timeout = 2
                        except Exception as e:
                            logger.warning(f"åˆå§‹åŒ– gRPC å®¢æˆ·ç«¯å¤±è´¥: {e}ï¼Œç¨åé‡è¯•")
                            socketio.sleep(5)
                            continue
                    
                    # ä½¿ç”¨è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
                    try:
                        # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆ1ç§’ï¼‰ï¼Œå¿«é€Ÿå¤±è´¥
                        original_timeout = getattr(grpc_client, 'timeout', 10)
                        grpc_client.timeout = 1  # 1ç§’è¶…æ—¶
                        result = grpc_client.get_positions(account_id='', symbol='', ticket=0, magic=0)
                        grpc_client.timeout = original_timeout  # æ¢å¤åŸå§‹è¶…æ—¶
                    except Exception as grpc_error:
                        logger.warning(f"gRPC è·å–æŒä»“è¶…æ—¶æˆ–å¤±è´¥: {grpc_error}")
                        socketio.sleep(5)  # å¤±è´¥åç­‰å¾…5ç§’é‡è¯•
                        continue
                    
                    if result.get('success') and result.get('positions'):
                        positions = result['positions']
                        
                        # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼å¹¶æ¨é€ï¼ˆä½¿ç”¨ç»Ÿä¸€è½¬æ¢å‡½æ•°ï¼‰
                        for pos in positions:
                            frontend_position = format_position_for_frontend(pos)
                            position_id = frontend_position['positionId']
                            
                            # æ£€æµ‹æŒä»“å˜åŒ–ï¼ˆä»·æ ¼æˆ–ç›ˆäºå˜åŒ–ï¼‰
                            last_pos = last_positions.get(position_id)
                            if not last_pos or (
                                last_pos.get('currentPrice') != frontend_position['currentPrice'] or
                                last_pos.get('unrealizedPnL') != frontend_position['unrealizedPnL']
                            ):
                                # æ¨é€æŒä»“æ›´æ–°
                                socketio.emit('position_update', frontend_position)
                                last_positions[position_id] = frontend_position
                                logger.debug(f"æ¨é€æŒä»“æ›´æ–°: {position_id}, ç›ˆäº={frontend_position['unrealizedPnL']:.2f}")
                        
                        # æ£€æµ‹å·²å¹³ä»“çš„æŒä»“ï¼ˆä»last_positionsä¸­åˆ é™¤ï¼‰
                        current_position_ids = {str(pos.get('ticket', 0)) for pos in positions}
                        for pos_id in list(last_positions.keys()):
                            if pos_id not in current_position_ids:
                                # æŒä»“å·²å¹³ä»“ï¼Œæ¨é€volume=0çš„æ›´æ–°ï¼ˆå‰ç«¯ä¼šåˆ é™¤ï¼‰
                                socketio.emit('position_update', {
                                    'positionId': pos_id,
                                    'volume': 0  # å‰ç«¯ä¼šåˆ é™¤volume=0çš„æŒä»“
                                })
                                del last_positions[pos_id]
                                logger.debug(f"æŒä»“å·²å¹³ä»“: {pos_id}")
                        
                        socketio.sleep(1)  # gRPC æˆåŠŸï¼Œæ¯1ç§’æ›´æ–°ä¸€æ¬¡
                        continue
                        
                except Exception as e:
                    logger.warning(f"gRPC è·å–æŒä»“å¤±è´¥: {e}")
                    socketio.sleep(5)  # å¤±è´¥åç­‰å¾…5ç§’é‡è¯•
                    continue
            else:
                logger.warning("gRPC ä¸å¯ç”¨ï¼ŒæŒä»“æ›´æ–°åŠŸèƒ½æš‚åœ")
                socketio.sleep(10)  # gRPC ä¸å¯ç”¨ï¼Œç­‰å¾…10ç§’åé‡è¯•
                continue
                
        except KeyboardInterrupt:
            raise
        except Exception as e:
            logger.error(f"æŒä»“æ›´æ–°çº¿ç¨‹å¼‚å¸¸: {e}")
            import traceback
            logger.error(traceback.format_exc())
            socketio.sleep(5)  # å¼‚å¸¸åç­‰å¾…5ç§’é‡è¯•


def listen_redis_pubsub():
    """
    ç›‘å¬ Redis Pub/Subï¼Œæ¥æ”¶ Windows gRPC æœåŠ¡æ¨é€çš„å®æ—¶æ›´æ–°ï¼ˆäº‹ä»¶é©±åŠ¨æ¶æ„ï¼‰
    
    ç»Ÿä¸€æ¶æ„ï¼šWindows gRPC æœåŠ¡ â†’ Redis Pub/Sub â†’ åç«¯ â†’ WebSocket â†’ å‰ç«¯
    
    è®¢é˜…é¢‘é“ï¼š
    - tick:{symbol}: TICKæ•°æ®ï¼ˆé«˜é¢‘ï¼Œæ¥è‡ªZeroMQæˆ–è½®è¯¢ï¼‰
    - kline:{symbol}:{timeframe}: Kçº¿æ•°æ®
    - mt5:position_update: å•ä¸ªæŒä»“æ›´æ–°
    - mt5:positions_update: æŒä»“åˆ—è¡¨æ›´æ–°
    - mt5:deal: æˆäº¤äº‹ä»¶
    - mt5:order_update: æŒ‚å•æ›´æ–°
    - mt5:account_info: è´¦æˆ·çŠ¶æ€æ›´æ–°
    - mt5:connection_status: è¿æ¥çŠ¶æ€æ›´æ–°
    - mt5:trade_events: ZeroMQæ¨é€çš„äº¤æ˜“äº‹ä»¶
    
    ğŸ”´ å…³é”®ä¿®å¤ï¼šå®Œæ•´çš„è‡ªåŠ¨é‡è¿å’ŒæŒ‡æ•°é€€é¿æœºåˆ¶ï¼Œç¡®ä¿çº¿ç¨‹æ°¸ä¸é€€å‡º
    """
    import json
    
    # ğŸ”´ ä¿®å¤ï¼šé‡è¿é…ç½®å¸¸é‡
    INITIAL_RETRY_DELAY = 1.0  # åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    MAX_RETRY_DELAY = 10.0     # æœ€å¤§é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    reconnect_delay = INITIAL_RETRY_DELAY
    pubsub = None
    
    # ğŸ”´ ä¿®å¤ï¼šè¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºå¹¶è®¢é˜…PubSub
    def create_and_subscribe_pubsub():
        """åˆ›å»ºæ–°çš„PubSubå®ä¾‹å¹¶è®¢é˜…æ‰€æœ‰é¢‘é“"""
        new_pubsub = redis_client.pubsub()
        # ä½¿ç”¨æ¨¡å¼è®¢é˜…æ”¯æŒé€šé…ç¬¦ï¼ˆpsubscribeï¼‰
        new_pubsub.psubscribe('tick:*')  # æ‰€æœ‰TICKæ•°æ®
        new_pubsub.psubscribe('kline:*')  # æ‰€æœ‰Kçº¿æ•°æ®ï¼ˆå·²é—­åˆï¼‰
        new_pubsub.psubscribe('current_kline:*')  # ğŸš€ å½“å‰æœªé—­åˆKçº¿ï¼ˆå®æ—¶è·³åŠ¨ï¼‰
        # è®¢é˜…å…·ä½“é¢‘é“
        new_pubsub.subscribe(
            'mt5:position_update',
            'mt5:positions_update',
            'mt5:deal',
            'mt5:order_update',
            'mt5:account_info',
            'mt5:connection_status',
            'mt5:trade_events'
        )
        return new_pubsub
    
    # ğŸ”´ ä¿®å¤ï¼šæ— é™é‡è¯•å¾ªç¯ï¼Œç¡®ä¿çº¿ç¨‹æ°¸ä¸é€€å‡º
    while True:
        try:
            # 1. æ£€æŸ¥Redisè¿æ¥å¥åº·çŠ¶æ€
            try:
                redis_client.ping()
            except (redis.exceptions.ConnectionError, redis.exceptions.TimeoutError) as e:
                logger.warning(f"Redis Pub/Sub: Redisè¿æ¥ä¸¢å¤±ï¼Œ{reconnect_delay:.1f}ç§’åé‡è¯•... é”™è¯¯: {e}")
                pubsub = None  # æ ‡è®°pubsubæ— æ•ˆ
                socketio.sleep(reconnect_delay)
                reconnect_delay = min(reconnect_delay * 2, MAX_RETRY_DELAY)  # æŒ‡æ•°é€€é¿
                continue  # ç»§ç»­å¾ªç¯ï¼Œå°è¯•é‡æ–°è¿æ¥
            
            # 2. å¦‚æœpubsubæ— æ•ˆï¼Œé‡æ–°åˆ›å»º
            if pubsub is None:
                try:
                    logger.info("ğŸ“¡ å°è¯•è¿æ¥ Redis Pub/Sub...")
                    pubsub = create_and_subscribe_pubsub()
                    logger.info("âœ… Redis Pub/Sub è¿æ¥æˆåŠŸï¼Œå·²è®¢é˜…æ‰€æœ‰é¢‘é“")
                    reconnect_delay = INITIAL_RETRY_DELAY  # é‡ç½®å»¶è¿Ÿ
                except Exception as create_error:
                    logger.error(f"âŒ Redis Pub/Sub: åˆ›å»ºè¿æ¥å¤±è´¥: {create_error}ï¼Œ{reconnect_delay:.1f}ç§’åé‡è¯•...")
                    pubsub = None
                    socketio.sleep(reconnect_delay)
                    reconnect_delay = min(reconnect_delay * 2, MAX_RETRY_DELAY)  # æŒ‡æ•°é€€é¿
                    continue
            
            # 3. ğŸ”´ ä¿®å¤ï¼šåœ¨ eventlet æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨éé˜»å¡ get_message() å¹¶é…åˆ socketio.sleep()
            # é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼Œå¯¼è‡´æ— æ³•å¤„ç†è¿æ¥å…³é—­äº‹ä»¶
            try:
                message = pubsub.get_message(timeout=0.1)  # 100msè¶…æ—¶ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
            except (redis.exceptions.ConnectionError, redis.exceptions.TimeoutError) as e:
                logger.warning(f"Redis Pub/Sub: è¯»å–æ¶ˆæ¯æ—¶è¿æ¥é”™è¯¯: {e}ï¼Œé‡æ–°è¿æ¥...")
                pubsub = None  # æ ‡è®°éœ€è¦é‡æ–°åˆ›å»º
                socketio.sleep(reconnect_delay)
                reconnect_delay = min(reconnect_delay * 2, MAX_RETRY_DELAY)  # æŒ‡æ•°é€€é¿
                continue
            
            if not message:
                # æ²¡æœ‰æ¶ˆæ¯æ—¶ï¼Œè®©å‡ºæ§åˆ¶æƒç»™äº‹ä»¶å¾ªç¯ï¼Œå¤„ç†è¿æ¥å…³é—­ç­‰äº‹ä»¶
                socketio.sleep(0.01)  # 10msï¼Œè®©äº‹ä»¶å¾ªç¯æœ‰æœºä¼šå¤„ç†å…¶ä»–äº‹ä»¶
                continue
            
            # å¤„ç†æ¨¡å¼è®¢é˜…æ¶ˆæ¯ï¼ˆpmessageï¼‰å’Œæ™®é€šæ¶ˆæ¯ï¼ˆmessageï¼‰
            if message['type'] == 'pmessage':
                # æ¨¡å¼è®¢é˜…æ¶ˆæ¯
                pattern = message['pattern']
                channel = message['channel']
                data_str = message['data']
            elif message['type'] == 'message':
                # æ™®é€šè®¢é˜…æ¶ˆæ¯
                channel = message['channel']
                data_str = message['data']
            else:
                continue
            
            # è§£ææ•°æ®
            # ğŸ”´ ä¿®å¤ï¼šcurrent_klineæ¶ˆæ¯å¯èƒ½æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            if channel.startswith('current_kline:'):
                # å½“å‰Kçº¿æ•°æ®ï¼šå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²ï¼Œéœ€è¦å…ˆè§£æ
                try:
                    if isinstance(data_str, str):
                        data = json.loads(data_str)
                    else:
                        data = data_str
                except (json.JSONDecodeError, TypeError):
                    # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨
                    data = data_str if isinstance(data_str, dict) else {}
                
                # ğŸ”´ è°ƒè¯•ï¼šæ‰“å°æ¥æ”¶åˆ°çš„Redisæ¶ˆæ¯ï¼ˆæ”¹ä¸ºINFOçº§åˆ«ï¼Œä¾¿äºè°ƒè¯•ï¼‰
                logger.info(f"ğŸ“¥ API Server: æ”¶åˆ°Redis current_klineæ¶ˆæ¯: channel={channel}, data={data}")
                handle_current_kline_data(channel, data)
                continue  # å¤„ç†å®Œç›´æ¥ç»§ç»­ï¼Œé¿å…é‡å¤å¤„ç†
            
            # å…¶ä»–é¢‘é“çš„æ ‡å‡†JSONè§£æ
            try:
                data = json.loads(data_str)
            except json.JSONDecodeError:
                logger.warning(f"æ— æ³•è§£æJSONæ•°æ®: {channel}")
                continue
            
            # ğŸ”´ ä¿®å¤ï¼šæˆåŠŸå¤„ç†æ¶ˆæ¯åï¼Œé‡ç½®å»¶è¿Ÿï¼ˆè¿æ¥æ­£å¸¸ï¼‰
            reconnect_delay = INITIAL_RETRY_DELAY
            
            # æ ¹æ®é¢‘é“ç±»å‹åˆ†å‘å¤„ç†
            if channel.startswith('tick:'):
                # TICKæ•°æ®ï¼štick:{symbol}
                handle_tick_data(channel, data)
            
            elif channel.startswith('kline:'):
                # Kçº¿æ•°æ®ï¼škline:{symbol}:{timeframe}
                handle_kline_data(channel, data)
            
            elif channel == 'mt5:position_update':
                # å•ä¸ªæŒä»“æ›´æ–°
                position = data.get('position')
                if position:
                    socketio.emit('position_update', position)
                    logger.debug(f"æ¨é€æŒä»“æ›´æ–°: {position.get('positionId')}, ç›ˆäº={position.get('unrealizedPnL', 0):.2f}")
            
            elif channel == 'mt5:positions_update':
                # æŒä»“ç»“æ„å˜åŒ–ï¼ˆå¼€/å¹³ä»“ï¼‰
                positions = data.get('positions', [])
                logger.debug(f"æ”¶åˆ°æŒä»“ç»“æ„æ›´æ–°: {len(positions)}ä¸ªæŒä»“")
                for pos in positions:
                    socketio.emit('position_update', pos)
            
            elif channel == 'mt5:deal':
                # æ–°æˆäº¤
                order_data = data.get('order')
                deal_data = data.get('deal', {})
                if order_data:
                    logger.info(f"æ”¶åˆ°MT5æˆäº¤: ticket={deal_data.get('ticket')}, order={deal_data.get('order')}")
                    socketio.emit('order_update', order_data)
                    logger.debug(f"æ¨é€è®¢å•æ›´æ–°: {order_data.get('orderId')}")
            
            elif channel == 'mt5:order_update':
                # æŒ‚å•æ›´æ–°
                order = data.get('order')
                if order:
                    socketio.emit('order_update', order)
                    logger.debug(f"æ¨é€æŒ‚å•æ›´æ–°: {order.get('orderId')}")
            
            elif channel == 'mt5:account_info':
                # è´¦æˆ·çŠ¶æ€æ›´æ–°
                account_info = data.get('account_info')
                if account_info:
                    socketio.emit('account_update', account_info)
                    logger.debug(f"æ¨é€è´¦æˆ·æ›´æ–°: å‡€å€¼={account_info.get('equity', 0):.2f}")
            
            elif channel == 'mt5:connection_status':
                # è¿æ¥çŠ¶æ€æ›´æ–°
                status = data.get('status')
                socketio.emit('connection_status', {'status': status})
                logger.info(f"MT5è¿æ¥çŠ¶æ€: {status}")
            
            elif channel == 'mt5:trade_events':
                # ZeroMQæ¨é€çš„äº¤æ˜“äº‹ä»¶
                handle_trade_event(data)
            
        except redis.exceptions.ConnectionError as e:
            logger.error(f"âŒ Redis Pub/Sub: è¿æ¥é”™è¯¯: {e}ï¼Œ{reconnect_delay:.1f}ç§’åé‡è¯•...")
            pubsub = None  # æ ‡è®°éœ€è¦é‡æ–°åˆ›å»º
            socketio.sleep(reconnect_delay)
            reconnect_delay = min(reconnect_delay * 2, MAX_RETRY_DELAY)  # æŒ‡æ•°é€€é¿
        except redis.exceptions.TimeoutError as e:
            logger.warning(f"âš ï¸ Redis Pub/Sub: è¶…æ—¶é”™è¯¯: {e}ï¼Œ{reconnect_delay:.1f}ç§’åé‡è¯•...")
            pubsub = None  # æ ‡è®°éœ€è¦é‡æ–°åˆ›å»º
            socketio.sleep(reconnect_delay)
            reconnect_delay = min(reconnect_delay * 2, MAX_RETRY_DELAY)  # æŒ‡æ•°é€€é¿
        except Exception as e:
            # ğŸ”´ ä¿®å¤ï¼šæ•è·æ‰€æœ‰æœªçŸ¥é”™è¯¯ï¼Œç¡®ä¿çº¿ç¨‹æ°¸ä¸é€€å‡º
            logger.error(f"âŒ Redis Pub/Sub ç›‘å¬å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            pubsub = None  # æ ‡è®°éœ€è¦é‡æ–°åˆ›å»º
            socketio.sleep(reconnect_delay)
            reconnect_delay = min(reconnect_delay * 2, MAX_RETRY_DELAY)  # æŒ‡æ•°é€€é¿


def handle_tick_data(channel: str, data: dict):
    """
    å¤„ç†TICKæ•°æ®
    
    Args:
        channel: tick:{symbol}
        data: TICKæ•°æ®å­—å…¸
    """
    try:
        symbol = channel.split(':')[-1]
        tick_type = data.get('type', 'tick')
        
        # æ›´æ–°Redisçƒ­ç¼“å­˜ï¼ˆæœ€æ–°TICKï¼‰
        tick_key = f'tick:{symbol}'
        tick_json = json.dumps(data)
        redis_client.set(f'{tick_key}:latest', tick_json, ex=60)  # 60ç§’è¿‡æœŸ
        
        # å¯é€‰ï¼šå­˜å‚¨åˆ°Sorted Setï¼ˆå†å²æ•°æ®ï¼‰
        if 'time_msc' in data:
            time_msc = data['time_msc']
            redis_client.zadd(tick_key, {tick_json: time_msc})
            # ä¿ç•™æœ€è¿‘10000æ¡
            redis_client.zremrangebyrank(tick_key, 0, -10001)
        
        # é€šè¿‡WebSocketæ¨é€ç»™å‰ç«¯
        # ğŸ”´ ä¿®å¤ï¼šç»Ÿä¸€äº‹ä»¶åç§°ä¸º tick_updateï¼ˆä¸å‰ç«¯ç›‘å¬ä¸€è‡´ï¼‰
        socketio.emit('tick_update', data)
        
        logger.debug(f"TICKè½¬å‘: {symbol} @ {data.get('bid', 0):.2f}/{data.get('ask', 0):.2f}")
    
    except Exception as e:
        logger.error(f"å¤„ç†TICKæ•°æ®å¤±è´¥: {e}")


def handle_current_kline_data(channel: str, data: dict):
    """
    å¤„ç†å½“å‰æœªé—­åˆçš„Kçº¿æ•°æ®ï¼ˆå®æ—¶è·³åŠ¨ï¼ŒåŸºäºTICKæ›´æ–°ï¼‰
    
    ã€æ ¸å¿ƒæœºåˆ¶ã€‘
    - L2ç­–ç•¥æ ¸å¿ƒæ¯æ¬¡æ”¶åˆ°TICKéƒ½ä¼šæ›´æ–°å½“å‰Kçº¿å¹¶æ¨é€
    - æ­¤å‡½æ•°æ¥æ”¶æ¨é€ï¼Œé€šè¿‡WebSocketå®æ—¶å‘é€ç»™å‰ç«¯
    - å‰ç«¯æ¥æ”¶åæ›´æ–°å›¾è¡¨æœ€åä¸€æ ¹Kçº¿ï¼Œå®ç°å®æ—¶è·³åŠ¨
    
    Args:
        channel: current_kline:{symbol}:{timeframe}
        data: å½“å‰Kçº¿æ•°æ®ï¼ˆJSONå­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
    """
    try:
        # è§£æé¢‘é“ï¼šcurrent_kline:{symbol}:{timeframe}
        parts = channel.split(':')
        if len(parts) >= 3:
            symbol = parts[1]
            timeframe = parts[2]
            
            # ğŸ”´ ä¿®å¤ï¼šå¤„ç†æ•°æ®æ ¼å¼ï¼šå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²æˆ–å­—å…¸
            # Redis Pub/Subè¿”å›çš„æ•°æ®å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
            if isinstance(data, str):
                try:
                    kline = json.loads(data)
                except json.JSONDecodeError:
                    logger.warning(f"æ— æ³•è§£æcurrent_kline JSON: {data[:100]}")
                    return
            elif isinstance(data, dict):
                kline = data
            else:
                logger.warning(f"current_klineæ•°æ®æ ¼å¼å¼‚å¸¸: {type(data)}")
                return
            
            if kline and isinstance(kline, dict) and kline.get('time', 0) > 0:
                # ğŸš€ è½¬æ¢æ—¶é—´å‘¨æœŸæ ¼å¼ï¼šåç«¯ä½¿ç”¨ m1/M1ï¼Œå‰ç«¯ä½¿ç”¨ 1m
                # ä¾‹å¦‚ï¼šm1 -> 1m, m5 -> 5m, h1 -> 1h, d1 -> 1d
                def convert_timeframe_to_interval(tf: str) -> str:
                    """å°†åç«¯æ—¶é—´å‘¨æœŸæ ¼å¼è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼"""
                    if not tf:
                        return '1m'
                    tf_lower = tf.lower()
                    # å¤„ç† M1, M5, H1, D1 ç­‰æ ¼å¼
                    if tf_lower.startswith('m'):
                        # m1 -> 1m, m5 -> 5m
                        minutes = tf_lower[1:] if len(tf_lower) > 1 else '1'
                        return f"{minutes}m"
                    elif tf_lower.startswith('h'):
                        # h1 -> 1h, h4 -> 4h
                        hours = tf_lower[1:] if len(tf_lower) > 1 else '1'
                        return f"{hours}h"
                    elif tf_lower.startswith('d'):
                        # d1 -> 1d
                        days = tf_lower[1:] if len(tf_lower) > 1 else '1'
                        return f"{days}d"
                    else:
                        # é»˜è®¤è¿”å› 1m
                        return '1m'
                
                frontend_interval = convert_timeframe_to_interval(timeframe)
                
                # ğŸš€ ç¡®ä¿æ•°æ®æ ¼å¼ç¬¦åˆLightweight Chartsæ ‡å‡†
                kline_dict = {
                    'symbol': symbol,
                    'interval': frontend_interval,  # ğŸš€ ä¿®å¤ï¼šä½¿ç”¨å‰ç«¯æ ¼å¼ï¼ˆ1mè€Œä¸æ˜¯m1ï¼‰
                    'timeframe': timeframe,  # ä¿ç•™åŸå§‹æ ¼å¼ä¾›è°ƒè¯•
                    'time': int(kline.get('time', 0)),  # Unixæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
                    'open': float(kline.get('open', 0)),
                    'high': float(kline.get('high', 0)),
                    'low': float(kline.get('low', 0)),
                    'close': float(kline.get('close', 0)),  # å®æ—¶TICKä»·æ ¼
                    'volume': int(kline.get('volume', 0)),
                    'openTime': int(kline.get('time', 0)) * 1000,  # å…¼å®¹IBackendKLineæ ¼å¼ï¼ˆæ¯«ç§’ï¼‰
                    'closeTime': int(kline.get('time', 0)) * 1000 + 60,  # å‡è®¾1åˆ†é’ŸKçº¿
                    'is_closed': kline.get('is_closed', False)  # æ ‡è®°æœªé—­åˆ
                }
                
                # é€šè¿‡WebSocketæ¨é€ç»™å‰ç«¯ï¼ˆå®æ—¶è·³åŠ¨ï¼‰
                # ğŸ”´ ä¿®å¤ï¼šæ ¹æ®Flask-SocketIOæ–‡æ¡£ï¼Œåœ¨åå°ä»»åŠ¡ä¸­ç›´æ¥ä½¿ç”¨socketio.emit()ä¼šå‘é€ç»™æ‰€æœ‰å®¢æˆ·ç«¯
                # å‚è€ƒï¼šhttps://flask-socketio.readthedocs.io/en/latest/getting_started.html#emitting-from-background-tasks
                if len(clients) > 0:
                    socketio.emit('kline_update', kline_dict)  # ğŸ”´ ä¿®å¤ï¼šç§»é™¤broadcastå‚æ•°ï¼ŒFlask-SocketIOä¸æ”¯æŒ
                    logger.debug(f"ğŸ“¤ æ¨é€å½“å‰Kçº¿è·³åŠ¨: symbol={symbol}, timeframe={timeframe}, interval={frontend_interval}, "
                               f"time={kline_dict['time']}, close={kline_dict['close']:.2f}, å®¢æˆ·ç«¯æ•°={len(clients)}")
                else:
                    logger.debug(f"âš ï¸ æ— å®¢æˆ·ç«¯è¿æ¥ï¼Œè·³è¿‡Kçº¿æ¨é€: symbol={symbol}, interval={frontend_interval}")
    
    except Exception as e:
        logger.error(f"å¤„ç†å½“å‰Kçº¿æ•°æ®å¤±è´¥: {e}")


def handle_kline_data(channel: str, data: dict):
    """
    å¤„ç†Kçº¿æ•°æ®ï¼ˆä»Redis Pub/Subæ¥æ”¶ï¼‰
    
    Args:
        channel: kline:{symbol}:{timeframe}
        data: Kçº¿æ•°æ®ï¼ˆå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
    """
    try:
        # è§£æé¢‘é“ï¼škline:{symbol}:{timeframe}
        parts = channel.split(':')
        if len(parts) >= 3:
            symbol = parts[1]
            timeframe = parts[2]
            
            # å¤„ç†æ•°æ®æ ¼å¼ï¼šå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²æˆ–å­—å…¸
            if isinstance(data, str):
                try:
                    kline = json.loads(data)
                except:
                    kline = {'kline': json.loads(data)}
            else:
                kline = data.get('kline') if 'kline' in data else data
            
            if kline and isinstance(kline, dict):
                # ğŸš€ ç¡®ä¿æ•°æ®æ ¼å¼ç¬¦åˆMT5å’ŒLightweight Chartsæ ‡å‡†
                kline_dict = {
                    'time': int(kline.get('time', 0)),  # Unixæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
                    'open': float(kline.get('open', 0)),
                    'high': float(kline.get('high', 0)),
                    'low': float(kline.get('low', 0)),
                    'close': float(kline.get('close', 0)),
                    'volume': int(kline.get('volume', 0)),
                    'real_volume': int(kline.get('real_volume', 0))
                }
                
                # å­˜å‚¨åˆ°Redis Sorted Setï¼ˆå¦‚æœå°šæœªå­˜å‚¨ï¼‰
                kline_key = f'kline:{symbol}:{timeframe}'
                kline_json = json.dumps(kline_dict, ensure_ascii=False)
                
                # ğŸ”´ ä¿®å¤ï¼šå…ˆåˆ é™¤ç›¸åŒæ—¶é—´æˆ³çš„æ—§æ•°æ®ï¼Œå†æ·»åŠ æ–°æ•°æ®ï¼ˆé¿å…é‡å¤ï¼‰
                kline_time = kline_dict['time']
                redis_client.zremrangebyscore(kline_key, kline_time, kline_time)
                
                # ä½¿ç”¨ZADDå­˜å‚¨æ–°æ•°æ®ï¼ˆç¡®ä¿æ—¶é—´æˆ³å”¯ä¸€ï¼‰
                redis_client.zadd(kline_key, {kline_json: kline_time})
                
                # ä¿ç•™æœ€è¿‘2880æ ¹ï¼ˆ2å¤©M1æ•°æ®ï¼‰
                current_count = redis_client.zcard(kline_key)
                if current_count > 2880:
                    remove_count = current_count - 2880
                    redis_client.zremrangebyrank(kline_key, 0, remove_count - 1)
                
                # é€šè¿‡WebSocketæ¨é€ç»™å‰ç«¯ï¼ˆLightweight Chartsæ ¼å¼ï¼‰
                # ğŸ”´ ä¿®å¤ï¼šæ ¹æ®Flask-SocketIOæ–‡æ¡£ï¼Œåœ¨åå°ä»»åŠ¡ä¸­ç›´æ¥ä½¿ç”¨socketio.emit()ä¼šå‘é€ç»™æ‰€æœ‰å®¢æˆ·ç«¯
                if len(clients) > 0:
                    socketio.emit('kline_update', {
                        'symbol': symbol,
                        'interval': timeframe,  # ä½¿ç”¨intervalä¿æŒä¸€è‡´æ€§
                        'timeframe': timeframe,
                        'time': kline_dict['time'],
                        'open': kline_dict['open'],
                        'high': kline_dict['high'],
                        'low': kline_dict['low'],
                        'close': kline_dict['close'],
                        'volume': kline_dict['volume'],
                        'openTime': kline_dict['time'] * 1000,  # å…¼å®¹IBackendKLineæ ¼å¼ï¼ˆæ¯«ç§’ï¼‰
                        'closeTime': kline_dict['time'] * 1000 + 60  # å‡è®¾1åˆ†é’ŸKçº¿
                    })  # ğŸ”´ ä¿®å¤ï¼šç§»é™¤broadcastå‚æ•°ï¼ŒFlask-SocketIOä¸æ”¯æŒ
                    logger.debug(f"Kçº¿è½¬å‘: {symbol} {timeframe} @ {kline_dict['time']} (O:{kline_dict['open']:.2f} C:{kline_dict['close']:.2f}), å®¢æˆ·ç«¯æ•°={len(clients)}")
                else:
                    logger.debug(f"âš ï¸ æ— å®¢æˆ·ç«¯è¿æ¥ï¼Œè·³è¿‡Kçº¿è½¬å‘: {symbol} {timeframe}")
    
    except Exception as e:
        logger.error(f"å¤„ç†Kçº¿æ•°æ®å¤±è´¥: {e}", exc_info=True)


def handle_trade_event(data: dict):
    """
    å¤„ç†ZeroMQæ¨é€çš„äº¤æ˜“äº‹ä»¶
    
    Args:
        data: äº¤æ˜“äº‹ä»¶æ•°æ®
    """
    try:
        trade_type = data.get('trade_type')
        order_ticket = data.get('order_ticket')
        position_ticket = data.get('position_ticket')
        
        logger.info(f"æ”¶åˆ°ZeroMQäº¤æ˜“äº‹ä»¶: type={trade_type}, order={order_ticket}, position={position_ticket}")
        
        # æ ¹æ®äº¤æ˜“ç±»å‹å¤„ç†
        if trade_type in [2, 3, 4]:  # TRADE_TRANSACTION_DEAL_ADD, POSITION, ORDER_ADD
            # è§¦å‘æŒä»“/è®¢å•æŸ¥è¯¢æ›´æ–°
            socketio.emit('trade_event', data)
            
            # å¯é€‰ï¼šé€šçŸ¥å‰ç«¯åˆ·æ–°è®¢å•/æŒä»“åˆ—è¡¨
            socketio.emit('refresh_orders')
            socketio.emit('refresh_positions')
    
    except Exception as e:
        logger.error(f"å¤„ç†äº¤æ˜“äº‹ä»¶å¤±è´¥: {e}")


def listen_order_feedback():
    """
    ç›‘å¬è®¢å•æ‰§è¡Œåé¦ˆé˜Ÿåˆ—ï¼Œæ¨é€è®¢å•å’ŒæŒä»“æ›´æ–°
    
    åŠŸèƒ½ï¼š
    1. ç›‘å¬Redisåé¦ˆé˜Ÿåˆ— l1:order:feedback
    2. å½“è®¢å•æ‰§è¡ŒæˆåŠŸåï¼Œæ¨é€ order_update å’Œ position_update äº‹ä»¶
    """
    logger.info("âœ“ è®¢å•åé¦ˆç›‘å¬çº¿ç¨‹å·²å¯åŠ¨")
    
    while True:
        try:
            # ä»Redis Listè¯»å–åé¦ˆï¼ˆéé˜»å¡ï¼‰
            feedback_json = redis_client.lpop('l1:order:feedback')
            
            if feedback_json:
                feedback = json.loads(feedback_json)
                logger.info(f"æ”¶åˆ°è®¢å•åé¦ˆ: {feedback}")
                
                action = feedback.get('action')
                status = feedback.get('status')
                order_id = feedback.get('order_id')
                
                if status == 'SUCCESS':
                    # è®¢å•æ‰§è¡ŒæˆåŠŸï¼Œæ¨é€è®¢å•æ›´æ–°
                    if action in ['BUY', 'SELL']:
                        # å¼€ä»“æˆåŠŸï¼Œéœ€è¦è·å–è®¢å•è¯¦æƒ…
                        try:
                            # ä»OrderEngineè·å–è®¢å•è¯¦æƒ…
                            all_orders = order_engine.get_all_orders()
                            order = next((o for o in all_orders if str(o.get('ticket')) == str(order_id)), None)
                            
                            if order:
                                # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
                                frontend_order = {
                                    'orderId': str(order.get('ticket', order_id)),
                                    'symbol': order.get('symbol', 'BTCUSDm'),
                                    'side': 'BUY' if order.get('type', 0) == 0 else 'SELL',
                                    'type': 'MARKET',
                                    'volume': order.get('volume_initial', order.get('volume', 0.0)),
                                    'price': order.get('price_open', 0.0),
                                    'status': 'FILLED',  # å¸‚ä»·å•ç«‹å³æˆäº¤
                                    'createdAt': order.get('time_setup', 0) * 1000,
                                    'updatedAt': int(time.time()) * 1000
                                }
                                
                                # æ¨é€è®¢å•æ›´æ–°
                                socketio.emit('order_update', frontend_order)
                                logger.info(f"æ¨é€è®¢å•æ›´æ–°: {frontend_order['orderId']}")
                                
                                # å¦‚æœæ˜¯å¼€ä»“ï¼ŒåŒæ—¶æ¨é€æŒä»“æ›´æ–°
                                if order.get('state') == 4:  # ORDER_STATE_FILLED
                                    positions = order_engine.get_all_positions()
                                    position = next((p for p in positions if str(p.get('ticket')) == str(order_id)), None)
                                    
                                    if position:
                                        frontend_position = format_position_for_frontend(position)
                                        socketio.emit('position_update', frontend_position)
                                        logger.info(f"æ¨é€æŒä»“æ›´æ–°: {frontend_position['positionId']}")
                        
                        except Exception as e:
                            logger.error(f"å¤„ç†è®¢å•åé¦ˆå¤±è´¥: {e}")
                    
                    elif action == 'CLOSE':
                        # å¹³ä»“æˆåŠŸï¼Œæ¨é€æŒä»“æ›´æ–°ï¼ˆvolume=0ï¼‰
                        socketio.emit('position_update', {
                            'positionId': str(order_id),
                            'volume': 0  # å‰ç«¯ä¼šåˆ é™¤
                        })
                        logger.info(f"æ¨é€å¹³ä»“æ›´æ–°: {order_id}")
            
            socketio.sleep(0.1)  # 100msæ£€æŸ¥ä¸€æ¬¡
            
        except Exception as e:
            logger.error(f"è®¢å•åé¦ˆç›‘å¬å¼‚å¸¸: {e}")
            socketio.sleep(1)


def broadcast_realtime_data():
    """
    ã€Flask-SocketIO + Redis Stream æœ€ä½³å®è·µã€‘
    
    å‚è€ƒï¼š
    - Flask-SocketIO: https://flask-socketio.readthedocs.io/en/latest/getting_started.html#background-tasks
    - Redis Stream: https://redis.io/docs/data-types/streams-tutorial/
    
    å…³é”®è¦ç‚¹ï¼š
    1. âœ… ä½¿ç”¨ socketio.start_background_task() å¯åŠ¨ï¼ˆä¸ç”¨ threading.Threadï¼‰
    2. âœ… ä½¿ç”¨ Consumer Group ç¡®ä¿æ¶ˆæ¯ä¸ä¸¢å¤±
    3. âœ… ä½¿ç”¨éé˜»å¡æˆ–çŸ­æ—¶é—´é˜»å¡ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
    4. âœ… ä½¿ç”¨ socketio.sleep() è€Œä¸æ˜¯ time.sleep()
    """
    
    # ğŸ”´ æ¶æ„ä¿®å¤ï¼šAPI Serveråº”è¯¥æ¶ˆè´¹å·²éªŒè¯æµï¼ˆä¸L2ç­–ç•¥æ ¸å¿ƒåŒçº§ï¼‰
    # åŸå§‹æµ: tick:BTCUSDm:stream (Data Pullerå†™å…¥)
    # å·²éªŒè¯æµ: tick:BTCUSDm:validated:stream (Data Integrity Serviceå†™å…¥)
    # L2ç­–ç•¥æ ¸å¿ƒå’ŒAPI Serveréƒ½åº”è¯¥æ¶ˆè´¹å·²éªŒè¯æµï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
    stream_key = 'tick:BTCUSDm:validated:stream'  # âœ… ä½¿ç”¨å·²éªŒè¯æµ
    stream_key_old = 'tick:BTCUSDm:stream'  # å…¼å®¹æ—§æ ¼å¼ï¼ˆåŸå§‹æµï¼‰
    group_name = 'backend_broadcast'
    consumer_name = 'worker_1'
    
    # åˆ›å»ºæ¶ˆè´¹è€…ç»„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    try:
        redis_client.xgroup_create(stream_key, group_name, id='$', mkstream=True)
        logger.info(f"âœ“ åˆ›å»ºæ¶ˆè´¹è€…ç»„: {group_name}")
    except Exception as e:
        if 'BUSYGROUP' not in str(e):
            logger.error(f"åˆ›å»ºæ¶ˆè´¹è€…ç»„å¤±è´¥: {e}")
    
    logger.info(f"âœ“ Streamæ¨é€å·²å¯åŠ¨: {stream_key} ({group_name}/{consumer_name})")
    logger.info(f"  - æ¶ˆè´¹å·²éªŒè¯æµï¼ˆä¸L2ç­–ç•¥æ ¸å¿ƒåŒçº§ï¼‰")
    logger.info(f"  - ç¡®ä¿æ•°æ®å®Œæ•´æ€§ï¼ˆseqã€checksumå·²éªŒè¯ï¼‰")
    
    last_kline_push = 0  # è®°å½•ä¸Šæ¬¡æ¨é€Kçº¿çš„æ—¶é—´
    current_stream_key = stream_key  # å½“å‰ä½¿ç”¨çš„Stream key
    fallback_attempted = False  # æ˜¯å¦å·²å°è¯•fallback
    no_data_count = 0  # ğŸ”´ ä¿®å¤ï¼šè®°å½•è¿ç»­æ— æ•°æ®æ¬¡æ•°ï¼Œé¿å…é¢‘ç¹åˆ‡æ¢
    
    while True:
        try:
            # ã€æœ€ä½³å®è·µã€‘ä½¿ç”¨çŸ­æ—¶é—´é˜»å¡ï¼ˆ10msï¼‰ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡äº‹ä»¶å¾ªç¯
            # ğŸ”´ ä¿®å¤ï¼šåœ¨ eventlet æ¨¡å¼ä¸‹ï¼Œç¼©çŸ­é˜»å¡æ—¶é—´å¹¶é…åˆ socketio.sleep()ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯å¯¼è‡´æ— æ³•æ¥å—æ–°è¿æ¥
            try:
                streams = redis_client.xreadgroup(
                    group_name, 
                    consumer_name,
                    {current_stream_key: '>'},
                    count=10,
                    block=10  # ğŸ”´ ä¿®å¤ï¼šç¼©çŸ­åˆ° 10msï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
                )
            except redis.exceptions.ConnectionError as e:
                logger.warning(f"Redis Streamè¯»å–è¿æ¥é”™è¯¯: {e}")
                socketio.sleep(1)
                continue
            except Exception as e:
                logger.error(f"Redis Streamè¯»å–å¼‚å¸¸: {e}")
                socketio.sleep(0.1)
                continue
            
            # ğŸ”´ ä¿®å¤ï¼šå¦‚æœæœ‰æ•°æ®ï¼Œé‡ç½®è®¡æ•°å™¨
            if streams:
                no_data_count = 0
            else:
                no_data_count += 1
                # ğŸ”´ ä¿®å¤ï¼šæ²¡æœ‰æ•°æ®æ—¶ï¼Œè®©å‡ºæ§åˆ¶æƒç»™äº‹ä»¶å¾ªç¯ï¼Œå¤„ç†è¿æ¥å…³é—­ç­‰äº‹ä»¶
                socketio.sleep(0.01)  # 10msï¼Œè®©äº‹ä»¶å¾ªç¯æœ‰æœºä¼šå¤„ç†å…¶ä»–äº‹ä»¶
            
            # ã€å…¼å®¹æ€§ã€‘å¦‚æœæ–°Streamé•¿æ—¶é—´æ²¡æ•°æ®ï¼ˆè¿ç»­100æ¬¡æ— æ•°æ®ï¼Œçº¦5ç§’ï¼‰ï¼Œå°è¯•åˆ‡æ¢åˆ°æ—§Stream
            # ğŸ”´ ä¿®å¤ï¼šé¿å…é¢‘ç¹åˆ‡æ¢Streamå¯¼è‡´æ—¥å¿—åˆ·å±å’Œæ€§èƒ½é—®é¢˜
            if not streams and not fallback_attempted and no_data_count >= 100:
                # æ£€æŸ¥æ—§Streamæ˜¯å¦æœ‰æ•°æ®
                try:
                    old_stream_info = redis_client.xinfo_stream(stream_key_old)
                    if old_stream_info.get('length', 0) > 0:
                        logger.info(f"åˆ‡æ¢åˆ°æ—§Stream: {stream_key_old} (è¿ç»­{no_data_count}æ¬¡æ— æ•°æ®)")
                        redis_client.xgroup_create(stream_key_old, group_name, id='$', mkstream=True)
                        current_stream_key = stream_key_old
                        fallback_attempted = True
                        no_data_count = 0  # é‡ç½®è®¡æ•°å™¨
                except Exception as e:
                    # ğŸ”´ ä¿®å¤ï¼šåªè®°å½•ä¸€æ¬¡é”™è¯¯ï¼Œé¿å…æ—¥å¿—åˆ·å±
                    if not hasattr(broadcast_realtime_data, '_fallback_error_logged'):
                        logger.warning(f"æ£€æŸ¥æ—§Streamå¤±è´¥: {e}")
                        broadcast_realtime_data._fallback_error_logged = True
                    fallback_attempted = True  # æ ‡è®°å·²å°è¯•ï¼Œé¿å…é‡å¤æ£€æŸ¥
            
            if streams:
                for stream_name, messages in streams:
                    for message_id, data in messages:
                        try:
                            # è§£æTICKæ•°æ®ï¼ˆRedisè¿”å›çš„keyå¯èƒ½æ˜¯stræˆ–bytesï¼‰
                            # Redis Stream å¯èƒ½ä½¿ç”¨ 'data' æˆ– 'value' å­—æ®µ
                            tick_json = data.get(b'data') or data.get('data') or data.get(b'value') or data.get('value')
                            if not tick_json:
                                logger.error(f"Streamæ•°æ®ç¼ºå°‘data/valueå­—æ®µ: {data}")
                                # ğŸ”´ ä¿®å¤ï¼šå³ä½¿æ•°æ®æ ¼å¼é”™è¯¯ä¹Ÿè¦ACKï¼Œé¿å…pendingå †ç§¯
                                redis_client.xack(current_stream_key, group_name, message_id)
                                continue
                            
                            # å¦‚æœæ˜¯bytesï¼Œéœ€è¦decode
                            if isinstance(tick_json, bytes):
                                tick_json = tick_json.decode('utf-8')
                            
                            tick = json.loads(tick_json)
                            
                            # éªŒè¯å¿…éœ€å­—æ®µï¼ˆMT5æœ€ä½³å®è·µï¼‰
                            if not all(k in tick for k in ['time_msc', 'bid', 'ask']):
                                logger.warning(f"TICKæ•°æ®æ ¼å¼ä¸å®Œæ•´: {tick}")
                                # ğŸ”´ ä¿®å¤ï¼šå³ä½¿æ•°æ®ä¸å®Œæ•´ä¹Ÿè¦ACKï¼Œé¿å…pendingå †ç§¯
                                redis_client.xack(current_stream_key, group_name, message_id)
                                continue
                            
                            # æ¨é€ç»™æ‰€æœ‰å·²è¿æ¥çš„å®¢æˆ·ç«¯ï¼ˆå¸¦å»é‡ï¼‰
                            for sid, client_info in list(clients.items()):
                                last_time = client_info.get('last_time', 0)
                                
                                if tick['time_msc'] > last_time:
                                    socketio.emit('tick_update', tick, room=sid)
                                    clients[sid]['last_time'] = tick['time_msc']
                            
                            # ACKç¡®è®¤æ¶ˆæ¯ï¼ˆRedis Streamæœ€ä½³å®è·µï¼‰
                            # ğŸ”´ ä¿®å¤ï¼šä½¿ç”¨current_stream_keyè€Œä¸æ˜¯stream_keyï¼Œå› ä¸ºå¯èƒ½å·²åˆ‡æ¢åˆ°æ—§Stream
                            redis_client.xack(current_stream_key, group_name, message_id)
                            
                        except json.JSONDecodeError as e:
                            logger.error(f"JSONè§£æå¤±è´¥: {e}, message_id={message_id}")
                            # ğŸ”´ ä¿®å¤ï¼šå³ä½¿è§£æå¤±è´¥ä¹Ÿè¦ACKï¼Œé¿å…pendingå †ç§¯
                            try:
                                redis_client.xack(current_stream_key, group_name, message_id)
                            except:
                                pass
                        except Exception as e:
                            logger.error(f"å¤„ç†æ¶ˆæ¯å¤±è´¥: {e}, message_id={message_id}")
                            # ğŸ”´ ä¿®å¤ï¼šå³ä½¿å¤„ç†å¤±è´¥ä¹Ÿè¦ACKï¼Œé¿å…pendingå †ç§¯
                            try:
                                redis_client.xack(current_stream_key, group_name, message_id)
                            except:
                                pass
            
            # ã€æœ€ä½³å®è·µã€‘å®šæœŸæ¨é€Kçº¿æ•°æ®ï¼ˆæ¯60ç§’ï¼‰ï¼Œä½†ä¸é˜»å¡ä¸»å¾ªç¯
            # ğŸ”´ ä¿®å¤ï¼šä½¿ç”¨try-exceptåŒ…è£¹ï¼Œé¿å…Redisæ“ä½œé˜»å¡äº‹ä»¶å¾ªç¯
            current_time = time.time()
            if current_time - last_kline_push >= 60:
                try:
                    klines = get_kline_data_from_redis('BTCUSDm', '1m', 30)  # ä¿®æ­£ï¼štimeframe='1m', count=30
                    if klines:
                        socketio.emit('kline_update', klines)
                        last_kline_push = current_time
                except Exception as e:
                    logger.warning(f"æ¨é€Kçº¿æ•°æ®å¤±è´¥: {e}")
                    # å³ä½¿å¤±è´¥ä¹Ÿæ›´æ–°æ—¶é—´ï¼Œé¿å…é¢‘ç¹é‡è¯•
                    last_kline_push = current_time
            
            # ã€Flask-SocketIOæœ€ä½³å®è·µã€‘ä½¿ç”¨ socketio.sleep() è®©å‡ºæ§åˆ¶æƒï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            # ä¸è¦ä½¿ç”¨ time.sleep()ï¼
            socketio.sleep(0.001)  # 1msï¼Œè®©äº‹ä»¶å¾ªç¯æœ‰æœºä¼šå¤„ç†å…¶ä»–äº‹ä»¶
                    
        except Exception as e:
            logger.error(f"Streamè¯»å–å¤±è´¥: {e}")
            socketio.sleep(1)  # å‡ºé”™æ—¶ç­‰å¾…1ç§’å†é‡è¯•


# ==================== MT5å›è°ƒæ¥å£ ====================

@app.route('/api/mt5/callback', methods=['POST'])
def mt5_callback():
    """
    MT5ä¸­ç»§æœåŠ¡å›è°ƒæ¥å£
    
    æ¥æ”¶MT5ä¸­ç»§æœåŠ¡æ¨é€çš„å®æ—¶æ›´æ–°ï¼š
    1. æ–°æˆäº¤ï¼ˆdealï¼‰
    2. æŒä»“å˜åŒ–ï¼ˆpositions_updateï¼‰
    """
    try:
        data = request.json
        callback_type = data.get('type')
        
        if callback_type == 'deal':
            # æ–°æˆäº¤ï¼Œæ¨é€è®¢å•æ›´æ–°
            order_data = data.get('order')
            deal_data = data.get('deal', {})
            
            if order_data:
                logger.info(f"æ”¶åˆ°MT5æˆäº¤å›è°ƒ: ticket={deal_data.get('ticket')}, order={deal_data.get('order')}")
                
                # ç›´æ¥ä½¿ç”¨è½¬æ¢åçš„è®¢å•æ•°æ®
                socketio.emit('order_update', order_data)
                logger.debug(f"æ¨é€è®¢å•æ›´æ–°: {order_data.get('orderId')}")
        
        elif callback_type == 'position_update':
            # å•ä¸ªæŒä»“æ›´æ–°ï¼ˆæµ®åŠ¨ç›ˆäºå˜åŒ–ï¼‰
            position = data.get('position')
            if position:
                socketio.emit('position_update', position)
                logger.debug(f"æ¨é€æŒä»“æ›´æ–°: {position.get('positionId')}, ç›ˆäº={position.get('unrealizedPnL', 0):.2f}")
        
        elif callback_type == 'positions_update':
            # æŒä»“å˜åŒ–ï¼Œæ¨é€æ‰€æœ‰æŒä»“æ›´æ–°
            positions = data.get('positions', [])
            logger.debug(f"æ”¶åˆ°MT5æŒä»“æ›´æ–°å›è°ƒ: {len(positions)}ä¸ªæŒä»“")
            
            for pos in positions:
                frontend_position = format_position_for_frontend(pos)
                socketio.emit('position_update', frontend_position)
        
        return jsonify({'success': True, 'message': 'å›è°ƒå¤„ç†æˆåŠŸ'})
    
    except Exception as e:
        logger.error(f"å¤„ç†MT5å›è°ƒå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ==================== å¸ƒå±€é…ç½®ç®¡ç† ====================

# é…ç½®æ–‡ä»¶ç›®å½•
CONFIG_DIR = BASE_DIR / "gui" / "user_configs"
CONFIG_DIR.mkdir(parents=True, exist_ok=True)

@app.route('/api/layout/save', methods=['POST'])
def save_layout():
    """ä¿å­˜ç”¨æˆ·å¸ƒå±€é…ç½®"""
    try:
        data = request.json
        user_id = data.get('userId', 'default')
        page = data.get('page', 'trading')
        config = data.get('config', {})
        
        config_file = CONFIG_DIR / f'{user_id}_{page}_layout.json'
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ“ å¸ƒå±€é…ç½®å·²ä¿å­˜: {user_id}/{page}")
        return jsonify({'success': True, 'message': 'å¸ƒå±€ä¿å­˜æˆåŠŸ'})
    
    except Exception as e:
        logger.error(f"ä¿å­˜å¸ƒå±€é…ç½®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/layout/load', methods=['GET'])
def load_layout():
    """åŠ è½½ç”¨æˆ·å¸ƒå±€é…ç½®"""
    try:
        user_id = request.args.get('userId', 'default')
        page = request.args.get('page', 'trading')
        
        config_file = CONFIG_DIR / f'{user_id}_{page}_layout.json'
        
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"âœ“ å¸ƒå±€é…ç½®å·²åŠ è½½: {user_id}/{page}")
            return jsonify({'success': True, 'config': config})
        else:
            logger.info(f"æœªæ‰¾åˆ°å¸ƒå±€é…ç½®: {user_id}/{page}")
            return jsonify({'success': True, 'config': None})
    
    except Exception as e:
        logger.error(f"åŠ è½½å¸ƒå±€é…ç½®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ==================== è®¢å•ç®¡ç†API ====================

@app.route('/api/orders/create', methods=['POST'])
def create_order():
    """
    åˆ›å»ºè®¢å•
    
    è¯·æ±‚å‚æ•°:
    {
        "symbol": "BTCUSDm",
        "action": "ä¹°å…¥ (å¤š)" | "å–å‡º (ç©º)" | "å¹³å¤š" | "å¹³ç©º",
        "price": 100000.0,
        "klineTime": 1234567890,
        "volume": 0.01
    }
    """
    try:
        # ğŸ”´ å®‰å…¨æœºåˆ¶ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºç”Ÿäº§æ¨¡å¼
        try:
            require_production_mode("åˆ›å»ºè®¢å•")
        except EnvironmentError as e:
            logger.error(f"APIè®¢å•åˆ›å»ºè¢«é˜»æ­¢: {str(e)}")
            return jsonify({
                'success': False,
                'error': 'ENVIRONMENT_BLOCKED',
                'message': f'éç”Ÿäº§ç¯å¢ƒï¼Œè®¢å•åˆ›å»ºå·²é˜»æ­¢ã€‚å½“å‰ç¯å¢ƒ: {get_env_info().get("env", "UNKNOWN")}',
                'env_info': get_env_info()
            }), 403  # 403 Forbidden
        
        data = request.json
        
        symbol = data.get('symbol', 'BTCUSDm')
        action = data.get('action', '')
        price = float(data.get('price', 0))
        kline_time = int(data.get('klineTime', 0))
        volume = float(data.get('volume', 0.01))
        
        # ğŸ”´ ä¿®å¤ï¼šæ”¯æŒæ ‡å‡†è‹±æ–‡å‚æ•°ï¼ˆBUY/SELLï¼‰ï¼Œå…¼å®¹ä¸­æ–‡å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
        action_upper = action.upper()
        if action_upper == 'BUY' or 'ä¹°å…¥' in action or 'åšå¤š' in action:
            order_type = OrderEngine.ORDER_TYPE_BUY
            comment = 'ä¹°å…¥å¼€ä»“'
        elif action_upper == 'SELL' or 'å–å‡º' in action or 'åšç©º' in action:
            order_type = OrderEngine.ORDER_TYPE_SELL
            comment = 'å–å‡ºå¼€ä»“'
        elif 'å¹³å¤š' in action:
            # å¹³å¤šï¼šéœ€è¦æ‰¾åˆ°å¯¹åº”çš„å¤šå•æŒä»“å¹¶å¹³ä»“
            positions = order_engine.get_all_positions()
            long_position = next((p for p in positions if p['type'] == OrderEngine.ORDER_TYPE_BUY), None)
            
            if long_position:
                close_order = order_engine.close_position(long_position['ticket'], price, kline_time)
                if close_order:
                    logger.info(f"âœ“ å¹³å¤šæˆåŠŸ: ç¥¨å·={close_order['ticket']}, ä»·æ ¼={price}")
                    return jsonify({
                        'success': True,
                        'order': close_order,
                        'message': 'å¹³å¤šæˆåŠŸ'
                    })
                else:
                    return jsonify({'success': False, 'error': 'å¹³ä»“å¤±è´¥'}), 400
            else:
                return jsonify({'success': False, 'error': 'æ²¡æœ‰å¤šå•æŒä»“'}), 400
                
        elif 'å¹³ç©º' in action:
            # å¹³ç©ºï¼šéœ€è¦æ‰¾åˆ°å¯¹åº”çš„ç©ºå•æŒä»“å¹¶å¹³ä»“
            positions = order_engine.get_all_positions()
            short_position = next((p for p in positions if p['type'] == OrderEngine.ORDER_TYPE_SELL), None)
            
            if short_position:
                close_order = order_engine.close_position(short_position['ticket'], price, kline_time)
                if close_order:
                    logger.info(f"âœ“ å¹³ç©ºæˆåŠŸ: ç¥¨å·={close_order['ticket']}, ä»·æ ¼={price}")
                    return jsonify({
                        'success': True,
                        'order': close_order,
                        'message': 'å¹³ç©ºæˆåŠŸ'
                    })
                else:
                    return jsonify({'success': False, 'error': 'å¹³ä»“å¤±è´¥'}), 400
            else:
                return jsonify({'success': False, 'error': 'æ²¡æœ‰ç©ºå•æŒä»“'}), 400
        else:
            return jsonify({'success': False, 'error': 'æœªçŸ¥æ“ä½œç±»å‹'}), 400
        
        # ğŸš€ æ¨é€åˆ°MT5æ‰§è¡Œï¼ˆé€šè¿‡Redis Streamï¼‰
        # å†™å…¥ l3:manual:commands Streamï¼Œç”± OrderExecutor ç›‘å¬å¹¶æ‰§è¡Œ
        manual_command = {
            'action': 'BUY' if order_type == OrderEngine.ORDER_TYPE_BUY else 'SELL',
            'symbol': symbol,
            'price': price if price > 0 else 0.0,  # å¸‚ä»·å•ä¸º0ï¼ŒMT5ä¼šä½¿ç”¨å½“å‰å¸‚ä»·
            'volume': volume,
            'sl': 0.0,  # æ­¢æŸï¼ˆå½“å‰æœªå®ç°ï¼‰
            'tp': 0.0,  # æ­¢ç›ˆï¼ˆå½“å‰æœªå®ç°ï¼‰
            'source': 'MANUAL',  # æ ‡è®°ä¸ºäººå·¥è®¢å•
            'timestamp': int(time.time()),
            'klineTime': kline_time,
        }
        
        # å†™å…¥Redis Stream
        try:
            redis_client.xadd('l3:manual:commands', manual_command, maxlen=1000)
            logger.info(f"âœ“ è®¢å•å·²æ¨é€åˆ°MT5æ‰§è¡Œé˜Ÿåˆ—: {manual_command['action']} {symbol} {volume}æ‰‹ @ {price}")
        except Exception as e:
            logger.error(f"æ¨é€è®¢å•åˆ°MT5é˜Ÿåˆ—å¤±è´¥: {e}")
            return jsonify({'success': False, 'error': f'è®¢å•æ¨é€å¤±è´¥: {str(e)}'}), 500
        
        # è¿”å›å“åº”ï¼ˆè®¢å•å°†åœ¨MT5æ‰§è¡Œåé€šè¿‡Socket.IOæ¨é€æ›´æ–°ï¼‰
        return jsonify({
            'success': True,
            'message': 'è®¢å•å·²æäº¤åˆ°MT5æ‰§è¡Œé˜Ÿåˆ—',
            'command': manual_command
        })
    
    except Exception as e:
        logger.error(f"åˆ›å»ºè®¢å•å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/orders', methods=['GET'])
@app.route('/api/orders/open', methods=['GET'])  # ğŸ”´ ä¿®å¤ï¼šæ·»åŠ åˆ«åï¼Œå…¼å®¹å‰ç«¯è°ƒç”¨
def get_orders():
    """è·å–å½“å‰æŒ‚å•ï¼ˆPending Ordersï¼‰"""
    try:
        # ğŸš€ å¿«é€Ÿå“åº”ï¼šå¦‚æœorder_engineä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        if order_engine is None:
            return jsonify({
                'success': True,
                'orders': [],
                'count': 0
            })
        
        # ğŸ”´ ä¿®å¤ï¼šæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…gRPCè°ƒç”¨é˜»å¡
        import concurrent.futures
        all_orders = []
        positions = []
        
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œè®¾ç½®è¶…æ—¶ä¿æŠ¤ï¼ˆ2ç§’ï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                # æäº¤ä»»åŠ¡
                future_orders = executor.submit(order_engine.get_all_orders)
                future_positions = executor.submit(order_engine.get_all_positions)
                
                # ç­‰å¾…ç»“æœï¼Œè®¾ç½®è¶…æ—¶ï¼ˆ2ç§’ï¼‰
                try:
                    all_orders = future_orders.result(timeout=2.0)
                    positions = future_positions.result(timeout=2.0)
                except concurrent.futures.TimeoutError:
                    logger.warning(f"è·å–è®¢å•/æŒä»“è¶…æ—¶ï¼ˆ2ç§’ï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                    all_orders = []
                    positions = []
        except Exception as e:
            logger.warning(f"è·å–è®¢å•/æŒä»“å¤±è´¥ï¼ˆgRPCå¯èƒ½è¶…æ—¶ï¼‰: {e}")
            all_orders = []
            positions = []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸é˜»å¡API
        
        # è·å–å½“å‰æŒä»“çš„position_idåˆ—è¡¨
        position_ids = {pos['ticket'] for pos in positions}
        
        # ç­›é€‰æŒ‚å•ï¼šstate=1ï¼ˆå·²ä¸‹å•ä½†æœªæˆäº¤ï¼‰ä¸”ä¸åœ¨æŒä»“ä¸­
        pending_orders = [
            order for order in all_orders 
            if order.get('state') == 1 and order.get('position_id', 0) not in position_ids
        ]
        
        return jsonify({
            'success': True,
            'orders': pending_orders,
            'count': len(pending_orders)
        })
                
    except Exception as e:
        logger.error(f"è·å–æŒ‚å•å¤±è´¥: {e}")
        # ğŸš€ é™çº§ï¼šè¿”å›æˆåŠŸä½†ç©ºåˆ—è¡¨ï¼Œé¿å…å‰ç«¯æ˜¾ç¤ºé”™è¯¯
        return jsonify({
            'success': True,
            'orders': [],
            'count': 0
        })


@app.route('/api/orders/positions', methods=['GET'])
def get_positions():
    """
    è·å–æ‰€æœ‰æŒä»“ï¼ˆç»Ÿä¸€è¿”å›å‰ç«¯æ ¼å¼ï¼‰
    
    ä½¿ç”¨ç»Ÿä¸€çš„ format_position_for_frontend å‡½æ•°ç¡®ä¿ä¸ WebSocket æ¨é€æ ¼å¼ä¸€è‡´ã€‚
    """
    try:
        # ğŸš€ å¿«é€Ÿå“åº”ï¼šå¦‚æœorder_engineä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        if order_engine is None:
            return jsonify({
                'success': True,
                'data': []
            })
        
        # ğŸ”´ ä¿®å¤ï¼šæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…gRPCè°ƒç”¨é˜»å¡
        import concurrent.futures
        positions = []
        
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œè®¾ç½®è¶…æ—¶ä¿æŠ¤ï¼ˆ2ç§’ï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                # æäº¤ä»»åŠ¡
                future = executor.submit(order_engine.get_all_positions)
                
                # ç­‰å¾…ç»“æœï¼Œè®¾ç½®è¶…æ—¶ï¼ˆ2ç§’ï¼‰
                try:
                    positions = future.result(timeout=2.0)
                except concurrent.futures.TimeoutError:
                    logger.warning(f"è·å–æŒä»“è¶…æ—¶ï¼ˆ2ç§’ï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                    positions = []
        except Exception as e:
            logger.warning(f"è·å–æŒä»“å¤±è´¥ï¼ˆgRPCå¯èƒ½è¶…æ—¶ï¼‰: {e}")
            positions = []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸é˜»å¡API
        
        # ğŸ”´ ç»Ÿä¸€è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼ï¼ˆä¸WebSocketæ¨é€æ ¼å¼ä¸€è‡´ï¼‰
        frontend_positions = [format_position_for_frontend(pos) for pos in positions]
        
        logger.debug(f"ğŸ“¤ HTTP API è¿”å›æŒä»“ {len(frontend_positions)} ä¸ª (å·²è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼)")
        
        return jsonify({
            'success': True,
            'data': frontend_positions
        })
                
    except Exception as e:
        logger.error(f"âŒ è·å–æŒä»“å¤±è´¥: {e}")
        # ğŸš€ é™çº§ï¼šè¿”å›æˆåŠŸä½†ç©ºåˆ—è¡¨ï¼Œé¿å…å‰ç«¯æ˜¾ç¤ºé”™è¯¯
        return jsonify({
            'success': True,
            'data': []
        })


@app.route('/api/orders/history', methods=['GET'])
def get_order_history():
    """
    è·å–å†å²è®¢å•ï¼ˆåªè¿”å›å·²å¹³ä»“çš„è®¢å•ï¼‰
    
    ğŸš€ ä¼˜åŒ–ï¼šå¿«é€Ÿå“åº”ï¼Œä¼˜å…ˆä½¿ç”¨Redisç¼“å­˜ï¼ŒgRPCè°ƒç”¨å¸¦è¶…æ—¶ä¿æŠ¤
    """
    try:
        # ğŸš€ å¿«é€Ÿå“åº”ï¼šå¦‚æœorder_engineä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        if order_engine is None:
            return jsonify({
                'success': True,
                'data': []
            })
        
        # ä¼˜å…ˆä»Redisç¼“å­˜è¯»å–ï¼Œé¿å…gRPCé˜»å¡
        all_orders = []
        positions = []
        
        # æ–¹æ³•1: å°è¯•ä»Redisç¼“å­˜å¿«é€Ÿè¯»å–ï¼ˆO(1)æ“ä½œï¼‰
        try:
            # ç›´æ¥è°ƒç”¨å†…éƒ¨æ–¹æ³•ï¼Œé¿å…gRPCè°ƒç”¨
            all_orders = order_engine._get_orders_from_redis_cache()
            positions = order_engine._get_positions_from_redis_cache()
            logger.debug(f"ä»Redisç¼“å­˜è¯»å–: {len(all_orders)} è®¢å•, {len(positions)} æŒä»“")
        except Exception as e:
            logger.debug(f"Redisç¼“å­˜è¯»å–å¤±è´¥: {e}ï¼Œå°è¯•gRPC")
        
        # æ–¹æ³•2: å¦‚æœç¼“å­˜ä¸ºç©ºï¼Œå°è¯•å¿«é€ŸgRPCè°ƒç”¨ï¼ˆå¸¦è¶…æ—¶ï¼Œæœ€å¤š1ç§’ï¼‰
        if not all_orders or not positions:
            try:
                # ğŸš€ ä½¿ç”¨è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…é˜»å¡ï¼ˆæœ€å¤š1ç§’ï¼‰
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                    future_orders = executor.submit(order_engine.get_all_orders)
                    future_positions = executor.submit(order_engine.get_all_positions)
                    
                    try:
                        all_orders = future_orders.result(timeout=1.0) or all_orders  # å‡å°‘åˆ°1ç§’
                        positions = future_positions.result(timeout=1.0) or positions
                    except concurrent.futures.TimeoutError:
                        logger.warning("gRPCè°ƒç”¨è¶…æ—¶ï¼ˆ1ç§’ï¼‰ï¼Œä½¿ç”¨Redisç¼“å­˜æ•°æ®æˆ–ç©ºåˆ—è¡¨")
                        # å¦‚æœè¶…æ—¶ï¼Œä½¿ç”¨å·²æœ‰æ•°æ®æˆ–ç©ºåˆ—è¡¨
                        all_orders = all_orders or []
                        positions = positions or []
            except Exception as e:
                logger.warning(f"gRPCè°ƒç”¨å¤±è´¥: {e}ï¼Œä½¿ç”¨Redisç¼“å­˜æ•°æ®æˆ–ç©ºåˆ—è¡¨")
                # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨å·²æœ‰æ•°æ®æˆ–ç©ºåˆ—è¡¨
                all_orders = all_orders or []
                positions = positions or []
        
        # è·å–å½“å‰æŒä»“çš„position_idåˆ—è¡¨ï¼ˆå®‰å…¨å¤„ç†ï¼‰
        position_ids = set()
        for pos in positions:
            try:
                ticket = pos.get('ticket') or pos.get('position_id')
                if ticket:
                    position_ids.add(ticket)
            except Exception:
                continue
        
        # åªè¿”å›ä¸åœ¨æŒä»“ä¸­çš„è®¢å•ï¼ˆå·²å¹³ä»“çš„ï¼‰
        history_orders = []
        for order in all_orders:
            try:
                order_position_id = order.get('position_id') or order.get('ticket')
                if order_position_id and order_position_id not in position_ids:
                    history_orders.append(order)
            except Exception as e:
                logger.debug(f"å¤„ç†è®¢å•æ—¶å‡ºé”™: {e}ï¼Œè·³è¿‡è¯¥è®¢å•")
                continue
        
        return jsonify({
            'success': True,
            'data': history_orders
        })
    except Exception as e:
        logger.error(f"è·å–å†å²è®¢å•å¤±è´¥: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/orders/deals', methods=['GET'])
def get_deals():
    """è·å–æˆäº¤è®°å½•"""
    try:
        deals = order_engine.get_all_deals()
        return jsonify({
            'success': True,
            'data': deals
        })
    except Exception as e:
        logger.error(f"è·å–æˆäº¤è®°å½•å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/orders/close', methods=['POST'])
def close_position():
    """
    å¹³ä»“
    
    ğŸš€ ä¼˜åŒ–ï¼šæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…gRPCè°ƒç”¨é˜»å¡
    """
    try:
        # ğŸš€ å¿«é€Ÿå“åº”ï¼šå¦‚æœorder_engineä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›é”™è¯¯
        if order_engine is None:
            return jsonify({
                'success': False,
                'error': 'è®¢å•å¼•æ“ä¸å¯ç”¨'
            }), 503
        
        data = request.json
        position_id = data.get('position_id')
        close_price = data.get('close_price')
        kline_time = data.get('kline_time')
        
        # å‚æ•°éªŒè¯ï¼šposition_id æ˜¯å¿…éœ€çš„ï¼Œclose_price å’Œ kline_time å¯é€‰ï¼ˆåç«¯ä¼šä½¿ç”¨å½“å‰å€¼ï¼‰
        if not position_id:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: position_id'
            }), 400
        
        # ğŸš€ ä½¿ç”¨è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…gRPCè°ƒç”¨é˜»å¡ï¼ˆæœ€å¤š3ç§’ï¼‰
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(
                order_engine.close_position,
                position_id=int(position_id),
                close_price=float(close_price) if close_price else 0,
                kline_time=int(kline_time) if kline_time else 0
            )
            
            try:
                closed_order = future.result(timeout=3.0)  # 3ç§’è¶…æ—¶
            except concurrent.futures.TimeoutError:
                logger.warning(f"å¹³ä»“æ“ä½œè¶…æ—¶ï¼ˆ3ç§’ï¼‰: position_id={position_id}")
                return jsonify({
                    'success': False,
                    'error': 'å¹³ä»“æ“ä½œè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'
                }), 504
        
        if closed_order:
            logger.info(f"âœ“ å¹³ä»“æˆåŠŸ: position_id={position_id}, price={close_price}")
            return jsonify({
                'success': True,
                'order': closed_order,
                'message': 'å¹³ä»“æˆåŠŸ'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'æŒä»“ä¸å­˜åœ¨æˆ–å¹³ä»“å¤±è´¥'
            }), 404
            
    except Exception as e:
        logger.error(f"å¹³ä»“å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/orders/clear', methods=['DELETE'])
def clear_orders():
    """æ¸…é™¤æ‰€æœ‰è®¢å•æ•°æ®"""
    try:
        order_engine.clear_all_orders()
        logger.info("å·²æ¸…é™¤æ‰€æœ‰è®¢å•æ•°æ®")
        return jsonify({
            'success': True,
            'message': 'å·²æ¸…é™¤æ‰€æœ‰è®¢å•æ•°æ®'
        })
    except Exception as e:
        logger.error(f"æ¸…é™¤è®¢å•æ•°æ®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ==================== è®­ç»ƒé…ç½®API ====================

@app.route('/api/training/config', methods=['GET', 'POST'])
def training_config():
    """è®­ç»ƒé…ç½®ç®¡ç†"""
    try:
        if request.method == 'GET':
            # è·å–è®­ç»ƒé…ç½®
            config_data = redis_client.get('training:config')
            if config_data:
                config = json.loads(config_data)
                logger.info("âœ“ è¯»å–è®­ç»ƒé…ç½®")
                return jsonify({
                    'success': True,
                    'time_windows': config.get('time_windows', []),
                    'primary_window': config.get('primary_window', 'medium'),
                    'use_open_price': config.get('use_open_price', True)
                })
            else:
                # è¿”å›é»˜è®¤é…ç½®
                from config.model_config import TIME_WINDOWS, DATA_PREPARATION_CONFIG
                logger.info("âš ï¸  è®­ç»ƒé…ç½®ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤é…ç½®")
                return jsonify({
                    'success': True,
                    'time_windows': TIME_WINDOWS,
                    'primary_window': DATA_PREPARATION_CONFIG['primary_window'],
                    'use_open_price': DATA_PREPARATION_CONFIG['use_open_price']
                })
        
        elif request.method == 'POST':
            # ä¿å­˜è®­ç»ƒé…ç½®
            config_data = request.json
            redis_client.set('training:config', json.dumps(config_data))
            logger.info(f"âœ“ ä¿å­˜è®­ç»ƒé…ç½®: {len(config_data.get('time_windows', []))} ä¸ªæ—¶é—´çª—å£")
            return jsonify({
                'success': True,
                'message': 'è®­ç»ƒé…ç½®å·²ä¿å­˜'
            })
    
    except Exception as e:
        logger.error(f"è®­ç»ƒé…ç½®APIå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/training/images/<path:filename>')
def serve_training_image(filename):
    """æä¾›è®­ç»ƒæ•°æ®å›¾ç‰‡è®¿é—®"""
    try:
        image_dir = BASE_DIR / 'data' / 'images'
        return send_from_directory(image_dir, filename)
    except Exception as e:
        logger.error(f"å›¾ç‰‡è®¿é—®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 404


# ==================== AIåˆ†èº«é…ç½®API ====================

@app.route('/api/ai-avatars', methods=['GET'])
def get_ai_avatars():
    """è·å–æ‰€æœ‰AIåˆ†èº«é…ç½®"""
    try:
        from config.ai_avatars import get_all_avatars
        
        avatars = get_all_avatars()
        logger.info(f"âœ“ è¿”å› {len(avatars)} ä¸ªAIåˆ†èº«é…ç½®")
        
        return jsonify({
            'success': True,
            'avatars': avatars
        })
    except Exception as e:
        logger.error(f"è·å–AIåˆ†èº«é…ç½®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/ai-avatars/save', methods=['POST'])
def save_ai_avatar():
    """ä¿å­˜AIåˆ†èº«é…ç½®"""
    try:
        data = request.get_json()
        avatar = data.get('avatar')
        config = data.get('config')
        
        if not avatar or not config:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        # ä¿å­˜åˆ°Redis
        avatar_key = f"ai_avatar:{avatar['id']}"
        avatar_data = {
            'id': avatar['id'],
            'name': avatar['name'],
            'description': avatar['description'],
            'icon': avatar['icon'],
            'color': avatar['color'],
            'config': json.dumps(config)
        }
        
        redis_client.hset(avatar_key, mapping=avatar_data)
        logger.info(f"âœ“ AIåˆ†èº«é…ç½®å·²ä¿å­˜: {avatar['name']}")
        
        return jsonify({'success': True})
    except Exception as e:
        logger.error(f"ä¿å­˜AIåˆ†èº«é…ç½®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/ai-avatars/<avatar_id>/train', methods=['POST'])
def start_ai_training(avatar_id):
    """å¯åŠ¨AIåˆ†èº«è®­ç»ƒ"""
    try:
        # è·å–AIåˆ†èº«é…ç½®
        avatar_key = f"ai_avatar:{avatar_id}"
        avatar_data = redis_client.hgetall(avatar_key)
        
        if not avatar_data:
            return jsonify({'success': False, 'error': 'AIåˆ†èº«ä¸å­˜åœ¨'}), 404
        
        # æ›´æ–°è®­ç»ƒçŠ¶æ€
        training_key = f"ai_training:{avatar_id}"
        training_data = {
            'status': 'training',
            'start_time': str(int(time.time())),
            'progress': 0
        }
        redis_client.hset(training_key, mapping=training_data)
        
        # å¯åŠ¨å¼‚æ­¥è®­ç»ƒ
        from training.ai_avatar_trainer import train_avatar_async
        train_avatar_async(avatar_id)
        
        logger.info(f"âœ“ AIåˆ†èº« {avatar_id} è®­ç»ƒå·²å¯åŠ¨")
        
        return jsonify({'success': True})
    except Exception as e:
        logger.error(f"å¯åŠ¨AIè®­ç»ƒå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/ai-avatars/<avatar_id>/stop', methods=['POST'])
def stop_ai_training(avatar_id):
    """åœæ­¢AIåˆ†èº«è®­ç»ƒ"""
    try:
        training_key = f"ai_training:{avatar_id}"
        redis_client.hset(training_key, 'status', 'stopped')
        
        logger.info(f"âœ“ AIåˆ†èº« {avatar_id} è®­ç»ƒå·²åœæ­¢")
        
        return jsonify({'success': True})
    except Exception as e:
        logger.error(f"åœæ­¢AIè®­ç»ƒå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/ai-avatars/battle-stats', methods=['GET'])
def get_battle_stats():
    """è·å–AIåˆ†èº«å¯¹æˆ˜ç»Ÿè®¡"""
    try:
        stats = {}
        
        # ä»Redisè·å–æ‰€æœ‰AIåˆ†èº«çš„ç»Ÿè®¡ä¿¡æ¯
        avatar_keys = redis_client.keys("ai_avatar:*")
        
        for avatar_key in avatar_keys:
            avatar_id = avatar_key.split(':')[1]
            stats_key = f"ai_stats:{avatar_id}"
            
            # è·å–ç»Ÿè®¡æ•°æ®
            stats_data = redis_client.hgetall(stats_key)
            if stats_data:
                # è½¬æ¢æ•°æ®ç±»å‹
                stats[avatar_id] = {
                    'avatar_id': avatar_id,
                    'total_trades': int(stats_data.get('total_trades', 0)),
                    'win_trades': int(stats_data.get('win_trades', 0)),
                    'lose_trades': int(stats_data.get('lose_trades', 0)),
                    'win_rate': float(stats_data.get('win_rate', 0)),
                    'total_profit': float(stats_data.get('total_profit', 0)),
                    'avg_profit_per_trade': float(stats_data.get('avg_profit_per_trade', 0)),
                    'max_profit': float(stats_data.get('max_profit', 0)),
                    'max_loss': float(stats_data.get('max_loss', 0)),
                    'sharpe_ratio': float(stats_data.get('sharpe_ratio', 0)),
                    'max_drawdown': float(stats_data.get('max_drawdown', 0)),
                    'profit_factor': float(stats_data.get('profit_factor', 0)),
                    'training_samples': int(stats_data.get('training_samples', 0)),
                    'model_accuracy': float(stats_data.get('model_accuracy', 0)),
                    'training_time': float(stats_data.get('training_time', 0)),
                    'last_updated': stats_data.get('last_updated')
                }
            else:
                # è¿”å›é»˜è®¤ç»Ÿè®¡
                stats[avatar_id] = {
                    'avatar_id': avatar_id,
                    'total_trades': 0,
                    'win_trades': 0,
                    'lose_trades': 0,
                    'win_rate': 0,
                    'total_profit': 0,
                    'avg_profit_per_trade': 0,
                    'max_profit': 0,
                    'max_loss': 0,
                    'sharpe_ratio': 0,
                    'max_drawdown': 0,
                    'profit_factor': 0,
                    'training_samples': 0,
                    'model_accuracy': 0,
                    'training_time': 0,
                    'last_updated': None
                }
        
        logger.info(f"âœ“ è¿”å› {len(stats)} ä¸ªAIåˆ†èº«çš„å¯¹æˆ˜ç»Ÿè®¡")
        
        return jsonify({
            'success': True,
            'stats': stats
        })
    except Exception as e:
        logger.error(f"è·å–å¯¹æˆ˜ç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/training/generate', methods=['POST'])
def generate_training_data():
    """ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆç›´æ¥åœ¨Flaskè¿›ç¨‹å†…æ‰§è¡Œï¼Œé¿å…subprocessé˜»å¡ï¼‰"""
    try:
        import importlib.util
        
        logger.info("å¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆFlaskè¿›ç¨‹å†…æ‰§è¡Œï¼‰")
        
        # ğŸ”¥ ç›´æ¥åŠ è½½å¹¶æ‰§è¡Œprepare_data.pyæ¨¡å—
        prepare_data_path = BASE_DIR / 'training' / 'prepare_data.py'
        
        spec = importlib.util.spec_from_file_location("prepare_data", prepare_data_path)
        prepare_data_module = importlib.util.module_from_spec(spec)
        
        # æ‰§è¡Œæ¨¡å—ï¼ˆä¼šè‡ªåŠ¨è°ƒç”¨main()ï¼‰
        spec.loader.exec_module(prepare_data_module)
        
        # è¯»å–ç”Ÿæˆçš„å…ƒæ•°æ®
        metadata_path = BASE_DIR / 'data' / 'training_metadata.json'
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                training_samples = json.load(f)
            
            logger.info(f"[OK] ç”Ÿæˆè®­ç»ƒæ•°æ®æˆåŠŸ: {len(training_samples)} ä¸ªæ ·æœ¬")
            
            return jsonify({
                'success': True,
                'samples': training_samples,
                'message': f'æˆåŠŸç”Ÿæˆ {len(training_samples)} ä¸ªè®­ç»ƒæ ·æœ¬'
            })
        else:
            logger.error("å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return jsonify({
                'success': False,
                'error': 'è®­ç»ƒæ•°æ®æœªç”Ÿæˆ'
            }), 500
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆè®­ç»ƒæ•°æ®å¼‚å¸¸: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500


# ==================== MT5 API ====================

# âŒ å·²åˆ é™¤ï¼šæ‰€æœ‰/api/mt5/*ç›¸å…³çš„APIç«¯ç‚¹ï¼ˆå…±7ä¸ªï¼‰
# ç°åœ¨ä½¿ç”¨ï¼šæ•°æ®é‡‡é›†å™¨ + Redis + WebSocket

# ==================== Redisæ•°æ®API ====================

@app.route('/api/redis/tick/<symbol>', methods=['GET'])
def get_tick_from_redis(symbol):
    """ä»Redisè·å–TICKæ•°æ®"""
    try:
        # ä»Redisè·å–æœ€æ–°çš„TICKæ•°æ®
        tick_key = f"tick:{symbol.upper()}"
        tick_data = redis_client.hgetall(tick_key)
        
        if not tick_data:
            return jsonify({
                'success': False,
                'error': f'æœªæ‰¾åˆ° {symbol} çš„TICKæ•°æ®'
            }), 404
        
        # è½¬æ¢æ•°æ®ç±»å‹
        tick = {
            'symbol': tick_data.get('symbol', symbol.upper()),
            'time': tick_data.get('time', ''),
            'bid': float(tick_data.get('bid', 0)),
            'ask': float(tick_data.get('ask', 0)),
            'last': float(tick_data.get('last', 0)),
            'volume': int(tick_data.get('volume', 0)),
            'spread': float(tick_data.get('spread', 0)),
            'change': float(tick_data.get('change', 0)),
            'change_percent': float(tick_data.get('change_percent', 0))
        }
        
        return jsonify({
            'success': True,
            'tick': tick
        })
        
    except Exception as e:
        logger.error(f"ä»Redisè·å–TICKæ•°æ®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/redis/ticks/history/<symbol>', methods=['GET'])
def api_get_tick_history_from_redis(symbol):
    """API: ä»Redisè·å–TICKå†å²æ•°æ®"""
    try:
        # è·å–å‚æ•°
        limit = request.args.get('limit', 100, type=int)
        
        # ä»Redisè·å–å†å²TICKæ•°æ®
        history_key = f"tick_history:{symbol.upper()}"
        tick_list = redis_client.lrange(history_key, 0, limit - 1)
        
        ticks = []
        for tick_json in tick_list:
            try:
                tick_data = json.loads(tick_json)
                ticks.append(tick_data)
            except json.JSONDecodeError:
                continue
        
        return jsonify({
            'success': True,
            'ticks': ticks,
            'count': len(ticks)
        })
        
    except Exception as e:
        logger.error(f"ä»Redisè·å–TICKå†å²æ•°æ®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/assets', methods=['GET'])
def get_assets():
    """
    è·å–æ‰€æœ‰å¯äº¤æ˜“èµ„äº§åˆ—è¡¨
    
    ğŸš€ ä¼˜åŒ–ï¼šå¿«é€Ÿå“åº”ï¼Œé¿å…ä½¿ç”¨KEYSå‘½ä»¤ï¼ˆé˜»å¡æ“ä½œï¼‰
    
    è¿”å›æ ¼å¼ï¼š
    [
        {
            "symbol": "BTCUSDm",
            "name": "Bitcoin/USD",
            "baseCurrency": "BTC",
            "quoteCurrency": "USD",
            "minVolume": 0.01,
            "maxVolume": 100.0,
            "pricePrecision": 2,
            "volumePrecision": 2,
            "status": "ACTIVE"
        }
    ]
    """
    try:
        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨SCANæ›¿ä»£KEYSï¼Œé¿å…é˜»å¡ï¼ˆä½†ä¸ºäº†å¿«é€Ÿå“åº”ï¼Œç›´æ¥è¿”å›é»˜è®¤åˆ—è¡¨ï¼‰
        # KEYSå‘½ä»¤ä¼šé˜»å¡Redisï¼Œå¯¼è‡´APIè¶…æ—¶
        # ç›´æ¥è¿”å›é»˜è®¤èµ„äº§åˆ—è¡¨ï¼Œæˆ–ä»é…ç½®ä¸­è¯»å–
        symbols = {'BTCUSDm'}  # é»˜è®¤èµ„äº§
        
        # ğŸ”´ ä¿®å¤ï¼šç§»é™¤Redis Streamæ£€æŸ¥ï¼Œç›´æ¥è¿”å›é»˜è®¤åˆ—è¡¨ï¼Œé¿å…é˜»å¡
        # xinfo_streamå¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹é˜»å¡ï¼Œå¯¼è‡´APIè¶…æ—¶
        # ç›´æ¥ä½¿ç”¨é»˜è®¤èµ„äº§åˆ—è¡¨ï¼Œç¡®ä¿å¿«é€Ÿå“åº”
        
        # æ„å»ºèµ„äº§åˆ—è¡¨
        assets = []
        for symbol in sorted(symbols):
            # è§£æsymbolï¼Œæå–åŸºç¡€è´§å¸å’ŒæŠ¥ä»·è´§å¸
            base_currency = symbol.replace('USDm', '').replace('USD', '').replace('XAU', 'GOLD')
            quote_currency = 'USD'
            
            # æ ¹æ®symbolç±»å‹è®¾ç½®ç²¾åº¦
            if 'XAU' in symbol or 'GOLD' in symbol:
                price_precision = 2
            elif 'USD' in symbol:
                price_precision = 2
            else:
                price_precision = 5
            
            asset = {
                'symbol': symbol,
                'name': f'{base_currency}/{quote_currency}',
                'baseCurrency': base_currency,
                'quoteCurrency': quote_currency,
                'minVolume': 0.01,
                'maxVolume': 100.0,
                'pricePrecision': price_precision,
                'volumePrecision': 2,
                'status': 'ACTIVE'
            }
            assets.append(asset)
        
        logger.info(f"âœ“ è¿”å› {len(assets)} ä¸ªå¯äº¤æ˜“èµ„äº§")
        # è¿”å›ç»Ÿä¸€æ ¼å¼ï¼š{success: true, data: [...]}
        return jsonify({
            'success': True,
            'data': assets
        })
        
    except Exception as e:
        logger.error(f"è·å–èµ„äº§åˆ—è¡¨å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤èµ„äº§åˆ—è¡¨ï¼ˆè‡³å°‘ä¿è¯ç³»ç»Ÿå¯ç”¨ï¼‰
        default_assets = [{
            'symbol': 'BTCUSDm',
            'name': 'Bitcoin/USD',
            'baseCurrency': 'BTC',
            'quoteCurrency': 'USD',
            'minVolume': 0.01,
            'maxVolume': 100.0,
            'pricePrecision': 2,
            'volumePrecision': 2,
            'status': 'ACTIVE'
        }]
        return jsonify({
            'success': True,
            'data': default_assets
        })


@app.route('/api/account', methods=['GET'])
@app.route('/api/account/info', methods=['GET'])  # ğŸ”´ ä¿®å¤ï¼šæ·»åŠ åˆ«åï¼Œå…¼å®¹å‰ç«¯è°ƒç”¨
def get_account_info():
    """è·å–è´¦æˆ·ä¿¡æ¯ï¼ˆç®¡ç†ä¿¡æ¯ï¼‰- æ··åˆæ¶æ„ï¼šåŒæ­¥æŸ¥è¯¢ï¼ˆgRPCï¼‰"""
    try:
        from src.trading.services.grpc_trade_client import get_grpc_client, is_grpc_available
        
        if not is_grpc_available():
            return jsonify({
                'success': False,
                'error': 'gRPC åŠŸèƒ½ä¸å¯ç”¨'
            }), 503
        
        # ğŸš€ æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼ˆ2ç§’ï¼‰ï¼Œé¿å…é˜»å¡API
        import concurrent.futures
        try:
            client = get_grpc_client()
            
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œè®¾ç½®è¶…æ—¶ä¿æŠ¤ï¼ˆ2ç§’ï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                # æäº¤ä»»åŠ¡
                future = executor.submit(client.get_account_info)
            
                # ç­‰å¾…ç»“æœï¼Œè®¾ç½®è¶…æ—¶ï¼ˆ2ç§’ï¼‰
                try:
                    result = future.result(timeout=2.0)
                except concurrent.futures.TimeoutError:
                    logger.warning(f"è·å–è´¦æˆ·ä¿¡æ¯è¶…æ—¶ï¼ˆ2ç§’ï¼‰")
                    return jsonify({
                        'success': False,
                        'error': 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'
                    }), 504
            
            if result.get('success'):
                return jsonify({
                    'success': True,
                    'data': result.get('account_info')
                })
            else:
                return jsonify({
                    'success': False,
                    'error': result.get('message', 'è·å–è´¦æˆ·ä¿¡æ¯å¤±è´¥')
                }), 500
        except Exception as grpc_error:
            # æ•è·æ‰€æœ‰gRPCç›¸å…³é”™è¯¯
            error_msg = str(grpc_error)
            logger.warning(f"è·å–è´¦æˆ·ä¿¡æ¯å¤±è´¥: {error_msg}")
            return jsonify({
                'success': False,
                'error': 'è·å–è´¦æˆ·ä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•'
            }), 500
            
    except Exception as e:
        logger.error(f"è·å–è´¦æˆ·ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/symbols/<symbol>', methods=['GET'])
def get_symbol_info(symbol):
    """è·å–å“ç§ä¿¡æ¯ï¼ˆç®¡ç†ä¿¡æ¯ï¼‰- æ··åˆæ¶æ„ï¼šåŒæ­¥æŸ¥è¯¢ï¼ˆgRPCï¼‰"""
    try:
        from src.trading.services.grpc_trade_client import get_grpc_client, is_grpc_available
        import concurrent.futures
        
        if not is_grpc_available():
            return jsonify({
                'success': False,
                'error': 'gRPC åŠŸèƒ½ä¸å¯ç”¨'
            }), 503
        
        # ğŸ”´ ä¿®å¤ï¼šæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…é˜»å¡API
        try:
            client = get_grpc_client()
            
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œè®¾ç½®è¶…æ—¶ä¿æŠ¤ï¼ˆ2ç§’ï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(client.get_symbol_info, symbol)
                try:
                    result = future.result(timeout=2.0)
                except concurrent.futures.TimeoutError:
                    logger.warning(f"è·å–å“ç§ä¿¡æ¯è¶…æ—¶ï¼ˆ2ç§’ï¼‰")
                    return jsonify({
                        'success': False,
                        'error': 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'
                    }), 504
            
            if result.get('success'):
                return jsonify({
                    'success': True,
                    'data': result.get('symbol_info')
                })
            else:
                return jsonify({
                    'success': False,
                    'error': result.get('message', 'è·å–å“ç§ä¿¡æ¯å¤±è´¥')
                }), 500
        except Exception as grpc_error:
            logger.error(f"gRPCè°ƒç”¨å¤±è´¥: {grpc_error}")
            return jsonify({
                'success': False,
                'error': f'gRPCè°ƒç”¨å¤±è´¥: {str(grpc_error)[:100]}'
            }), 500
            
    except Exception as e:
        logger.error(f"è·å–å“ç§ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/terminal', methods=['GET'])
def get_terminal_info():
    """è·å–ç»ˆç«¯ä¿¡æ¯ï¼ˆç®¡ç†ä¿¡æ¯ï¼‰- æ··åˆæ¶æ„ï¼šåŒæ­¥æŸ¥è¯¢ï¼ˆgRPCï¼‰"""
    try:
        from src.trading.services.grpc_trade_client import get_grpc_client, is_grpc_available
        import concurrent.futures
        
        if not is_grpc_available():
            return jsonify({
                'success': False,
                'error': 'gRPC åŠŸèƒ½ä¸å¯ç”¨'
            }), 503
        
        # ğŸ”´ ä¿®å¤ï¼šæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…é˜»å¡API
        try:
            client = get_grpc_client()
            
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œè®¾ç½®è¶…æ—¶ä¿æŠ¤ï¼ˆ2ç§’ï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(client.get_terminal_info)
                try:
                    result = future.result(timeout=2.0)
                except concurrent.futures.TimeoutError:
                    logger.warning(f"è·å–ç»ˆç«¯ä¿¡æ¯è¶…æ—¶ï¼ˆ2ç§’ï¼‰")
                    return jsonify({
                        'success': False,
                        'error': 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'
                    }), 504
            
            if result.get('success'):
                return jsonify({
                    'success': True,
                    'data': result.get('terminal_info')
                })
            else:
                return jsonify({
                    'success': False,
                    'error': result.get('message', 'è·å–ç»ˆç«¯ä¿¡æ¯å¤±è´¥')
                }), 500
        except Exception as grpc_error:
            logger.error(f"gRPCè°ƒç”¨å¤±è´¥: {grpc_error}")
            return jsonify({
                'success': False,
                'error': f'gRPCè°ƒç”¨å¤±è´¥: {str(grpc_error)[:100]}'
            }), 500
            
    except Exception as e:
        logger.error(f"è·å–ç»ˆç«¯ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/server/time', methods=['GET'])
def get_server_time():
    """è·å–æœåŠ¡å™¨æ—¶é—´ï¼ˆç®¡ç†ä¿¡æ¯ï¼‰- æ··åˆæ¶æ„ï¼šåŒæ­¥æŸ¥è¯¢ï¼ˆgRPCï¼‰"""
    try:
        from src.trading.services.grpc_trade_client import get_grpc_client, is_grpc_available
        import concurrent.futures
        
        if not is_grpc_available():
            return jsonify({
                'success': False,
                'error': 'gRPC åŠŸèƒ½ä¸å¯ç”¨'
            }), 503
        
        # ğŸ”´ ä¿®å¤ï¼šæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…é˜»å¡API
        try:
            client = get_grpc_client()
            
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œè®¾ç½®è¶…æ—¶ä¿æŠ¤ï¼ˆ2ç§’ï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(client.get_server_time)
                try:
                    result = future.result(timeout=2.0)
                except concurrent.futures.TimeoutError:
                    logger.warning(f"è·å–æœåŠ¡å™¨æ—¶é—´è¶…æ—¶ï¼ˆ2ç§’ï¼‰")
                    return jsonify({
                        'success': False,
                        'error': 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'
                    }), 504
            
            if result.get('success'):
                return jsonify({
                    'success': True,
                    'data': {
                        'time': result.get('time'),
                        'time_msc': result.get('time_msc')
                    }
                })
            else:
                return jsonify({
                    'success': False,
                    'error': result.get('message', 'è·å–æœåŠ¡å™¨æ—¶é—´å¤±è´¥')
                }), 500
        except Exception as grpc_error:
            logger.error(f"gRPCè°ƒç”¨å¤±è´¥: {grpc_error}")
            return jsonify({
                'success': False,
                'error': f'gRPCè°ƒç”¨å¤±è´¥: {str(grpc_error)[:100]}'
            }), 500
            
    except Exception as e:
        logger.error(f"è·å–æœåŠ¡å™¨æ—¶é—´å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/symbols', methods=['GET'])
def get_symbol_list():
    """è·å–å“ç§åˆ—è¡¨ï¼ˆç®¡ç†ä¿¡æ¯ï¼‰- æ··åˆæ¶æ„ï¼šåŒæ­¥æŸ¥è¯¢ï¼ˆgRPCï¼‰"""
    try:
        from src.trading.services.grpc_trade_client import get_grpc_client, is_grpc_available
        import concurrent.futures
        
        group = request.args.get('group', '')
        
        if not is_grpc_available():
            return jsonify({
                'success': False,
                'error': 'gRPC åŠŸèƒ½ä¸å¯ç”¨'
            }), 503
        
        # ğŸ”´ ä¿®å¤ï¼šæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…é˜»å¡API
        try:
            client = get_grpc_client()
            
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œè®¾ç½®è¶…æ—¶ä¿æŠ¤ï¼ˆ2ç§’ï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(client.get_symbol_list, group=group)
                try:
                    result = future.result(timeout=2.0)
                except concurrent.futures.TimeoutError:
                    logger.warning(f"è·å–å“ç§åˆ—è¡¨è¶…æ—¶ï¼ˆ2ç§’ï¼‰")
                    return jsonify({
                        'success': False,
                        'error': 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'
                    }), 504
            
            if result.get('success'):
                return jsonify({
                    'success': True,
                    'data': {
                        'symbols': result.get('symbols', []),
                        'count': result.get('count', 0)
                    }
                })
            else:
                return jsonify({
                    'success': False,
                    'error': result.get('message', 'è·å–å“ç§åˆ—è¡¨å¤±è´¥')
                }), 500
        except Exception as grpc_error:
            logger.error(f"gRPCè°ƒç”¨å¤±è´¥: {grpc_error}")
            return jsonify({
                'success': False,
                'error': f'gRPCè°ƒç”¨å¤±è´¥: {str(grpc_error)[:100]}'
            }), 500
            
    except Exception as e:
        logger.error(f"è·å–å“ç§åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/redis/symbols', methods=['GET'])
def get_available_symbols():
    """è·å–Redisä¸­å¯ç”¨çš„äº¤æ˜“å“ç§"""
    try:
        # ä»Redisè·å–æ‰€æœ‰TICKæ•°æ®çš„key
        tick_keys = redis_client.keys("tick:*")
        symbols = []
        
        for key in tick_keys:
            symbol = key.decode('utf-8').replace('tick:', '')
            symbols.append(symbol)
        
        return jsonify({
            'success': True,
            'symbols': symbols
        })
        
    except Exception as e:
        logger.error(f"è·å–å¯ç”¨å“ç§å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/redis/latest-tick', methods=['GET'])
def get_latest_tick_from_collector():
    """ä»æ•°æ®é‡‡é›†å™¨è·å–æœ€æ–°TICKæ•°æ®"""
    try:
        # ä»æ•°æ®é‡‡é›†å™¨çš„Redisé”®è·å–æœ€æ–°TICKæ•°æ®
        latest_tick_key = 'tick:BTCUSDm:latest'
        tick_data = redis_client.get(latest_tick_key)
        
        if not tick_data:
            return jsonify({
                'success': False,
                'error': 'æœªæ‰¾åˆ°æœ€æ–°TICKæ•°æ®ï¼Œè¯·ç¡®ä¿æ•°æ®é‡‡é›†å™¨æ­£åœ¨è¿è¡Œ'
            }), 404
        
        # è§£æJSONæ•°æ®
        tick = json.loads(tick_data)
        
        return jsonify({
            'success': True,
            'tick': tick
        })
        
    except Exception as e:
        logger.error(f"è·å–æœ€æ–°TICKæ•°æ®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/redis/tick-history', methods=['GET'])
def get_tick_history_from_collector():
    """ä»æ•°æ®é‡‡é›†å™¨è·å–TICKå†å²æ•°æ®"""
    try:
        # è·å–å‚æ•°
        limit = request.args.get('limit', 100, type=int)
        
        # ä»æ•°æ®é‡‡é›†å™¨çš„Redisé”®è·å–TICKå†å²æ•°æ®
        tick_history_key = 'tick:BTCUSDm:realtime'
        tick_list = redis_client.zrange(tick_history_key, -limit, -1)  # è·å–æœ€æ–°çš„Næ¡
        
        ticks = []
        for tick_json in tick_list:
            try:
                tick_data = json.loads(tick_json)
                ticks.append(tick_data)
            except json.JSONDecodeError:
                continue
        
        return jsonify({
            'success': True,
            'ticks': ticks,
            'count': len(ticks)
        })
        
    except Exception as e:
        logger.error(f"è·å–TICKå†å²æ•°æ®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/strategy/logs', methods=['GET'])
def get_strategy_logs():
    """è·å–ç­–ç•¥æ—¥å¿—/ä¿¡å·ï¼ˆä»Redis Streamè¯»å–ï¼‰
    
    å‚æ•°:
        symbol: äº¤æ˜“å¯¹ (é»˜è®¤: BTCUSDm)
        count: è·å–æ•°é‡ (é»˜è®¤: 100)
    """
    try:
        symbol = request.args.get('symbol', 'BTCUSDm')
        count = request.args.get('count', 100, type=int)
        
        # ä»Redis Streamè¯»å–ç­–ç•¥ä¿¡å·
        signal_stream_key = f"signal:{symbol}:stream"
        signal_history_key = f"signal:{symbol}:history"
        
        logs = []
        
        # ä¼˜å…ˆä»Sorted Setè¯»å–ï¼ˆå†å²ä¿¡å·ï¼ŒæŒ‰æ—¶é—´æ’åºï¼‰
        try:
            # è·å–æœ€è¿‘çš„countæ¡ä¿¡å·
            signal_jsons = redis_client.zrange(signal_history_key, -count, -1, withscores=False)
            
            for signal_json in signal_jsons:
                try:
                    signal = json.loads(signal_json)
                    # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
                    action = signal.get('action', 'UNKNOWN')
                    reason = signal.get('reason', '')
                    price = signal.get('price', 0)
                    
                    # ç¡®å®šæ—¥å¿—çº§åˆ«
                    if action in ['BUY', 'SELL']:
                        level = 'SIGNAL'
                        message = f"{symbol} {action} ä¿¡å· @ {price:.2f}"
                        details = reason
                    else:
                        level = 'INFO'
                        message = f"{symbol} çŠ¶æ€æ›´æ–°: {action}"
                        details = reason
                    
                    logs.append({
                        'timestamp': int(signal.get('timestamp', signal.get('tick_time_ms', 0) / 1000)),
                        'level': level,
                        'message': message,
                        'details': details,
                        'signal': signal  # ä¿ç•™å®Œæ•´ä¿¡å·æ•°æ®
                    })
                except Exception as e:
                    logger.warning(f"è§£æç­–ç•¥ä¿¡å·å¤±è´¥: {e}")
                    continue
        except Exception as e:
            logger.warning(f"ä»Redisè¯»å–ç­–ç•¥ä¿¡å·å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ä»Sorted Setè·å–åˆ°æ•°æ®ï¼Œå°è¯•ä»Streamè¯»å–
        if not logs:
            try:
                # ä»Streamè¯»å–æœ€æ–°æ¶ˆæ¯
                messages = redis_client.xrevrange(signal_stream_key, count=count)
                for msg_id, fields in messages:
                    signal_json = fields.get('signal_json')
                    if signal_json:
                        try:
                            signal = json.loads(signal_json)
                            action = signal.get('action', 'UNKNOWN')
                            reason = signal.get('reason', '')
                            price = signal.get('price', 0)
                            
                            if action in ['BUY', 'SELL']:
                                level = 'SIGNAL'
                                message = f"{symbol} {action} ä¿¡å· @ {price:.2f}"
                            else:
                                level = 'INFO'
                                message = f"{symbol} çŠ¶æ€æ›´æ–°: {action}"
                            
                            logs.append({
                                'timestamp': int(signal.get('timestamp', signal.get('tick_time_ms', 0) / 1000)),
                                'level': level,
                                'message': message,
                                'details': reason,
                                'signal': signal
                            })
                        except Exception as e:
                            logger.warning(f"è§£æStreamä¿¡å·å¤±è´¥: {e}")
                            continue
            except Exception as e:
                logger.warning(f"ä»Redis Streamè¯»å–ç­–ç•¥ä¿¡å·å¤±è´¥: {e}")
        
        # æŒ‰æ—¶é—´æˆ³é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        logs.sort(key=lambda x: x['timestamp'], reverse=True)
        
        return jsonify({
            'success': True,
            'data': logs,
            'count': len(logs)
        })
    except Exception as e:
        logger.error(f"è·å–ç­–ç•¥æ—¥å¿—å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'data': [],
            'error': str(e)
        }), 500


@app.route('/api/redis/collector-status', methods=['GET'])
def get_collector_status():
    """è·å–æ•°æ®é‡‡é›†å™¨çŠ¶æ€"""
    try:
        # ä»æ•°æ®é‡‡é›†å™¨çš„çŠ¶æ€é”®è·å–çŠ¶æ€ä¿¡æ¯
        status_key = 'status:BTCUSDm:collector'
        status_data = redis_client.get(status_key)
        
        if not status_data:
            return jsonify({
                'success': False,
                'error': 'æœªæ‰¾åˆ°æ•°æ®é‡‡é›†å™¨çŠ¶æ€ï¼Œè¯·ç¡®ä¿æ•°æ®é‡‡é›†å™¨æ­£åœ¨è¿è¡Œ'
            }), 404
        
        # è§£æJSONæ•°æ®
        status = json.loads(status_data)
        
        return jsonify({
            'success': True,
            'status': status
        })
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é‡‡é›†å™¨çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/system/services', methods=['GET'])
def get_system_services():
    """è·å–ç³»ç»ŸæœåŠ¡çŠ¶æ€ï¼ˆsystemdï¼‰"""
    import subprocess
    import json as json_lib
    
    services = {
        'mt5-collector': {
            'name': 'MT5æ•°æ®é‡‡é›†å™¨',
            'description': 'ä»MetaTrader 5é‡‡é›†å¸‚åœºæ•°æ®',
            'service': 'mt5-collector.service'
        },
        'hft-core': {
            'name': 'HFTæ ¸å¿ƒæœåŠ¡',
            'description': 'ç­–ç•¥çŠ¶æ€æœºã€äº¤æ˜“æ‰§è¡Œå™¨ã€ç›‘æ§æœåŠ¡',
            'service': 'hft-core.service'
        },
        'backend-api': {
            'name': 'åç«¯APIæœåŠ¡',
            'description': 'Flask APIå’ŒWebSocketæœåŠ¡',
            'service': 'backend-api.service'
        }
    }
    
    result = {}
    
    for key, info in services.items():
        service_name = info['service']
        try:
            # æ£€æŸ¥æœåŠ¡çŠ¶æ€
            status_cmd = ['systemctl', 'is-active', service_name]
            is_active = subprocess.run(status_cmd, capture_output=True, text=True, timeout=2)
            active = is_active.stdout.strip() == 'active'
            
            # è·å–æœåŠ¡è¯¦ç»†ä¿¡æ¯
            show_cmd = ['systemctl', 'show', service_name, '--property=ActiveState,SubState,MainPID,LoadState']
            show_result = subprocess.run(show_cmd, capture_output=True, text=True, timeout=2)
            
            # è§£æè¾“å‡º
            service_info = {}
            for line in show_result.stdout.strip().split('\n'):
                if '=' in line:
                    k, v = line.split('=', 1)
                    service_info[k] = v
            
            # è·å–æ—¥å¿—ï¼ˆæœ€è¿‘å‡ è¡Œï¼‰
            try:
                journal_cmd = ['journalctl', '-u', service_name, '-n', '5', '--no-pager', '-o', 'json']
                journal_result = subprocess.run(journal_cmd, capture_output=True, text=True, timeout=2)
                logs = []
                for line in journal_result.stdout.strip().split('\n'):
                    if line:
                        try:
                            log_entry = json_lib.loads(line)
                            logs.append({
                                'message': log_entry.get('MESSAGE', ''),
                                'timestamp': log_entry.get('__REALTIME_TIMESTAMP', ''),
                                'level': log_entry.get('PRIORITY', '')
                            })
                        except:
                            pass
            except:
                logs = []
            
            result[key] = {
                'name': info['name'],
                'description': info['description'],
                'service': service_name,
                'active': active,
                'state': service_info.get('ActiveState', 'unknown'),
                'substate': service_info.get('SubState', 'unknown'),
                'pid': service_info.get('MainPID', ''),
                'loaded': service_info.get('LoadState', 'unknown') == 'loaded',
                'recent_logs': logs[-3:] if logs else [],  # æœ€è¿‘3æ¡æ—¥å¿—
                'frontend_machine': {
                    'host': '192.168.10.131',
                    'description': 'å‰ç½®æœº - MT5å’Œè¿æ¥å™¨æ‰€åœ¨æœºå™¨'
                } if key == 'mt5-collector' else None
            }
            
        except subprocess.TimeoutExpired:
            result[key] = {
                'name': info['name'],
                'description': info['description'],
                'service': service_name,
                'active': False,
                'error': 'æŸ¥è¯¢è¶…æ—¶'
            }
        except Exception as e:
            logger.error(f"è·å–æœåŠ¡ {service_name} çŠ¶æ€å¤±è´¥: {e}")
            result[key] = {
                'name': info['name'],
                'description': info['description'],
                'service': service_name,
                'active': False,
                'error': str(e)
            }
    
    return jsonify({
        'success': True,
        'services': result
    })


@app.route('/api/system/service/<service_name>/control', methods=['POST'])
def control_service(service_name):
    """æ§åˆ¶æœåŠ¡ï¼ˆå¯åŠ¨/åœæ­¢/é‡å¯ï¼‰"""
    import subprocess
    
    action = request.json.get('action', 'status')  # start, stop, restart, status
    
    valid_services = ['mt5-collector', 'hft-core', 'backend-api']
    if service_name not in valid_services:
        return jsonify({
            'success': False,
            'error': f'æ— æ•ˆçš„æœåŠ¡åç§°: {service_name}'
        }), 400
    
    service_file = f'{service_name}.service'
    
    try:
        if action == 'start':
            cmd = ['systemctl', 'start', service_file]
            action_name = 'å¯åŠ¨'
        elif action == 'stop':
            cmd = ['systemctl', 'stop', service_file]
            action_name = 'åœæ­¢'
        elif action == 'restart':
            cmd = ['systemctl', 'restart', service_file]
            action_name = 'é‡å¯'
        else:
            return jsonify({
                'success': False,
                'error': f'æ— æ•ˆçš„æ“ä½œ: {action}'
            }), 400
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            return jsonify({
                'success': True,
                'message': f'{action_name}æœåŠ¡æˆåŠŸ'
            })
        else:
            return jsonify({
                'success': False,
                'error': result.stderr or f'{action_name}æœåŠ¡å¤±è´¥'
            }), 500
            
    except subprocess.TimeoutExpired:
        return jsonify({
            'success': False,
            'error': 'æ“ä½œè¶…æ—¶'
        }), 500
    except Exception as e:
        logger.error(f"æ§åˆ¶æœåŠ¡ {service_name} å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/redis/status', methods=['GET'])
def get_redis_status():
    """è·å–Redisè¿æ¥çŠ¶æ€å’Œæ•°æ®ç»Ÿè®¡"""
    try:
        # æ£€æŸ¥Redisè¿æ¥
        redis_client.ping()
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        info = redis_client.info()
        
        # ç»Ÿè®¡TICKæ•°æ®
        tick_keys = redis_client.keys("tick:*")
        history_keys = redis_client.keys("tick_history:*")
        
        status = {
            'connected': True,
            'redis_version': info.get('redis_version', 'unknown'),
            'used_memory': info.get('used_memory_human', 'unknown'),
            'connected_clients': info.get('connected_clients', 0),
            'tick_symbols': len(tick_keys),
            'history_symbols': len(history_keys),
            'uptime': info.get('uptime_in_seconds', 0)
        }
        
        return jsonify({
            'success': True,
            'status': status
        })
        
    except Exception as e:
        logger.error(f"è·å–RedisçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'status': {'connected': False}
        }), 500


@app.route('/api/system/connections', methods=['GET'])
def get_system_connections():
    """è·å–æ‰€æœ‰ç³»ç»Ÿè¿æ¥çŠ¶æ€ï¼ˆMT5ä¸­ç»§å™¨ã€gRPCã€Redisã€åç«¯ï¼‰"""
    import requests
    
    connections = {
        'backend': {'connected': True, 'status': 'ok'},  # åç«¯æœ¬èº«æ€»æ˜¯è¿æ¥çš„
        'redis': {'connected': False, 'status': 'unknown'},
        'mt5_relay': {'connected': False, 'status': 'unknown'},
        'grpc': {'connected': False, 'status': 'unknown'},
    }
    
    # 1. æ£€æŸ¥ Redis è¿æ¥ï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼Œé¿å…é˜»å¡ï¼‰
    try:
        redis_client.ping()
        # ğŸ”´ ä¿®å¤ï¼šç§»é™¤info()è°ƒç”¨ï¼Œé¿å…é˜»å¡ï¼ˆinfo()å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹æ…¢ï¼‰
        connections['redis'] = {
            'connected': True,
            'status': 'ok',
            'version': '7.0.15'  # ç›´æ¥è¿”å›å·²çŸ¥ç‰ˆæœ¬ï¼Œé¿å…info()é˜»å¡
        }
    except Exception as e:
        connections['redis'] = {
            'connected': False,
            'status': 'error',
            'error': str(e)[:50]  # æˆªæ–­é”™è¯¯ä¿¡æ¯ï¼Œé¿å…è¿‡é•¿
        }
    
    # 2. æ£€æŸ¥ gRPC æœåŠ¡è¿æ¥ï¼ˆWindows ä¸»æœºä¸Šçš„ä¸­ç»§æœåŠ¡ï¼‰
    # ğŸ”´ æ¶æ„è¯´æ˜ï¼ˆæ ¹æ®docs/ç³»ç»Ÿæ¶æ„/åç«¯æ¶æ„.mdï¼‰ï¼š
    # - Windowsä¸­ç»§æœåŠ¡ = gRPCæœåŠ¡ï¼ˆ50051ï¼‰+ ZeroMQæœåŠ¡ï¼ˆ5555ï¼‰
    # - gRPCæœåŠ¡ï¼šå¤„ç†æŸ¥è¯¢å’ŒæŒ‡ä»¤ï¼ˆåŒæ­¥ï¼‰ï¼ŒåŒ…æ‹¬è®¢å•æ‰§è¡Œ
    # - ZeroMQæœåŠ¡ï¼šWindowsç«¯å†…éƒ¨é€šä¿¡ï¼ˆMQL EA â†’ Pythonï¼‰ï¼Œä¸ç”¨äºè®¢å•æ‰§è¡Œ
    # - è®¢å•æ‰§è¡Œèµ°gRPCï¼ŒZeroMQåªç”¨äºWindowsç«¯å†…éƒ¨äº‹ä»¶æ¨é€
    # ğŸ”´ ä¿®å¤ï¼šä½¿ç”¨è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…é˜»å¡APIå“åº”
    from src.trading.services.grpc_trade_client import get_grpc_client, is_grpc_available
    grpc_host = '192.168.10.131'
    grpc_port = 50051
    grpc_address = f"{grpc_host}:{grpc_port}"
    
    # åˆå§‹åŒ–gRPCå’ŒMT5ä¸­ç»§å™¨çŠ¶æ€
    grpc_connected = False
    mt5_relay_connected = False
    grpc_error = None
    mt5_relay_error = None
    
    # ğŸ”´ ä¿®å¤ï¼šä½¿ç”¨ThreadPoolExecutoråŒ…è£…gRPCæ£€æŸ¥ï¼Œè®¾ç½®ä¸¥æ ¼è¶…æ—¶ï¼Œé¿å…é˜»å¡
    # å¦‚æœgRPCæ£€æŸ¥è¶…æ—¶ï¼Œç›´æ¥æ ‡è®°ä¸ºæœªè¿æ¥
    try:
        import concurrent.futures
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œè®¾ç½®0.5ç§’è¶…æ—¶ï¼ˆå¥åº·æ£€æŸ¥å¿…é¡»å¿«é€Ÿï¼‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(is_grpc_available)
            try:
                grpc_available = future.result(timeout=0.5)  # 0.5ç§’è¶…æ—¶
                if grpc_available:
                    # gRPCåŠŸèƒ½å¯ç”¨ï¼Œä½†ä¸è¿›è¡Œå®é™…è¿æ¥æ£€æŸ¥ï¼ˆé¿å…ç½‘ç»œå»¶è¿Ÿï¼‰
                    grpc_connected = False
                    mt5_relay_connected = False
                    grpc_error = "gRPCåŠŸèƒ½å¯ç”¨ä½†æœªè¿æ¥ï¼ˆè·³è¿‡è¿æ¥æ£€æŸ¥ä»¥é¿å…é˜»å¡ï¼‰"
                else:
                    grpc_connected = False
                    mt5_relay_connected = False
                    grpc_error = 'gRPCåŠŸèƒ½ä¸å¯ç”¨'
            except concurrent.futures.TimeoutError:
                # è¶…æ—¶ï¼šç›´æ¥æ ‡è®°ä¸ºæœªè¿æ¥
                grpc_connected = False
                mt5_relay_connected = False
                grpc_error = 'gRPCæ£€æŸ¥è¶…æ—¶ï¼ˆ0.5ç§’ï¼‰'
    except Exception as e:
        grpc_connected = False
        mt5_relay_connected = False
        grpc_error = f'gRPCæ£€æŸ¥å¼‚å¸¸: {str(e)[:50]}'
    
    # è®¾ç½®gRPCè¿æ¥çŠ¶æ€
    connections['grpc'] = {
        'connected': grpc_connected,
        'status': 'ok' if grpc_connected else 'error',
        'address': grpc_address,
        'error': grpc_error if not grpc_connected else None
    }
    
    # è®¾ç½®MT5ä¸­ç»§å™¨è¿æ¥çŠ¶æ€ï¼ˆä¸gRPCçŠ¶æ€ä¸€è‡´ï¼‰
    connections['mt5_relay'] = {
        'connected': mt5_relay_connected,
        'status': 'ok' if mt5_relay_connected else 'error',
        'mt5_connected': mt5_relay_connected,
        'service_status': 'gRPCæœåŠ¡è¿è¡Œä¸­' if mt5_relay_connected else 'gRPCæœåŠ¡æœªè¿æ¥',
        'address': grpc_address,
        'protocol': 'gRPC',
        'error': mt5_relay_error if not mt5_relay_connected else None,
        'hint': 'è¯·æ£€æŸ¥ Windows ä¸»æœºä¸Šçš„ gRPC æœåŠ¡æ˜¯å¦è¿è¡Œ (mt5_relay_service.py)' if not mt5_relay_connected else None
    }
    
    return jsonify({
        'success': True,
        'connections': connections,
        'timestamp': time.time()
    })


@app.route('/api/system/clients', methods=['GET'])
def get_connected_clients():
    """è·å–å½“å‰è¿æ¥çš„ WebSocket å®¢æˆ·ç«¯ä¿¡æ¯"""
    try:
        client_list = []
        for sid, client_info in clients.items():
            client_list.append({
                'sid': sid,
                'last_tick_time': client_info.get('last_time', 0),
                'connected_at': client_info.get('connected_at', None)
            })
        
        return jsonify({
            'success': True,
            'clients': client_list,
            'total_count': len(clients),
            'timestamp': time.time()
        })
    except Exception as e:
        logger.error(f"è·å–å®¢æˆ·ç«¯ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'clients': [],
            'total_count': 0
        }), 500


# ==================== æ ‡æ³¨API ====================

@app.route('/api/annotations/save', methods=['POST'])
def save_annotations():
    """ä¿å­˜æ ‡æ³¨æ•°æ®åˆ°Redis"""
    try:
        data = request.json
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ä¿å­˜æœ€æ–°ç‰ˆæœ¬
        redis_client.set('annotations:hourly:latest', json.dumps(data, ensure_ascii=False))
        
        # 2. ä¿å­˜å†å²ç‰ˆæœ¬ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        history_key = f'annotations:hourly:history:{timestamp}'
        redis_client.set(history_key, json.dumps(data, ensure_ascii=False))
        
        # 3. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_annotations': len(data.get('annotations', [])),
            'last_updated': timestamp,
            'data_quality': data.get('stats', {}).get('dataQuality', {})
        }
        redis_client.set('annotations:hourly:stats', json.dumps(stats, ensure_ascii=False))
        
        # 4. å°†å†å²ç‰ˆæœ¬é”®åŠ å…¥åˆ—è¡¨ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰
        redis_client.zadd('annotations:hourly:versions', {history_key: int(time.time())})
        
        logger.info(f"âœ… ä¿å­˜æ ‡æ³¨æ•°æ®æˆåŠŸ: {len(data.get('annotations', []))} ä¸ªæ ‡æ³¨")
        
        return jsonify({
            'success': True,
            'message': f'âœ… ä¿å­˜æˆåŠŸï¼š{len(data.get("annotations", []))} ä¸ªæ ‡æ³¨',
            'timestamp': timestamp
        })
    
    except Exception as e:
        logger.error(f'ä¿å­˜æ ‡æ³¨å¤±è´¥: {str(e)}')
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/annotations/load', methods=['GET'])
def load_annotations():
    """ä»RedisåŠ è½½æœ€æ–°æ ‡æ³¨æ•°æ®"""
    try:
        # è·å–æœ€æ–°æ•°æ®
        data_str = redis_client.get('annotations:hourly:latest')
        
        if data_str:
            data = json.loads(data_str)
            return jsonify({
                'success': True,
                'data': data
            })
        else:
            return jsonify({
                'success': True,
                'data': None,
                'message': 'æš‚æ— æ ‡æ³¨æ•°æ®'
            })
    
    except Exception as e:
        logger.error(f'åŠ è½½æ ‡æ³¨å¤±è´¥: {str(e)}')
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ==================== å¯åŠ¨ ====================

if __name__ == '__main__':
    # é…ç½®æ—¥å¿—
    logger.add(
        LOG_DIR / "gui_{time}.log",
        rotation="1 day",
        retention="30 days",
        level="INFO"
    )
    
    # ğŸ”´ ä¿®å¤ï¼šç§»é™¤å¯åŠ¨æ—¶æ•°æ®åˆå§‹åŒ–ï¼Œé¿å…é˜»å¡API Serverå¯åŠ¨
    # æ•°æ®åˆå§‹åŒ–åº”è¯¥ç”±Data Integrity Serviceåœ¨åå°å®Œæˆï¼Œè€Œä¸æ˜¯åœ¨API Serverå¯åŠ¨æ—¶é˜»å¡
    # try:
    #     from src.trading.services.data_integrity_checker import initialize_data_on_startup
    #     logger.info("ğŸ”§ API Serverå¯åŠ¨ï¼šåˆå§‹åŒ–å†å²æ•°æ®...")
    #     initialize_data_on_startup(symbol="BTCUSDm", count=2880)
    # except Exception as e:
    #     logger.warning(f"å¯åŠ¨æ—¶æ•°æ®åˆå§‹åŒ–å¤±è´¥ï¼ˆéå…³é”®ï¼‰: {e}")
    
    logger.info("\n" + "="*60)
    logger.info("AIäº¤æ˜“ç³»ç»Ÿ - GUIæœåŠ¡å¯åŠ¨")
    logger.info("="*60 + "\n")
    
    # æš‚æ—¶ä¸åŠ è½½æ¨¡å‹
    # logger.info("æ­£åœ¨åŠ è½½AIæ¨¡å‹...")
    # best_model_path = CHECKPOINT_DIR / "best_model.pth"
    # load_model(str(best_model_path) if best_model_path.exists() else None)
    
    # ã€Flask-SocketIOæœ€ä½³å®è·µã€‘ä½¿ç”¨ socketio.start_background_task è€Œä¸æ˜¯ threading.Thread
    # å‚è€ƒï¼šhttps://flask-socketio.readthedocs.io/en/latest/getting_started.html#background-tasks
    logger.info("å¯åŠ¨å®æ—¶æ•°æ®å¹¿æ’­åå°ä»»åŠ¡...")
    socketio.start_background_task(broadcast_realtime_data)
    
    logger.info("å¯åŠ¨æŒä»“å®æ—¶æ›´æ–°åå°ä»»åŠ¡ï¼ˆgRPCï¼‰...")
    socketio.start_background_task(broadcast_positions_updates)
    
    logger.info("å¯åŠ¨è®¢å•åé¦ˆç›‘å¬åå°ä»»åŠ¡...")
    socketio.start_background_task(listen_order_feedback)
    
    logger.info("å¯åŠ¨ Redis Pub/Sub è®¢é˜…ï¼ˆäº‹ä»¶é©±åŠ¨æ¨¡å¼ï¼Œæ¥æ”¶ Windows gRPC æœåŠ¡æ¨é€ï¼‰...")
    socketio.start_background_task(listen_redis_pubsub)
    logger.info("  - è®¢é˜…é¢‘é“: tick:*, kline:*, mt5:position_update, mt5:deal, mt5:trade_events ç­‰")
    
    # å¯åŠ¨FlaskæœåŠ¡
    logger.info("\nğŸš€ æœåŠ¡å¯åŠ¨:")
    logger.info("  - APIåœ°å€: http://localhost:5000")
    logger.info("  - WebSocket: ws://localhost:5000")
    logger.info("\n")
    
    # ä½¿ç”¨ socketio.run() è€Œä¸æ˜¯ app.run()
    # æ³¨æ„ï¼šthreaded å‚æ•°åªåœ¨ threading æ¨¡å¼ä¸‹æœ‰æ•ˆï¼Œeventlet æ¨¡å¼ä¸æ”¯æŒ
    try:
        run_kwargs = {
            'app': app,
            'host': '0.0.0.0',
            'port': 5000,
            'debug': False,
            'use_reloader': False,
            'log_output': True,
            'allow_unsafe_werkzeug': True,
            # ğŸ”´ æ³¨æ„ï¼šsocketio.run() ä¸æ”¯æŒ backlog å‚æ•°ï¼Œè¿æ¥é˜Ÿåˆ—ç”±åº•å±‚socketæ§åˆ¶
        }
        # åªåœ¨ threading æ¨¡å¼ä¸‹æ·»åŠ  threaded å‚æ•°
        if async_mode == 'threading':
            run_kwargs['threaded'] = True
        
        socketio.run(**run_kwargs)
    except Exception as e:
        logger.error(f"Flask-SocketIO å¯åŠ¨å¤±è´¥: {e}", exc_info=True)
        raise
fo=True)
        raise
