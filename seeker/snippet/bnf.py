#date: 2023-06-09T16:50:29Z
#url: https://api.github.com/gists/6e1f8a08a744a9742f29999a518c704c
#owner: https://api.github.com/users/neorobin

#!/usr/bin/env python

import re
import argparse
from collections import namedtuple
from graphviz import Digraph

# Utility types to keep things a little bit neater
Token = "**********"
Rule = namedtuple('Rule', ['name', 'expression'])
Grammar = namedtuple('Grammar', ['root', 'rules'])
Tree = namedtuple('Tree', ['root', 'children', 'length'])

terminals = ['identifier', 'integer', 'symbol', 'literal']


 "**********"d "**********"e "**********"f "**********"  "**********"t "**********"o "**********"k "**********"e "**********"n "**********"i "**********"z "**********"e "**********"( "**********"s "**********") "**********": "**********"
    ''' Return a list of tokens from a string '''
    tokens = "**********"
    offset = 0  # Offset into string
    s = re.sub(r'%[^\n]*\n', '', s)  # Remove comments
    while offset < len(s):
        # Newlines and spaces (ignored)
        if s[offset] == '\n' or s[offset] == ' ':
            offset = offset + 1
            continue
        # Identifiers
        match = re.match(r'^[a-z_A-Z][a-zA-Z0-9]*', s[offset:])
        if match is not None:
            tokens.append(Token('identifier', match.group()))
            offset = offset + len(match.group())
            continue
        # Integers
        match = re.match(r'^[1-9][0-9]*|0', s[offset:])
        if match is not None:
            tokens.append(Token('integer', match.group()))
            offset = offset + len(match.group())
            continue
        # Symbols
        match = re.match(r'^[=|;:!<>\+\-\*\/\(\)]+', s[offset:])
        if match is not None:
            tokens.append(Token('symbol', match.group()))
            offset = offset + len(match.group())
            continue
        # Literals
        match = re.match(r'^"([^"\n]*)"', s[offset:])
        if match is not None:
            tokens.append(Token('literal', match.groups()[0]))
            offset = offset + len(match.group())
            continue
        # Error case
        raise ValueError('Unable to tokenize: "**********":]))
    return tokens


 "**********"d "**********"e "**********"f "**********"  "**********"i "**********"s "**********"_ "**********"t "**********"e "**********"r "**********"m "**********"( "**********"t "**********"o "**********"k "**********"e "**********"n "**********", "**********"  "**********"s "**********"y "**********"m "**********"b "**********"o "**********"l "**********") "**********": "**********"
    ''' Return true if token is the given terminal symbol '''
 "**********"  "**********"  "**********"  "**********"  "**********"r "**********"e "**********"t "**********"u "**********"r "**********"n "**********"  "**********"t "**********"o "**********"k "**********"e "**********"n "**********". "**********"t "**********"o "**********"k "**********"e "**********"n "**********"  "**********"= "**********"= "**********"  "**********"' "**********"s "**********"y "**********"m "**********"b "**********"o "**********"l "**********"' "**********"  "**********"a "**********"n "**********"d "**********"  "**********"t "**********"o "**********"k "**********"e "**********"n "**********". "**********"v "**********"a "**********"l "**********"u "**********"e "**********"  "**********"= "**********"= "**********"  "**********"s "**********"y "**********"m "**********"b "**********"o "**********"l "**********"


 "**********"d "**********"e "**********"f "**********"  "**********"p "**********"a "**********"r "**********"s "**********"e "**********"_ "**********"g "**********"r "**********"a "**********"m "**********"m "**********"a "**********"r "**********"( "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********") "**********": "**********"
    ''' Parse a list of tokens into a BNF grammar '''
    rules = {}
    total = 0
    first_rule = None
 "**********"  "**********"  "**********"  "**********"  "**********"w "**********"h "**********"i "**********"l "**********"e "**********"  "**********"t "**********"o "**********"t "**********"a "**********"l "**********"  "**********"< "**********"  "**********"l "**********"e "**********"n "**********"( "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********") "**********": "**********"
        # Parse rule name
 "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"a "**********"s "**********"s "**********"e "**********"r "**********"t "**********"  "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********"[ "**********"t "**********"o "**********"t "**********"a "**********"l "**********"] "**********". "**********"t "**********"o "**********"k "**********"e "**********"n "**********"  "**********"= "**********"= "**********"  "**********"' "**********"i "**********"d "**********"e "**********"n "**********"t "**********"i "**********"f "**********"i "**********"e "**********"r "**********"' "**********", "**********"  "**********"' "**********"{ "**********"} "**********"  "**********"{ "**********"} "**********"' "**********". "**********"f "**********"o "**********"r "**********"m "**********"a "**********"t "**********"( "**********"
            tokens[total], total)
        name = "**********"
        if first_rule is None:
            first_rule = name
        total += 1
        # Parse assignment symbol
        assert is_term(tokens[total], '= "**********"
        total += 1
        # Parse expression
        expression = []
        while True:
            # Parse list
            list_ = []
 "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"w "**********"h "**********"i "**********"l "**********"e "**********"  "**********"n "**********"o "**********"t "**********"  "**********"( "**********"i "**********"s "**********"_ "**********"t "**********"e "**********"r "**********"m "**********"( "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********"[ "**********"t "**********"o "**********"t "**********"a "**********"l "**********"] "**********", "**********"  "**********"' "**********"| "**********"' "**********") "**********"  "**********"o "**********"r "**********"  "**********"i "**********"s "**********"_ "**********"t "**********"e "**********"r "**********"m "**********"( "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********"[ "**********"t "**********"o "**********"t "**********"a "**********"l "**********"] "**********", "**********"  "**********"' "**********"; "**********"' "**********") "**********") "**********": "**********"
                list_.append(tokens[total].value)
                total += 1
            expression.append(list_)
 "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"i "**********"f "**********"  "**********"i "**********"s "**********"_ "**********"t "**********"e "**********"r "**********"m "**********"( "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********"[ "**********"t "**********"o "**********"t "**********"a "**********"l "**********"] "**********", "**********"  "**********"' "**********"; "**********"' "**********") "**********": "**********"
                break
 "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"i "**********"f "**********"  "**********"i "**********"s "**********"_ "**********"t "**********"e "**********"r "**********"m "**********"( "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********"[ "**********"t "**********"o "**********"t "**********"a "**********"l "**********"] "**********", "**********"  "**********"' "**********"| "**********"' "**********") "**********": "**********"
                total += 1
        # Parse terminating semicolon
        assert is_term(tokens[total], ';')
        total += 1
        # Add rule to list
        rules[name] = expression
    return Grammar(first_rule, rules)


 "**********"d "**********"e "**********"f "**********"  "**********"p "**********"a "**********"r "**********"s "**********"e "**********"( "**********"g "**********"r "**********"a "**********"m "**********"m "**********"a "**********"r "**********", "**********"  "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********", "**********"  "**********"i "**********"t "**********"e "**********"m "**********"s "**********"= "**********"N "**********"o "**********"n "**********"e "**********") "**********": "**********"
    ''' Given a grammar, parse a list of tokens into syntax trees given nonterminals '''
    if items is None:
        items = [grammar.root]
    if len(items) == 0:
        return []
 "**********"  "**********"  "**********"  "**********"  "**********"i "**********"f "**********"  "**********"l "**********"e "**********"n "**********"( "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********") "**********"  "**********"= "**********"= "**********"  "**********"0 "**********": "**********"
        return None

    item, *tail = items

    if item not in grammar.rules:
 "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"i "**********"f "**********"  "**********"( "**********"( "**********"i "**********"t "**********"e "**********"m "**********"  "**********"i "**********"n "**********"  "**********"t "**********"e "**********"r "**********"m "**********"i "**********"n "**********"a "**********"l "**********"s "**********"  "**********"a "**********"n "**********"d "**********"  "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********"[ "**********"0 "**********"] "**********". "**********"t "**********"o "**********"k "**********"e "**********"n "**********"  "**********"= "**********"= "**********"  "**********"i "**********"t "**********"e "**********"m "**********") "**********"  "**********"o "**********"r "**********"
 "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"( "**********"i "**********"t "**********"e "**********"m "**********"  "**********"= "**********"= "**********"  "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********"[ "**********"0 "**********"] "**********". "**********"v "**********"a "**********"l "**********"u "**********"e "**********"  "**********"a "**********"n "**********"d "**********"  "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********"[ "**********"0 "**********"] "**********". "**********"t "**********"o "**********"k "**********"e "**********"n "**********"  "**********"i "**********"n "**********"  "**********"t "**********"e "**********"r "**********"m "**********"i "**********"n "**********"a "**********"l "**********"s "**********") "**********") "**********": "**********"
            tail_trees = parse(grammar, tokens[1: "**********"
            if tail_trees is not None:
                root = item
 "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"  "**********"i "**********"f "**********"  "**********"i "**********"t "**********"e "**********"m "**********"  "**********"= "**********"= "**********"  "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********"[ "**********"0 "**********"] "**********". "**********"t "**********"o "**********"k "**********"e "**********"n "**********"  "**********"a "**********"n "**********"d "**********"  "**********"t "**********"o "**********"k "**********"e "**********"n "**********"s "**********"[ "**********"0 "**********"] "**********". "**********"t "**********"o "**********"k "**********"e "**********"n "**********"  "**********"! "**********"= "**********"  "**********"' "**********"l "**********"i "**********"t "**********"e "**********"r "**********"a "**********"l "**********"' "**********": "**********"
                    root = "**********"
                return [Tree(root, [], 1)] + tail_trees
        return None

    for exp in grammar.rules[item]:
        exp_trees = "**********"
        if exp_trees is None:
            continue
        offset = sum(tree.length for tree in exp_trees)
        tail_trees = parse(grammar, tokens[offset: "**********"
        if tail_trees is None:
            continue
        return [Tree(item, exp_trees, offset)] + tail_trees
    return None


def render(tree, filepath):
    ''' Draw the syntax tree as a graph to an output image file '''
    dot = Digraph(format='png')
    to_visit = [tree]
    while to_visit:
        node = to_visit.pop(0)
        dot.node(str(id(node)), label=str(node.root))
        for child in node.children:
            if isinstance(child, Tree):
                dot.edge(str(id(node)), str(id(child)))
                to_visit.append(child)
    dot.render(filepath, cleanup=True)


def main():
    parser = argparse.ArgumentParser(description='Make syntax tree graphs')
    parser.add_argument('grammar_file', help='BNF-encoded grammar to parse')
    parser.add_argument('program_file', help='program to parse with grammar')
    parser.add_argument('output_image', help='image file to save syntax tree')
    args = parser.parse_args()

    grammar = "**********"
    program = "**********"
    syntax, = parse(grammar, program)
    render(syntax, args.output_image)


if __name__ == '__main__':
    main()
