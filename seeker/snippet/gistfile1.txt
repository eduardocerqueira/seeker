#date: 2022-03-07T17:08:56Z
#url: https://api.github.com/gists/14edd33caac1f752a5a6ba405f6601e4
#owner: https://api.github.com/users/chrisj-back2work

#!/usr/bin/env bash

set -eu
set -o pipefail
unset CDPATH

##----------------------------------------------------------------------------
##
##  For local dev-test only
##
##  Sets up the DB environment - use reset.sh to tear it down
##
##  Use INIT_FAKE_DATA=1 to use canned data and NOT ETL jobs
##
##----------------------------------------------------------------------------

cat <<BANNER

██╗███╗░░██╗██╗████████╗░░░░░░██████╗░██████╗░
██║████╗░██║██║╚══██╔══╝░░░░░░██╔══██╗██╔══██╗
██║██╔██╗██║██║░░░██║░░░█████╗██║░░██║██████╦╝
██║██║╚████║██║░░░██║░░░╚════╝██║░░██║██╔══██╗
██║██║░╚███║██║░░░██║░░░░░░░░░██████╔╝██████╦╝
╚═╝╚═╝░░╚══╝╚═╝░░░╚═╝░░░░░░░░░╚═════╝░╚═════╝░

BANNER

##----------------------------------------------------------------------------
##
##  Setting up
##  -- fail early on potentially missing configs
##
##----------------------------------------------------------------------------

##
## get shared local dev-test configuration

script_path="$(
    cd -- "$(dirname "$0")" >/dev/null 2>&1
    pwd -P
)"

source "${script_path}"/config.sh

##
## bash error handling -- make it visible

err_notify () {
    CODE=$?
    FAILED_COMMAND="$(caller): ${BASH_COMMAND}" && \
    if [[ ${CODE} != 0 ]]; then
        echo
        printf ${TEXT_PINK}
        echo '------------------------------------'
        echo '  FAILED - see error message above'
        echo '------------------------------------'
        printf ${TEXT_RESET}
    fi
}

trap err_notify ERR EXIT

##
## use canned data, or ETL

INIT_FAKE_DATA="${INIT_FAKE_DATA:-0}"

if [ "$INIT_FAKE_DATA" == "1" ]; then

    cat <<FAKE

███████╗░█████╗░██╗░░██╗███████╗  ██████╗░░█████╗░████████╗░█████╗░
██╔════╝██╔══██╗██║░██╔╝██╔════╝  ██╔══██╗██╔══██╗╚══██╔══╝██╔══██╗
█████╗░░███████║█████═╝░█████╗░░  ██║░░██║███████║░░░██║░░░███████║
██╔══╝░░██╔══██║██╔═██╗░██╔══╝░░  ██║░░██║██╔══██║░░░██║░░░██╔══██║
██║░░░░░██║░░██║██║░╚██╗███████╗  ██████╔╝██║░░██║░░░██║░░░██║░░██║
╚═╝░░░░░╚═╝░░╚═╝╚═╝░░╚═╝╚══════╝  ╚═════╝░╚═╝░░╚═╝░░░╚═╝░░░╚═╝░░╚═╝

FAKE

fi

##----------------------------------------------------------------------------
##
##  Main process
##
##----------------------------------------------------------------------------

echo "Creating the local dev-test DB environment"

##
## traps to ensure bare "blitz" and "prisma" are not used
## -- we must use the "yarn blitz [prisma]" wrappers only

function blitz {
    echo
    printf ${TEXT_PINK}
    echo This script is calling \"blitz\" directly, which
    echo is not supported - change to \"yarn blitz\"
    printf ${TEXT_RESET}
    echo
    exit 1
}

function prisma {
    echo
    printf ${TEXT_PINK}
    echo This script is calling \"prisma\" directly, which
    echo is not supported - change to \"yarn blitz prisma\"
    printf ${TEXT_RESET}
    echo
    exit 1
}

##
## verify clean starting point

if [ -d "${data_path}" ]; then
    echo
    printf ${TEXT_PINK}
    echo The local dev-test DB environment already exists.
    echo To re-initialize it, first run \"make reset-db\".
    printf ${TEXT_RESET}
    echo
    exit 1
fi

##
## verify required utilites are in place

function no_blitz {
    echo
    printf ${TEXT_PINK}
    echo The \"yarn blitz\" command is not found. Please
    echo run \"make dep\" then try this command again.
    printf ${TEXT_RESET}
    echo
    exit 1
}

yarn blitz --version >/dev/null || no_blitz

function no_docker {
    echo
    printf ${TEXT_PINK}
    echo The \"docker\" command is not found. Please
    echo install that then try this command again.
    printf ${TEXT_RESET}
    echo
    exit 1
}

docker --version >>/dev/null || no_docker

##
## get a secret from the GCP dev env
## -- simpler than enabling gcloud in the ETL docker when running locally
## -- secret will not be queried if the env var is set in .env.local
## -- the function is just to provide an informational message

if [ "$INIT_FAKE_DATA" != "1" ]; then
    if [ ! -v AZURE_ACCOUNT_KEY ] || [ -z "$AZURE_ACCOUNT_KEY" ] ; then
        printf ${TEXT_YELLOW}
        echo '------------------------------------------------------------'
        echo '  GCP dev-env secrets - this may require your GCP password  '
        echo '------------------------------------------------------------'
        printf ${TEXT_RESET}
        echo
        AZURE_ACCOUNT_KEY=$(gcloud secrets versions access 1 --project=elementalcognition-app-dev --secret="${ETL_GCP_SECRET_NAME}")
    fi
fi

##
## butterfly-etl image
## -- DEFAULT_ETL_IMAGE is used unless you export ETL_IMAGE from the calling env
## -- update DEFAULT_ETL_IMAGE when there is a newer versions of butterfly-etl
## -- see https://console.cloud.google.com/artifacts/docker/elementalcognition-app-source/us-east4/docker-dev/ec%2Fbutterfly-etl

DEFAULT_ETL_IMAGE=us-east4-docker.pkg.dev/elementalcognition-app-source/docker-all/ec/butterfly-etl:09bd368622bd2db6426edf35f7f0ec9cb7b90c84@sha256:d95416785bce6f01222f6b41881ca5cd9b21aebed2d1fa57251a89c39d0f4a47
ETL_IMAGE="${ETL_IMAGE:-${DEFAULT_ETL_IMAGE}}"

function auth_needed {
    echo
    printf ${TEXT_PINK}
    echo Cannot complete the "docker pull" operation.
    echo
    echo Please verify the docker daemon is running
    echo and/or check ETL_IMAGE is a valid image:tag.
    echo
    echo Or we can\'t access the EC artifact registry.
    echo Please run \"make auth-gcloud\" and try again.
    printf ${TEXT_RESET}
    echo
    exit 1
}

if [ "$INIT_FAKE_DATA" != "1" ]; then
    docker pull "${ETL_IMAGE}" || auth_needed
fi

##----------------------------------------------------------------------------
##
##  Initialize the DB environment
##
##----------------------------------------------------------------------------

echo "Creating data directory: ${data_path}"
mkdir -p "${data_path}"

echo "Starting Zookeeper"
docker run -d --rm --name "${zookeeper}" \
    --hostname "${zookeeper}" \
    -p 2181 \
    -p 2888 \
    -p 3888 \
    debezium/zookeeper:1.7

echo "Starting Kafka"
docker run -d --rm --name "${kafka}" \
    --hostname "${kafka}" \
    -p 9092 \
    --link "${zookeeper}":"${zookeeper}" \
    debezium/kafka:1.7

echo "Starting Confluent Schema Registry"
docker run -d --rm --name "${schema_registry}" \
    --hostname "${schema_registry}" \
    --link "${kafka}" \
    -e SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS="${kafka}":9092 \
    -e SCHEMA_REGISTRY_HOST_NAME="${schema_registry}" \
    -p 8081 \
    confluentinc/cp-schema-registry

echo "Starting Postgres"
docker run -d --rm --name "${postgres}" \
    --hostname "${postgres}" \
    -p 5432:5432 \
    -e POSTGRES_USER="${POSTGRES_USER}" \
    -e POSTGRES_PASSWORD="${POSTGRES_PASSWORD}" \
    -v "${model_path}":/home/"${POSTGRES_USER}"/assets/postgres:ro \
    -v "${data_path}":/var/lib/postgresql/data \
    postgres:14-alpine \
    -c wal_level=logical \
    -c max_wal_senders=1 \
    -c max_replication_slots=1

echo "Waiting for Postgres to initialize"
until docker exec -it "${postgres}" pg_isready -U "${POSTGRES_USER}" >/dev/null; do
    printf '.'
    sleep 1
done
printf '\n'

## sometimes fails without this extra
sleep 5

echo "Creating DBs, roles, permissions, paths"
docker exec -it "${postgres}" \
    psql \
    -v ON_ERROR_STOP=1 \
    -f /home/"${POSTGRES_USER}"/assets/postgres/0-dev-base.sql \
    -U "${POSTGRES_USER}"

echo "Creating schemas, extensions, more permissions (x2)"
databases=("${MAIN_DB_NAME}" "${SHADOW_DB_NAME}")
for database in "${databases[@]}"; do
    echo "-- for DB" "${database}"
    docker exec -it -e PGPASSWORD="${POSTGRES_PASSWORD}" "${postgres}" \
        psql \
        -v ON_ERROR_STOP=1 \
        -f /home/"${POSTGRES_USER}"/assets/postgres/1-dev-schema.sql \
        -v ON_ERROR_STOP=1 \
        -U "${DB_OWNER_NAME}" \
        -d "${database}"
done

## ensuring this critical param matches .env.local -- see config.sh
export DATABASE_URL="${EXPECTED_DATABASE_URL}"

echo "Checking prisma migrations"
if [ -d "db/migrations" ]; then
    ## @@nonprod this works while new migrations are NOT moving through Git
    echo "Applying existing DB migrations"
    yarn blitz prisma migrate reset --force --skip-seed
    echo "Applying local schema updates"
    yarn blitz prisma db push --accept-data-loss
else
    echo "Base migration(s) missing"
    exit 1
fi

echo "Starting Kafka Connect"
docker run -d --rm --name "${connect}" \
    --hostname "${connect}" \
    -p 8083:8083 \
    -e GROUP_ID=connect \
    -e CONFIG_STORAGE_TOPIC=connect_configs \
    -e OFFSET_STORAGE_TOPIC=connect_offsets \
    -e STATUS_STORAGE_TOPIC=connect_statuses \
    -e KEY_CONVERTER=io.confluent.connect.avro.AvroConverter \
    -e VALUE_CONVERTER=io.confluent.connect.avro.AvroConverter \
    -e CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL=http://"${schema_registry}":8081 \
    -e CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL=http://"${schema_registry}":8081 \
    --link "${zookeeper}":"${zookeeper}" \
    --link "${kafka}":"${kafka}" \
    --link "${schema_registry}":"${schema_registry}" \
    --link "${postgres}":"${postgres}" \
    debezium/connect:1.7

echo "Waiting for Kafka Connect to initialize"
until curl --output /dev/null --silent --fail http://localhost:8083/connectors/; do
    printf '.'
    sleep 1
done
printf '\n'

echo "Creating Debezium connector"
curl -i \
    -X POST \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    localhost:8083/connectors/ \
    -d "{
      \"name\": \"buy-side-connector\",
      \"config\": {
        \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",
        \"plugin.name\": \"pgoutput\",
        \"database.hostname\": \"${postgres}\",
        \"database.port\": \"5432\",
        \"database.user\": \"${MAIN_DB_NAME}_replication\",
        \"database.password\": \"password\",
        \"database.dbname\" : \"${MAIN_DB_NAME}\",
        \"database.server.name\": \"${MAIN_DB_NAME}\"
      }
    }"
echo

## change the grep param e.g. to include bw-fake data

if [ "$INIT_FAKE_DATA" == "1" ]; then
    ## use static companies, assets, and prices
    DATAFILE_GREPSPEC='real\|static\|fakeusers'
else
    ## ETL wull provide companies, assets, and prices
    DATAFILE_GREPSPEC='real\|fakeusers'
fi

for seed in $(ls -1 ${model_path}/data-*.sql | grep ${DATAFILE_GREPSPEC} | sort | xargs -L 1 basename); do
    echo "-- for seed" "${seed}"
    docker exec -it -e PGPASSWORD="${POSTGRES_PASSWORD}" "${postgres}" \
        psql \
        -v ON_ERROR_STOP=1 \
        -f /home/"${POSTGRES_USER}"/assets/postgres/${seed} \
        -U "${DB_OWNER_NAME}" \
        -d "${MAIN_DB_NAME}"
done

echo "Preparing for Flink"

##
##  Note POSTGRES_URL doesn't include the schema name -- that assumes a schema
##  path has been defined in the DB, which we did using "0-dev-base.sql" above
## -- see https://jdbc.postgresql.org/documentation/head/connect.html
##

## not sure, this may require ?options=--search_path%3Dbuy_side"
ETL_POSTGRES_URL="postgresql://${postgres}:5432/${MAIN_DB_NAME}"

if [ "$INIT_FAKE_DATA" != "1" ]; then
    echo "Starting Flink ETL job"
    set -x
    docker run -d --rm --name "${etl}" \
        -p 9081:8081 \
        --link "${postgres}":"${postgres}" \
        --env AZURE_ACCOUNT_KEY="${AZURE_ACCOUNT_KEY}" \
        --env POSTGRES_URI="${ETL_POSTGRES_URL}" \
        --env POSTGRES_USER="${DB_OWNER_NAME}" \
        --env POSTGRES_PASSWORD="${DB_OWNER_PASSWORD}" \
        --env POLL_INTERVAL_SECONDS="30" \
        --env HEAP_SIZE="2g" \
        "${ETL_IMAGE}"
    set +x
else
    echo "Using fake data, NOT starting Flink ETL job"
fi

echo "Starting Materialize"
docker run -d --rm --name "${materialize}" \
    --hostname "${materialize}" \
    -p 6875:6875 \
    --link "${kafka}":"${kafka}" \
    --link "${schema_registry}":"${schema_registry}" \
    materialize/materialized:v0.9.11 \
    --workers 1

echo "Waiting for Materialize to initialize"
materialize_ip=$(docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' "${materialize}")
until docker exec -it "${postgres}" pg_isready -U materialize -h "${materialize_ip}" -p 6875 >/dev/null; do
    printf '.'
    sleep 1
done
printf '\n'

echo "Retrieving topic list"
topics=()
while IFS='' read -r line; do topics+=("$line"); done < <(docker exec -it "$kafka" bash \
    -c 'bin/kafka-topics.sh \
    --bootstrap-server=$(awk '"'"'END{print $1}'"'"' /etc/hosts):9092 \
    --list \
    | grep ^'"${MAIN_DB_NAME}")

echo "Adding sources to Materialize"
for topic in "${topics[@]}"; do
    trimmed_topic=${topic//[$'\t\r\n']/}
    source_name="${trimmed_topic//${MAIN_DB_NAME}.${DB_SCHEMA_NAME}./}"
    echo "Creating ${source_name} source"
    source=$(echo "
                CREATE SOURCE ${source_name}
                FROM KAFKA BROKER '${kafka}:9092'
                TOPIC '${trimmed_topic}'
                KEY FORMAT AVRO USING CONFLUENT SCHEMA REGISTRY 'http://${schema_registry}:8081'
                VALUE FORMAT AVRO USING CONFLUENT SCHEMA REGISTRY 'http://${schema_registry}:8081'
                ENVELOPE DEBEZIUM UPSERT;
            " | tr -s ' ')
    docker exec -i "${postgres}" psql \
        -v ON_ERROR_STOP=1 \
        -U materialize \
        -h "${materialize_ip}" \
        -p 6875 \
        -c "${source}"
done

printf ${TEXT_GREEN}
echo
echo "Init complete"
printf ${TEXT_RESET}
